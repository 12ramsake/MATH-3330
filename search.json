[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 3330: Regression Notes",
    "section": "",
    "text": "Preface\nWelcome to the MATH 3330 Notes! Please install R and R studio (Or you can use VSCode if you’re comfortable there.)\nIn order to make the most of these notes, do all of the exercises in the order they appear.\nThere will likely be typos and some errors, please let me know if you encounter any.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Unit_0.html",
    "href": "Unit_0.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is the course about?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Unit_0.html#what-is-the-course-about",
    "href": "Unit_0.html#what-is-the-course-about",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1.1 The main question\nThe whole course is concerned with the following problem: Suppose that \\(X\\) and \\(Y\\) are some attributes of a population. What is the relationship between \\(X\\) and \\(Y\\). How can we use \\(X\\) to predict \\(Y\\), or how can we use \\(X\\) to explain \\(Y\\)?\nFor example, questions of this form include:\n\nHow is location, square feet, parking available related to the price of an Airbnb?\nHow is hours played and age related to win rate in League of Legends?\nHow are creatine and protein consumption related to deadlift 1RM?\nHow is treatment (A or B) related to pain levels of patients?\n\nAll of these can be answered with regression!\n\nExercise 1.1 What is \\(X\\) and what is \\(Y\\) here?\n\n\nSolution 1.1. \n\n\\(X\\): location, square feet, parking available \\(Y\\): price of an Airbnb\n\\(X\\): hours played and age \\(Y\\): win rate in League of Legends\n\\(X\\): creatine and protein consumption \\(Y\\): deadlift 1RM\n\\(X\\): treatment (A or B) \\(Y\\): pain levels of patients\n\n\nWe suppose at the population level, on average that \\(Y=f(X).\\) By on average, we mean that each person may not have exactly \\(Y=f(X),\\), but if we we average out \\(Y\\) for many people, we will have that the average is approximately \\(f(X)\\). (This will be made more formal later).\nFor instance, consider the pain level question in the above example. Suppose that \\(f(A)=2\\) and \\(f(B)=5\\). Then, if we average the pain level of many patients who take treatment \\(B\\), it should be close to 5.\nObviously, we cannot observe the whole population, and so we will assume that we have observed \\(X\\) and \\(Y\\) for a set of \\(n\\) individuals. Specifically, we observe some outcome \\(Y_1,\\ldots, Y_n\\), which is a real number and some attributes (categorical or numeric) about the \\(n\\) individuals, denoted by \\(X_1,\\ldots, X_n\\). Note that here \\(X_i\\) can be vectors or single numbers.\n\n\n1.1.2 Using our data, how can we determine \\(f\\)?\nOther, related questions:\n\nWhat is the form of \\(f\\)? Is it linear?\nHow can we estimate \\(f\\), say with \\(\\hat f\\)? What is the best \\(\\hat f\\)? What is the error of \\(\\hat f\\) on average?\nHow can we tell if our model is good? i.e. how does \\(\\hat f\\) fit the data?\nHow can we tell which \\(X\\) values are important? How can we tell if \\(X\\) is related to \\(Y\\) at all?\nWhat is the effect of correlation of \\(X\\) values?\n\nThese are all questions we will answer in this course.\nStatistical modelling starts as follows:\n\nQuestion about a population, e.g., “How are hours played and age related to win rate in League of Legends?”\nData: \\((Y_1,X_1),\\ldots,(Y_n,X_n)\\)\nExplore data with graphs and summary stats\nUse exploratory data analysis to posit a model for the population.\n\nNote that step 4 is necessary! Letting \\(f\\) be anything is too general and won’t work well, so we need to use the data to give us a hint at the form of \\(f\\)! For instance, we might suppose that \\(f\\) is a linear function! That is, \\(f\\in \\{g(X)=X\\beta\\colon \\beta\\in\\mathbb{R}^d\\}\\).\nNext, we proceed with the following steps:\n\nEstimation: How to get an estimate \\(\\hat\\beta\\) of \\(\\beta\\)?\nInference: What is the error of \\(\\hat\\beta\\)? Is \\(f\\) degenerate? I.e., is \\(\\beta=0\\)?\nFit: Does our fitted line match up with the data? What about the normality assumption? Do the errors appear normal?\nPrediction: Predict any values if necessary.\n\n\n\n1.1.3 Comparison with means example\nLet’s compare to what we learned in previous statistics courses about two sample testing with the above steps in mind. Below we have different hours of extra sleep for two different treatments. Let’s see if the sleep for groups 1 and 2 differ.\n\nDo the counts for A and B differ?\n\n\n# 2. \ndata('sleep')\nhead(sleep)\n\n  extra group ID\n1   0.7     1  1\n2  -1.6     1  2\n3  -0.2     1  3\n4  -1.2     1  4\n5  -0.1     1  5\n6   3.4     1  6\n\n# 3. \naggregate(extra ~ group, data = sleep, FUN = function(x){hist(x,main=names(x))})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning in format.data.frame(if (omit) x[seq_len(n0), , drop = FALSE] else x, :\ncorrupt data frame: columns will be truncated or padded with NAs\n\n\n  group                   extra\n1     1   -2, -1, 0, 1, 2, 3, 4\n2     2 -1, 0, 1, 2, 3, 4, 5, 6\n\nsummary_stats = aggregate(extra ~ group, data = sleep,  FUN = summary)\naggregate(extra ~ group, data = sleep, FUN = length)\n\n  group extra\n1     1    10\n2     2    10\n\n\nWe will assume that the extra hours are normal from the histograms.\nRecall then that the pooled standard deviation is \\(\\hat\\sigma_p=\\sqrt{((n_x-1)\\hat \\sigma_x^2+(n_y-1)\\hat \\sigma_y^2)\\big/ (n_x+n_y-2)}\\) and the test statistic is: \\[T=\\frac{\\bar X-\\bar Y}{\\hat\\sigma_p\\times\\sqrt{1/n_x+1/n_y}}.\\] In addition, we have that \\(T\\sim t_{n_x+n_y-2}\\).\n\n# 5 and 6 - here these steps are the same, since we are only doing inference\nt.test(sleep$extra[sleep$group==1],sleep$extra[sleep$group==2])\n\n\n    Welch Two Sample t-test\n\ndata:  sleep$extra[sleep$group == 1] and sleep$extra[sleep$group == 2]\nt = -1.8608, df = 17.776, p-value = 0.07939\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.3654832  0.2054832\nsample estimates:\nmean of x mean of y \n     0.75      2.33 \n\n# 7 - we checked normality earlier, 8 is not applicable\n\nHere, we fail to reject the null hypothesis, and there is not enough evidence to suggest that there is a different between the groups. Notice that the p-value is 0.08, which is moderately low, so there is some evidence of a difference between the groups.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Unit_0.html#important-course-information-and-preparation-tasks",
    "href": "Unit_0.html#important-course-information-and-preparation-tasks",
    "title": "1  Introduction",
    "section": "1.2 Important course information and preparation tasks",
    "text": "1.2 Important course information and preparation tasks\n\n1.2.1 Prerequisite review\nIf you have forgotten, you should review the following concepts:\n\nSample vs. population, estimates vs. parameters, hypothesis testing and confidence intervals\nNormal theory, random variables, conditional variance and expectation.\nCLT, LLN\nLinear algebra: Matrix operations, inverse, transpose etc.\n\n\n\n1.2.2 Software\nDownload RStudio/R. You can use python, but I’ll use R in class. If you are not familiar with R, please follow this tutorial here.\n\n\n1.2.3 Outline\nThe course will proceed as follows:\n\nReview\nCore linear regression concepts\nSpecial Cases\nAdvanced\n\n\n\n1.2.4 Homework tasks:\n\nDownload and install RStudio and R Software\nThink of a relationship you would want to model, what is \\(X\\)? what is \\(Y\\)?\nReview prerequisites as stated above",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Unit_1.html",
    "href": "Unit_1.html",
    "title": "2  Review material",
    "section": "",
    "text": "2.1 Review of random variables\nRecall that\nGenerally, we will just write \\(X\\), and ignore the fact that \\(X\\) is a function.\nWe can categorize a random variable \\(X\\) as follows: - If \\(X\\colon \\Omega\\rightarrow S\\) where \\(S\\) is countable, then \\(X\\) is a discrete random variable - We say \\(X\\) is a continuous random variable if \\(\\Pr(X=r)=0\\) for all \\(r\\in\\mathbb{R}\\). - Otherwise, \\(X\\) is a mixed random variable (which we won’t worry about in this course)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Review material</span>"
    ]
  },
  {
    "objectID": "Unit_1.html#review-of-random-variables",
    "href": "Unit_1.html#review-of-random-variables",
    "title": "2  Review material",
    "section": "",
    "text": "Definition 2.1 A random variable \\(X\\) is a function which maps outcomes \\(\\omega\\in \\Omega\\) to the real numbers, i.e., \\(X\\colon \\Omega\\rightarrow\\mathbb{R}\\).\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the notation \\(f\\colon A\\rightarrow B\\) means that \\(f\\) is a function whose domain is \\(A\\) and range is \\(B\\). That is, \\(f\\) takes a value from \\(A\\) and outputs some value in \\(B\\).\n\n\n\n\n\n2.1.1 Discrete Random Variables\nIf \\(X\\colon \\Omega\\rightarrow S\\) where \\(S\\) is countable, then \\(X\\) is a discrete random variable. \\(S\\) can be finite, but can also be any infinite subset of the integers \\(\\mathbb{Z}\\). The distribution of \\(X\\) is given by its PMF, denoted by \\(f(x)\\). For any \\(x\\in S\\), \\(f(x)=\\Pr(X=x)\\). (Note that ‘\\(\\in\\)’ means the word “in”.) We must have that: - \\(\\sum_{x\\in S}f(x)=1\\), (This notation means summing over all the elements in \\(S\\).) - \\(\\forall x\\in S\\), \\(0\\leq f(x)\\leq 1\\). (This notation means for all \\(x\\) in \\(S\\), \\(0\\leq f(x)\\leq 1\\).)\nExamples: Binomial random variables, Poisson random variables and Geometric random variables are all discrete random variables.\n\nExercise 2.1 What is the PMF of a Binomial random variable? Can two different random variables have the same PMF? Why or why not?\n\n\nSolution 2.1. First: \\(\\Pr\\left(X=x\\right)\\binom{n}{x}p^x(1-p)^{n-x}\\) Second: Yes. Two random variables can be different random variables, but have the same distribution.\n\n\n\n2.1.2 Continuous Random Variables\nWe say \\(X\\) is a continuous random variable if \\(\\Pr(X=r)=0\\) for all \\(r\\in\\mathbb{R}\\). If \\(X\\colon \\Omega\\rightarrow S\\) and \\(X\\) is a continuous random variable, then \\(S\\) is typically the real numbers, denoted by \\(\\mathbb{R}\\), but can be any uncountable subset of \\(\\mathbb{R}\\). The distribution of \\(X\\) is given by the PDF \\(f(x)\\). For any interval \\((a,b)\\subset\\in S\\), \\(\\Pr(X\\in (a,b))=\\int_a^b f(x)dx\\). We must have that: - \\(\\int_{- \\infty}^\\infty f(x)dx=1\\), - \\(\\forall x\\in \\mathbb{R}\\), \\(f(x)\\geq 0\\).\nExamples: Normal random variables, Chi- squared random variables, \\(t\\) random variables, Cauchy random variables, \\(F\\) random variables are all continuous random variables. Generally, we will focus on continuous random variables.\n\nExercise 2.2 What is the PMF of a Normal random variable?\n\n\nSolution 2.2. \\(f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-1(x-\\mu)^2/2\\sigma^2}\\)\n\n\n\n2.1.3 Properties of Random Variables\nLet \\(X,X_1,X_2\\) be random variables.\nRecall the important quantities \\(\\mathrm{E}{X}\\), \\(\\mathrm{var}{X}\\), \\(\\mathrm{cov}(X_1,X_2)\\), \\(\\mathrm{corr}(X_1,X_2)\\). Recall expectation: ::: {#def-2} The expectation of a random variable \\(X\\) is \\[\\mathrm{E}{X}=\\sum_{x\\in S}x\\Pr\\left(X=x\\right),\\] if \\(X\\) is discrete and is \\[\\mathrm{E}{X}=\\int_{-\\infty}^\\infty f(x)dx,\\] if \\(X\\) is continuous. ::: This is the ``average’’ value of the random variable. Note that it is possible for it to be impossible for \\(X=\\mathrm{E}{X}\\). Try to come up with an example of this!\n\nDefinition 2.2 The variance of a random variable \\(X\\) is \\[{\\textrm{Var}}\\left[X\\right]={\\textrm{E}}\\left[|X-{\\textrm{E}}\\left[X\\right]|^2\\right]=\\sum_{x\\in S}(x-{\\textrm{E}}\\left[X\\right])^2\\Pr\\left(X=x\\right),\\] if \\(X\\) is discrete and is \\[{\\textrm{Var}}\\left[X\\right]={\\textrm{E}}\\left[|X-{\\textrm{E}}\\left[X\\right]|^2\\right]=\\int_{-\\infty}^\\infty (x-{\\textrm{E}}\\left[X\\right])^2f(x)dx,\\] if \\(X\\) is continuous.\n\nThe variance describes the variation of \\(X\\) about its mean. In other words, it describes on ``average’’, how far is \\(X\\) from its mean.\n\nDefinition 2.3 The covariance between two random variables \\(X\\) and \\(Y\\) is \\[{\\textrm{cov}}\\left[X,Y\\right]={\\textrm{E}}\\left[(X-{\\textrm{E}}\\left[X\\right])(Y-{\\textrm{E}}\\left[Y\\right])\\right].\\]\n\nThe covariance describes the unnormalised linear association between \\(X\\) and \\(Y\\).\n\nDefinition 2.4 The correlation between two random variables \\(X\\) and \\(Y\\) is \\[{\\textrm{corr}}\\left[X,Y\\right]={\\textrm{cov}}\\left[X,Y\\right]/\\sqrt{{\\textrm{Var}}\\left[X\\right]{\\textrm{Var}}\\left[Y\\right]}.\\]\n\nThe correlation describes the normalized linear association between \\(X\\) and \\(Y\\).\nNext, recall that for a random variable \\(X\\), its cumulative distribution function (CDF) is given by \\(F_X(x)=\\Pr\\left(X\\leq x\\right)\\). The joint CDF of \\(X\\) and \\(Y\\) is given by \\(F_{XY}(x,y)=\\Pr\\left(X\\leq x, Y\\leq y\\right)\\).\nLastly, for a vector of \\(d\\) random variables \\(\\mathbf{X}=(X_1,\\ldots,X_d)\\), let its CDF by \\(F_{\\mathbf{X}}(\\mathbf{x})=\\Pr\\left(X_1\\leq x_1,\\ldots X_d\\leq x_d\\right)\\), where here \\(\\mathbf{x}\\in\\mathbb{R}^{d}\\) and \\(\\mathbf{x}=(x_1,\\ldots,x_d)\\).\nWe next present the concept of independence of random variables. Let \\(F_{XY}(x,y)\\) be the joint CDF of \\(X\\) and \\(Y\\) and let \\(F_X\\) and \\(F_Y\\) be the univariate CDFs of \\(X\\) and \\(Y\\), respectively. For two random variables \\(X\\) and \\(Y\\), we say that \\(X\\) and \\(Y\\) are independent if \\(F_{XY}(x,y)=F_X(x)F_Y(y)\\). More generally, two vectors of random variables \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are independent if \\(F_{\\mathbf{X},\\mathbf{Y}}(\\mathbf{x},\\mathbf{y})=F_{\\mathbf{X}}(\\mathbf{x})F_{\\mathbf{Y}}(\\mathbf{y})\\), where A set of random variables \\(\\{X_i\\}_{i=1}^n\\) are mutually independent if for any two subsets mutually exclusive subsets of \\(\\{X_i\\}_{i=1}^n\\) are also independent. Note that we write \\(X \\perp Y\\) if \\(X\\) is independent of \\(Y\\).\nWe have that:\n\nTheorem 2.1  \n\n\\(X_1\\perp X_2 \\implies {\\textrm{E}}\\left[X_1 X_2\\right]={\\textrm{E}}\\left[X_1\\right]{\\textrm{E}}\\left[X_2\\right]\\)\n\\(X_1\\perp X_2 \\implies {\\textrm{corr}}\\left[X_1,X_2\\right]=0\\)\n\\({\\textrm{corr}}\\left[X_1,X_2\\right]=0\\) does not imply \\(X_1\\perp X_2\\)\n\n\n\nExercise 2.3 Prove Theorem 2.1 .\n\nLet \\(X,X_1,X_2,\\ldots X_n\\) be random variables. Recall the linearity of expectation property:\n\nTheorem 2.2 For \\(a,b\\in\\mathbb{R}\\), it holds that \\(\\mathrm{E}{aX+b}=a\\mathrm{E}{X}+b\\).\n\n\nExercise 2.4 Prove Theorem 2.2 .\n\nAs a corollary of Theorem 2.2 , we have that - \\({\\textrm{E}}\\left[\\sum_{i=1}^n a_i X_i\\right]=\\sum_{i=1}^n a_i{\\textrm{E}}\\left[ X_i\\right]\\) - \\({\\textrm{Var}}\\left[\\sum_{i=1}^n a_i X_i\\right]=\\sum_{i=1}^n a_i^2{\\textrm{Var}}\\left[ X_i\\right]+\\sum_{i\\neq j}a_ia_j{\\textrm{cov}}\\left[X_i,X_j\\right]\\) - \\({\\textrm{Var}}\\left[aX_1+bX_2+c\\right]=a^2\\mathrm{var}{X_1}+b^2{\\textrm{Var}}\\left[X_2\\right]+2ab{\\textrm{cov}}\\left[X_1,X_2\\right]\\)\n\nExercise 2.5 What happens to \\({\\textrm{Var}}\\left[aX_1+bX_2+c\\right]\\) when \\(\\{X_i\\}_{i=1}^n\\) are mutually independent?\n\n\nExercise 2.6 Let \\(X_1,X_2,\\ldots X_n\\) be iid random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\). What is the mean and variance of \\[\\bar X=\\sum_{i=1}^n X_i/n?.\\]\n\n\n\n2.1.4 Useful properties of normal and related random variables\nLet\n\n\\(\\mathcal{N}(\\mu,\\sigma^2)\\) represent the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\n\\(\\chi^2_k\\) be the Chi-squared distribution with \\(k\\) degrees of freedom\n\\(t_n\\) be the student-\\(t\\) distribution with \\(n\\) degrees of freedom\n\\(F_{m,n}\\) be the \\(F\\) distribution with \\(m\\) numerator degrees of freedom and \\(n\\) denominator degrees of freedom\n\nWe have the following results:\n\nTheorem 2.3 Suppose that \\(X\\sim \\mathcal{N}(\\mu,\\sigma^2)\\), then - \\(Z=\\frac{X- \\mu}{\\sigma}\\sim \\mathcal{N}(0,1)\\) - \\(Z^2\\sim \\chi^2_1\\).\n\nLet \\([n]=\\{1,\\ldots,n\\}\\). We also have that\n\nTheorem 2.4  \n\nIf for \\(i\\in [n]\\) \\(Y_i\\sim \\chi^2_{k_i}\\) and \\(Y_i \\perp Y_j\\) for \\(i\\neq j\\) then \\(\\sum_{i=1}^n Y_i \\sim \\chi^2_{k_1+\\ldots +k_n}\\).\nIf \\(Y\\sim \\chi^2_k\\) and \\(Y\\perp Z\\), then \\(Z/\\sqrt{Y/k}\\sim t_k\\).\nIf \\(Y_1\\sim \\chi^2_{k_1}\\), \\(Y_2\\sim \\chi^2_{k_2}\\) and \\(Y_1 \\perp Y_2\\) then \\(\\frac{Y_1/k_1}{Y_2/k_2}\\sim F_{k_1,k_2}\\).\n\n\nDefine \\[\\hat\\sigma^2=\\frac{1}{n- 1}\\sum_{i=1}^n (X_i- \\bar X)^2.\\]\n\nTheorem 2.5 Suppose that \\(X_1,X_2,\\ldots X_n\\sim \\mathcal{N}(\\mu,\\sigma^2)\\) and are independent, then \\(\\frac{\\bar X- \\mu}{\\sigma/\\sqrt{n}}\\sim \\mathcal{N}(0,1)\\), \\(\\bar X\\perp \\hat\\sigma^2\\), \\((n- 1)\\hat\\sigma^2/\\sigma^2\\sim \\chi^2_{n- 1}\\) and \\(\\frac{\\bar X- \\mu}{\\hat\\sigma/\\sqrt{n}}\\sim t_{n- 1}\\).\n\n\n\n2.1.5 Central Limit Theorem\n If \\(X_1,X_2,\\ldots X_n\\) are i.i.d. with mean \\(\\mu\\) and variance \\(\\sigma^2&lt;\\infty\\), then \\(\\frac{\\bar X- \\mu}{\\sigma/\\sqrt{n}}\\stackrel{\\text{d}}{\\rightarrow}\\mathcal{N}(0,1)\\) as \\(n\\rightarrow\\infty\\).\nWe have that in general, for large \\(n\\), regardless of the distribution of the random variables, the sample mean is approximately normally distributed.\n\n\n2.1.6 Homework stop 1\nReview your material and complete the above exercises before continuing to the next section.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Review material</span>"
    ]
  },
  {
    "objectID": "Unit_1.html#review-of-introductory-statistics",
    "href": "Unit_1.html#review-of-introductory-statistics",
    "title": "2  Review material",
    "section": "2.2 Review of introductory statistics",
    "text": "2.2 Review of introductory statistics\nThe followings are some concepts that you have learned from prerequisites, and/or we have reviewed in the last two lectures.\n\nSample vs. Population\nObservation vs. Random variable\nStatistic vs. Parameter\nEstimate vs. Estimator\nEstimator is a random variable and estimate is a number calculated from data\nMean and variance of random variable\nRelationships between Normal, \\(t\\) , \\(\\chi^2\\), \\(F\\) etc.\n\n\n2.2.1 Basic premise of statistics\nThe whole purpose of statistics is to learn something about a population using only a sample of units from that population. A sample is a smaller, typically randomly selected, subset of a population. A population is a collection of units which we would like to know something about. For example, we may collect a sample of hamburgers from McDonald’s if we want to learn something about the population of McDonald’s hamburgers.\nIn general, at least for this course, we assume that we have access to a sample of units from a given population. Furthermore, we assume that that sample is a random sample. Specifically, we assume that these units in the sample are realizations of random variables. In addition, we also assume that these random variables are mutually independent. For example, we could assume that our sample \\(X_1,\\ldots,X_n\\) is Normally distributed with some fixed mean \\(\\mu\\) and fixed variance \\(\\sigma^2\\). In this case, \\(\\mu\\) and \\(\\sigma^2\\) are unknown parameters of the population. A parameter of a population is some quantity that is a function of the distribution of our given sample. For instance, \\({\\textrm{E}}\\left[X_i\\right]=\\mu\\). Generally, we are concerned with unknown population parameters, which are parts of the distribution that are unknown, and can only ever be estimated. For example, we may know our data is normal, but not know the mean parameter. In that case, we need to use an estimate of the parameter. We use a function of the data, typically called the estimator, say \\(T\\), which produces the estimate, given by \\(T\\) computed at the sample we observed: \\(T(X_1,\\ldots,X_n)\\).\nFor example, to estimate \\(\\mu\\), we typically use the sample mean. Here, the estimate is given by \\(\\bar X=\\sum_{i=1}^nX_i/n\\). To be specific, the estimate is the value of \\(\\bar X\\) and the estimator \\(T\\) is the function that maps \\(n\\) real numbers to their mean. In general, estimates are used to give our `best guess’ at population parameters.\n\n\n2.2.2 Confidence intervals:\nRecall from the previous section that our estimate of a parameter is only that, an estimate. In other words, it is not exactly equal to the population parameter. For instance, if we drew a different sample our estimate would change. A confidence interval is used to acknowledge this phenomenon in the reporting of our statistics. Its used to give a range of estimates that we might have obtained from any “regular” sample we might observe. It is ultimately used to quantify the error (sometimes called uncertainty) in our estimate.\nConfidence intervals consist of a level, usually denoted by \\((1-\\alpha)100\\%\\) and two end points. For example, you have learned confidence intervals for the population mean. When we say \\((-1,1)\\) is \\(95\\%\\) confidence interval for the population mean, what does this mean? Colloquially, it means that we expect the sample mean to be somewhere within \\((-1,1)\\) with high confidence. Note that confidence intervals are computed from the data, which means also that for each new sample, we would get a different confidence interval. However, the population parameter never changes. Therefore, the interval is what is varying from sample to sample. This impacts the interpretation of a confidence interval.\nContinuing our example, we have that the interval \\((-1,1)\\) can be interpreted as: “if we drew many more samples, 95% of the intervals will contain the population parameter.” We do not say that the parameter has a 95% chance of falling in (-1,1), since the parameter is not random, the interval end points are.\nFor example, we have the formula for a confidence interval for the population mean is given by: \\(\\bar X\\pm 1.96\\hat\\sigma\\). Notice that it is based only on the data. Therefore, it will change if we drew a new sample.\nTo summarize this section, a confidence interval is used to quantify the uncertainty in our reported estimates. By uncertainty, we specifically mean the uncertainty resulting from the fact that we have only a sample of the population, and our estimate varies depending on the sample.\n\n\n2.2.3 Hypothesis tests:\nHypothesis tests are used to determine whether an effect is spurious or a real property of the population. A spurious effect is one that is specific to the sample we observed, and is not a real property of the population. For example, if the heights of males and female students are measured, and we observe that the sample mean of both male and females are equal, then this would be a spurious effect. We know that the population heights of males and females are substantially different. If we drew a new sample, we would likely observe something that mirrors the population reality (provided it is large enough).\nFormally, a hypothesis test compares two competing beliefs about a population parameter, called the null and alternative hypothesis. For instance, we may wish to test whether the population heights of men is greater than women, vs. the heights being less than or equal to that of men.\nWe write this as follows: \\(H_0\\colon \\mu_{men}\\leq \\mu_{women}\\) vs. \\(H_a\\colon \\mu_{men}&gt; \\mu_{women}\\).\n\nThe null hypothesis is usually chosen to be one such that if we make a mistake, the error is most serious. However, it is usually clear from the context.\n\nIn general, we compute a test statistic and its distribution under the null hypothesis. Then we compute how likely it was to see the observed test statistic we saw, if the null hypothesis was true. This is likelihood is given by the p-value. If it was sufficiently unlikely (in other words, the p-value is less than the threshold \\(\\alpha\\)), then we reject the null hypothesis. Otherwise, we fail to reject the null hypothesis. If we fail to reject the null hypothesis then either the null hypothesis is true, it is not true, but there was not enough data collected to show the effect.\nThere are two types of errors we can make in a hypothesis test: Type 1 and Type 2 error. Type one error occurs when we reject the null hypothesis when it is true. Type two error occurs when we fail to reject the null hypothesis when the alternative is true.\nLet’s do an example.\n\nExercise 2.7 In a study about online dating, you are interested in determining the average age of individuals who use online dating platforms. You want to know whether the average age of online daters is significantly different from 30. You have a dataset of 40 ages of people using online dating platforms.\n\nHow would you answer this question?\n\\[H_0\\colon \\mu=30\\qquad vs. \\qquad H_1\\colon \\mu\\neq 30.\\]\nFirst, we can explore the data:\n\ngetwd()\n\n[1] \"C:/Users/12RAM/OneDrive - York University/Teaching/Courses/Math 3330 Regression/Math 3330 Notes\"\n\nages=read.csv('C:\\\\Users\\\\12RAM\\\\OneDrive - York University\\\\Teaching\\\\Courses\\\\Math 3330 Regression\\\\Lecture Codes\\\\Data\\\\dating_ages.csv')[,2]\n\n\n\nhist(ages,freq=F)\n\n\n\n\n\n\n\n\nNow, assume that \\(X_1,\\ldots, X_{40}\\sim \\mathcal{N}(\\mu,\\sigma^2)\\), and independent. (We can justify normality with the histogram, or we could also invoke the CLT to get normality of the sample mean (not the data itself).) Therefore, we can do a one sample \\(t\\)-test. Recall that, under the null hypothesis, we have \\(\\frac{\\bar X-30}{\\hat\\sigma/\\sqrt{n}}\\sim t_{n-1}\\). This means that if \\(\\left|\\frac{\\bar X-30}{\\hat\\sigma/\\sqrt{n}}\\right|\\geq t_{n-1,1-\\alpha/2}\\), then we reject the null hypothesis! Here, \\(t_{n-1,1-p}\\) is the \\((1-p)\\)th quantile of the \\(t_{n-1}\\) distribution. For large \\(n\\) and \\(p=0.025\\), this is roughly equal to 2.\nNow, recall that \\[ H_0\\colon \\mu=30\\qquad vs. \\qquad H_1\\colon \\mu\\neq 30 \\] We have that \\(\\left|\\frac{\\bar X-30}{\\hat\\sigma/\\sqrt{n}}\\right|=66.234\\). Using R, we get that the p-value is \\(&lt; 2.2\\times 10^{-16}\\).\n\n# Calculate the mean of the 'ages' data and assign it to xbar\nxbar = mean(ages)\nxbar  # Print the mean\n\n[1] 28.16378\n\n# Calculate the variance of the 'ages' data and assign it to ssq\nssq = var(ages)\nssq  # Print the variance\n\n[1] 1.277377\n\n# Calculate the length (number of observations) of the 'ages' data and assign it to n\nn = length(ages)\nn  # Print the number of observations\n\n[1] 40\n\n# Set the significance level \nalpha = 0.05\n\n# Perform a two-sided t-test to check if the mean of 'ages' is significantly different from 30\n# t.test() is the function for performing t-tests in R\ntest = t.test(ages, mu = 30, alternative = 'two.sided')\ntest  # Print the test result\n\n\n    One Sample t-test\n\ndata:  ages\nt = -10.275, df = 39, p-value = 1.179e-12\nalternative hypothesis: true mean is not equal to 30\n95 percent confidence interval:\n 27.80232 28.52524\nsample estimates:\nmean of x \n 28.16378 \n\n\nHere the p-value measures how much evidence there is against the null hypothesis. If the p-value is very small, then this constitutes strong evidence against the null hypothesis. If the p-value is small, but closer to 0.05, then there is evidence against the null. If it is larger, but still small, say 0.1, then this is weak evidence against the null hypothesis. It is not helpful to throw it away if it is above 0.05, therefore we should not just take \\(\\alpha=0.05\\). Choosing \\(\\alpha\\) depends on how serious a type 1 error is. If it is not that serious, we can take \\(\\alpha\\) larger. If it is very serious, we can take \\(\\alpha\\) smaller.\nIn this example, there is very strong evidence against the null hypothesis.\n\n\n\n\n\n\nNote\n\n\n\nNote also that we can use the confidence interval method with \\[ \\bar X\\pm t_{n-1,1-\\alpha/2}\\sqrt{\\hat\\sigma^2/n}.\\]\n\n\n\n# Alternative method to calculate the confidence interval\n# ci will store the confidence interval values\nci = xbar + c(-1, 1) * qt(1 - alpha / 2, n - 1) * sqrt(ssq / n)\nci  # Print the confidence interval\n\n[1] 27.80232 28.52524\n\n\n\n\n\n\n\n\nNote\n\n\n\nMoving beyond the one-sample testing problem, we might be interested in other population parameters, say \\(\\theta\\in \\Theta\\). Think Lecture 1: \\({\\textrm{E}}\\left[Y|X\\right]=\\beta_0+X\\beta_1\\), we might want to estimate \\({\\textrm{E}}\\left[Y|X\\right]\\), which amounts to \\(\\beta_0,\\beta_1\\in \\mathbb{R}\\)! In general, we may estimate \\(\\theta\\) by \\(\\hat\\theta\\). Then we may compute the variance and distribution of \\(\\hat\\theta\\). From there, we can make confidence intervals and conduct hypothesis tests etc.\n\n\nLet’s do another example:\n\nExercise 2.8 In a study about online dating, you are interested in determining if the average age of those who identify as men who use online dating platforms differs from those who identify as women. You have a dataset of 20 ages of each group using online dating platforms.\n\nWhat is the population parameter of interest here? It is \\(\\Delta=\\mu_{1}-\\mu_2\\), the difference in means between the two populations. Now, suppose that \\(X_1,\\ldots, X_{20}\\sim \\mathcal{N}(\\mu_1,\\sigma^2)\\) and \\(Y_1,\\ldots, Y_{20}\\sim \\mathcal{N}(\\mu_2,\\sigma^2)\\), and are mutually independent. (We could also invoke the CLT instead of assuming normality.) We can estimate those parameters with estimates. For instance, \\(\\bar X,\\ \\bar Y\\), \\[\\hat\\sigma^2=\\frac{(n_1-1)\\hat\\sigma_1^2+(n_2-1)\\hat\\sigma_2^2}{n_1+n_2-2}.\\]\n\nExercise 2.9 Suppose that \\(X_1,\\ldots, X_{20}\\sim \\mathcal{N}(\\mu_1,\\sigma^2)\\) and \\(Y_1,\\ldots, Y_{20}\\sim \\mathcal{N}(\\mu_2,\\sigma^2)\\), and are mutually independent. Compute \\({\\textrm{Var}}\\left[\\bar X-\\bar Y\\right]\\).\n\n\nSolution 2.3. Using independence of \\(\\bar X\\) and \\(\\bar Y\\) and the result of the Exercise 2.6 , we have that \\[{\\textrm{Var}}\\left[\\bar X-\\bar Y\\right]={\\textrm{Var}}\\left[\\bar X\\right]+{\\textrm{Var}}\\left[\\bar Y\\right]=\\sigma_1^2/n_1+\\sigma_1^2/n_2.\\]\n\nFirst, we write down the null and alternative hypothesis: \\[H_0\\colon \\Delta=0\\qquad vs.. \\qquad H_1\\colon \\Delta\\neq 0.\\] Here, we can do a two sample \\(t\\)-test.\nRecall that the pooled variance is given by: \\[\\hat\\sigma_p^2=\\frac{(n_1-1)\\hat\\sigma_1^2+(n_2-1)\\hat\\sigma_2}{(n_1+n_2-2)}\\] We previously said that a multiple of a one sample standard deviation follows a Chi-squared distribution. It follows that \\((n_1-1)\\hat\\sigma_1^2/\\sigma^2\\sim \\chi^2_{n_1-1}\\) and \\((n_2-1)\\hat\\sigma_2^2/\\sigma^2\\sim \\chi^2_{n_2-1}\\). Using the theory from here, specifically, \\((n_1-1)\\hat\\sigma_1^2/\\sigma^2+(n_2-1)\\hat\\sigma_2^2/\\sigma^2\\) is a sum of independent Chi-squared random variables, and so we have \\((n_1-1)\\hat\\sigma_1^2/\\sigma^2+(n_2-1)\\hat\\sigma_2^2/\\sigma^2\\sim \\chi^2_{n_1+n_2-2}\\).\nAgain, using the theory from here, under the null hypothesis, we have that \\[\\frac{\\bar X-\\bar Y}{\\hat\\sigma_p/\\sqrt{1/n_1+1/n_2}}=\\frac{(\\bar X-\\bar Y)/\\sigma\\sqrt{1/n_1+1/n_2}}{\\hat\\sigma_p/\\sigma\\sqrt{1/n_1+1/n_2}}\\sim t_{n_1+n_2-2}.\\]\nThis follows from 3 facts, first, letting \\(Z=(\\bar X-\\bar Y)/\\sqrt{{\\textrm{Var}}\\left[\\bar X-\\bar Y\\right]}\\), note that \\(Z\\sim \\mathcal{N}(0,1)\\). We have that \\[Z=(\\bar X-\\bar Y)/\\sqrt{{\\textrm{Var}}\\left[\\bar X-\\bar Y\\right]}=(\\bar X-\\bar Y)/\\sigma\\sqrt{1/n_1+1/n_2}.\\]\nNext, we said earlier that \\(\\bar X\\) is independent of \\(\\hat\\sigma_1\\) and \\(\\bar Y\\) is independent of \\(\\hat\\sigma_2\\). Now, recall that if two random variables are independent, then any function of them is also independent. In other words, if \\(X\\) and \\(Y\\) are independent, then for real functions \\(f\\) and \\(g\\), we have that \\(g(X)\\) is independent of \\(f(Y)\\). It follows that \\(\\bar X\\) is independent of \\(\\hat\\sigma_2\\) and \\(\\bar Y\\) is independent of \\(\\hat\\sigma_1\\). It follows that \\(\\bar X-\\bar Y\\) is independent of \\(\\hat\\sigma_p\\). Then, \\[\\frac{(\\bar X-\\bar Y)/\\sigma\\sqrt{1/n_1+1/n_2}}{\\hat\\sigma_p/\\sigma\\sqrt{1/n_1+1/n_2}}\\] is a ratio of a standard normal random variable and the square root of a Chi-squared random variable, divided by its degrees of freedom. Further, the numerator and denominator are independent. Therefore, the above quantity follows a \\(t\\) distribution with \\(n_1+n_2-2\\) degrees of freedom.\nThis means that if \\(\\left|\\frac{\\bar X-\\bar Y}{\\hat\\sigma_p/\\sqrt{1/n_1+1/n_2}}\\right|\\geq t_{n_1+n_2-2,1-\\alpha/2}\\), then we reject the null hypothesis.\nLet;s execute the test in R:\n\n# Normally, I will give you a dataset. Here I generate the data\nset.seed(440) \nfemale_ages=rnorm(20,28,4)\nmale_ages=rnorm(20,32,4)\n\n\n\n# Check for equal variance\nvar(female_ages)\n\n[1] 15.72805\n\nvar(male_ages)\n\n[1] 26.22371\n\n## Putting the data in a dataframe\ncbind(\"Age\"=c(female_ages,male_ages),\"Gender\"=rep(c(0,1),each=20))\n\n           Age Gender\n [1,] 37.19809      0\n [2,] 20.69693      0\n [3,] 27.80284      0\n [4,] 27.69463      0\n [5,] 29.53143      0\n [6,] 29.46190      0\n [7,] 30.41164      0\n [8,] 33.27790      0\n [9,] 22.65974      0\n[10,] 30.73540      0\n[11,] 34.08564      0\n[12,] 27.58077      0\n[13,] 23.26108      0\n[14,] 30.94523      0\n[15,] 31.52404      0\n[16,] 29.13246      0\n[17,] 26.95470      0\n[18,] 24.80749      0\n[19,] 28.60051      0\n[20,] 26.76294      0\n[21,] 25.94775      1\n[22,] 40.16080      1\n[23,] 25.58905      1\n[24,] 32.16780      1\n[25,] 29.87934      1\n[26,] 35.46593      1\n[27,] 35.71651      1\n[28,] 37.76510      1\n[29,] 27.23068      1\n[30,] 33.41994      1\n[31,] 40.43822      1\n[32,] 31.04841      1\n[33,] 32.66165      1\n[34,] 38.28678      1\n[35,] 34.72411      1\n[36,] 39.57994      1\n[37,] 26.85585      1\n[38,] 31.87533      1\n[39,] 23.71793      1\n[40,] 30.54803      1\n\ndf=data.frame(cbind(\"Age\"=c(female_ages,male_ages),\"Gender\"=rep(c(0,1),each=20)))\n\n#exploring the data\n#hist(x) creates a histogram of the vector x\n\nhist(df$Age[df$Gender==0])\n\n\n\n\n\n\n\nhist(df$Age[df$Gender==1])\n\n\n\n\n\n\n\n#boxplot creates boxplots of Age against gender\nboxplot(Age~Gender, df)\n\n\n\n\n\n\n\ntest=t.test(Age~Gender,data=df,var.equal=TRUE)\ntest\n\n\n    Two Sample t-test\n\ndata:  Age by Gender\nt = -2.7603, df = 38, p-value = 0.008841\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -6.929630 -1.065749\nsample estimates:\nmean in group 0 mean in group 1 \n       28.65627        32.65396 \n\n#Interpret the P value, and CI, what are we going to say to a stakeholder?\n\n#e.g.\ntest$estimate\n\nmean in group 0 mean in group 1 \n       28.65627        32.65396 \n\n\n\n\n\n\n\n\nNote\n\n\n\nNote also that we can use the confidence interval method, meaning that if if 0 is in the interval: \\[ \\hat\\Delta\\pm t_{n_1+n_2-2,1-\\alpha/2}\\sqrt{\\hat\\sigma_1^2/n_1+\\hat\\sigma_2^2/n_2},\\] then we fail to reject the null hypothesis.\n\n\n\n\n2.2.4 Homework stop 2\n\nExercise 2.10 IBM Human Resources (HR) department is evaluating job applicants from York University.\nThey are interested to know if the 2020 ITEC graduating class has an average GPA higher than 6 (i.e. average GPA higher than ``B’’). They collected the GPA of 25 ITEC students graduated in 2020.\n\n\n\n\n4.92\n4.79\n6.76\n5.64\n6.12\n7.37\n6.45\n6.31\n6.68\n\n\n6.30\n4.91\n6.95\n5.87\n6.18\n6.60\n6.71\n6.69\n5.62\n\n\n6.40\n5.51\n6.44\n6.13\n8.55\n7.94\n4.78\n-\n-\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse chatGPT to convert the above table to an R vector, so you don’t have to waste time!\n\n\n\nFor the one sample testing problem, i.e., you have a sample of \\(n\\) normal random variables, with unknown mean and variance and you want to test whether \\(H_0\\colon \\mu=0\\) vs. \\(H_0\\colon \\mu\\neq 0\\), show that \\(\\frac{\\bar X}{\\hat\\sigma/\\sqrt{n}}\\sim t_{n-1}\\) under the null hypothesis.\nWhat is the distribution of each of the following: \\(\\bar X,\\bar Y,\\hat\\sigma\\) under the assumption of normal data with unknown mean and variance?\n\nCompare and contrast the following concepts. That is, define them and explain the difference between them.\n\nSample vs. Population\nObservation vs. Random variable\nStatistic vs. Parameter\nEstimate vs. Estimator",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Review material</span>"
    ]
  },
  {
    "objectID": "Unit_1.html#review-of-matrices-and-linear-algebra",
    "href": "Unit_1.html#review-of-matrices-and-linear-algebra",
    "title": "2  Review material",
    "section": "2.3 Review of matrices and linear algebra",
    "text": "2.3 Review of matrices and linear algebra\nRecall that\n\nDefinition 2.5 An \\((n \\times m)\\) matrix \\(A\\) takes the form \\[\\begin{eqnarray*}\nA & = & \\left( \\begin{array}{cccc}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm} \\\\\n\\end{array} \\right) \\\\\n&=& (( \\ a_{ij} \\ )) \\hspace{0.5in} i=1, \\ldots, n, \\ \\ j=1, \\ldots, m\n\\end{eqnarray*}\\] and \\(a_{ij}\\) is the element in the \\(i^{th}\\) row and \\(j^{th}\\) column of the matrix \\(A\\)\n\nWe also define the following:\n\nAn \\((n \\times 1)\\) matrix is also known as a \\(n\\) dimensional column vector. Note: in this course, a vector means a column vector.\nA \\((1 \\times m)\\) matrix is also known as a \\(m\\) dimensional row vector\nThe \\(n\\) dimensional one vector, \\(1_n\\), (sometimes the subscript \\(n\\) is suppressed when the dimension is obvious), is an \\(n\\) dimensional column vector with all entries being \\(1\\).\nThe \\((n \\times n)\\) identity matrix, \\(I_n\\), is the \\((n \\times n)\\) matrix with diagonal entries set equal to 1 and the off diagonal entries set equal to 0\n\nThroughout this section, we will use the following matrices to demonstrate the numerical calculations: \\[\nU = \\left( \\begin{array}{ccc} 1 & 2 & 3 \\\\ -1 & 4 & -2 \\end{array} \\right), \\\nV = \\left( \\begin{array}{cc} 2 & 4 \\\\ 1 & -2 \\\\ -1 & 0 \\end{array} \\right), \\ k = 4\n\\]\n\n2.3.1 Matrix properties\nFirst, we define the transpose of a matrix:\n\nDefinition 2.6 Let \\(A = (( \\ a_{ij} \\ ))\\) for \\(i=1, \\ldots, n\\) and \\(j=1, \\ldots, m\\), is an \\((n \\times m)\\) matrix. Then \\(A^\\top = A\\) transpose \\(=  (( \\ a_{ji} \\ ))\\) for \\(j=1, \\ldots, m\\) and \\(i=1, \\ldots, n\\), and \\(A^\\top\\) is an \\((m \\times n)\\) matrix.\n\nWhen we transpose a matrix \\(A\\), the rows of \\(A\\) becomes the columns of \\(A^\\top\\) and the columns of \\(A\\) becomes the rows of \\(A^\\top\\).\n\nExample 2.1 Using our example matrices, we have that \\[\nU^\\top = \\left( \\begin{array}{cc} 1 & -1 \\\\ 2 & 4 \\\\ 3 & -2 \\end{array} \\right), \\\nV^\\top = \\left( \\begin{array}{ccc} 2 & 1 & -1 \\\\ 4 & -2 & 0 \\end{array} \\right)\n\\]\n\n\nDefinition 2.7 Let \\(A = (( \\ a_{ij} \\ ))\\) and \\(B = (( \\ b_{ij} \\ ))\\) be two \\((n \\times m)\\) matrices. Then \\[\nA \\pm B = (( \\ a_{ij} \\pm \\ b_{ij} \\ )).\n\\]\n\nAddition and subtraction of matrices required the matrices to have the same dimension.\n\nExample 2.2 Using our example matrices, we have that: \\(U + V\\) is undefined because they are not of the same dimension, and \\[\nU + V^\\top = \\left( \\begin{array}{ccc} 1+2 & 2+1 & 3+(-1) \\\\ (-1)+4 & 4 + (-2) & (-2)+0\n\\end{array} \\right) =\n\\left( \\begin{array}{ccc} 3 & 3 & 3 \\\\ 3 & 2 & -2 \\end{array} \\right)\\]\n\n\nDefinition 2.8 Let \\(A = (( \\ a_{ij} \\ ))\\) for \\(i=1, \\ldots, n\\) and \\(j=1, \\ldots, m\\), is an \\((n \\times m)\\) matrix and \\(k\\) is a constant. Then \\[\nkA = (( \\ k a_{ij} \\ )) = Ak,\n\\] i.e. each element of the matrix \\(A\\) is multiplied by \\(k\\).\n\n\nExample 2.3 Using our example matrices, we have that: \\[\nkU^\\top = 4 \\left( \\begin{array}{cc} 1 & -1 \\\\ 2 & 4 \\\\ 3 & -2 \\end{array} \\right)\n= \\left( \\begin{array}{cc} 4(1) &4(-1) \\\\ 4(2) & 4(4) \\\\ 4(3) & 4(-2) \\end{array} \\right)\n= \\left( \\begin{array}{cc} 4 & -4 \\\\ 8 & 8 \\\\ 12 & -2 \\end{array} \\right)\n\\]\n\n\nDefinition 2.9 Let \\(A\\) and \\(B\\) be two matrices. Then \\(A\\) multiplied by \\(B\\), \\(AB\\), is defined only if (number of columns of \\(A\\)) \\(=\\) (number of rows of \\(B\\)).\nThe product is a \\(\\left( \\right.\\) (number of rows of \\(A\\)) \\(\\times\\) (number of columns of \\(B\\)) \\(\\left. \\right)\\) matrix.\nMore precisely, let \\(A = (( \\ a_{ij} \\ ))\\) be an \\((n \\times m)\\) matrix and \\(B = (( \\ b_{ij} \\ ))\\) be an \\((m \\times p)\\) matrix. Then \\(C = AB = (( \\ c_{ij} \\ ))\\) is an \\((n \\times p)\\) matrix with \\[\nc_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \\cdots + a_{im}b_{mj}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nIn matrix algebra, \\(AB\\) is not necessarily equal to \\(BA\\).\n\n\n\nExample 2.4 Using our example matrices, we have that: \\[\\begin{eqnarray*}\nU V & = & \\left( \\begin{array}{ccc} 1 & 2 & 3 \\\\ -1 & 4 & -2 \\end{array} \\right)\n\\left( \\begin{array}{cc} 2 & 4 \\\\ 1 & -2 \\\\ -1 & 0 \\end{array} \\right) \\\\\n& = & \\left( \\begin{array}{cc}\n1(2) + 2(1) + 3(-1) & 1(4) + 2(-2) + 3(0) \\\\\n(-1)(2) + 4(1) + (-2)(-1) & (-1)(4) + 4(-2) + (-2)(0)\n\\end{array} \\right) \\\\\n& = & \\left( \\begin{array}{cc} 1 & 0 \\\\ 4 & -12 \\end{array} \\right)\n\\end{eqnarray*}\\]\n\nAssume all the matrix multiplication works. Let \\(I_n\\) be an \\((n \\times n)\\) identity matrix. Then \\[\nAI_n = A, \\ \\ \\ and \\ \\ \\ I_nB = B.\n\\]\n\nDefinition 2.10 Let \\(A\\) be an \\((n \\times n)\\) matrix. The inverse of \\(A\\), \\(A^{-1}\\), if exists satisfies \\[\nA A^{-1} = A^{-1} A = I_n\n\\] and if \\(A^{-1}\\) does not exist, then \\(A\\) is a singular matrix.\n\n\n\n\n\n\n\nImportant\n\n\n\nFrom your linear algebra course, a prerequisite, you have learned the condition(s) for the existence of an inverse, (https://mathworld.wolfram.com/InvertibleMatrixTheorem.html)[The Invertible Matrix Theorem] and you have learned how to obtain an inverse. You should review them.\nSpecifically, you should know how to obtain inverse of any diagonal matrix and any \\((2 \\times 2)\\) non-singular matrix, i.e., \\[\n\\left( \\begin{array}{cc} a & b \\\\ c & d \\end{array} \\right)^{-1} =\n\\frac{1}{ad - bc} \\left( \\begin{array}{cc} d & -b \\\\ -c & a \\end{array} \\right).\n\\]\n\n\n\nExample 2.5 Using our example matrices, let \\[\nW = UV = \\left( \\begin{array}{cc} 1 & 0 \\\\ 4 & -12 \\end{array} \\right)\n\\] Then \\[\nW^{-1} = \\frac{1}{1(-12) - 0(4)}\n\\left( \\begin{array}{cc} -12 & 0 \\\\ -4 & 1 \\end{array} \\right) =\n\\left( \\begin{array}{cc} 1 & 0 \\\\ 1/3 & -1/12 \\end{array} \\right)\n.\\]\nYou can verify that \\(W W^{-1} = W^{-1} W = I_2\\).\n\n\n\n2.3.2 Important identities\nLastly, we introduce some important identities:\n\\[\nX = \\left( \\begin{array}{cc} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{array} \\right), \\ \\ \\\nand \\ \\ \\\ny = \\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right)\n\\] Then \\[\nX^\\top X = \\left( \\begin{array}{cc} n & \\sum_{i=1}^n x_i \\\\\n\\sum_{i=1}^n x_i & \\sum_{i=1}^n x_i^2 \\end{array} \\right), \\ \\ \\ {\\textrm{and}} \\ \\ \\\nX^\\top y = \\left( \\begin{array}{c} \\sum_{i=1}^n y_i \\\\ \\sum_{i=1}^n x_i y_i\n\\end{array} \\right).\n\\] Also \\(\\bar{y} = \\frac{1}{n} 1^\\top y\\) and \\(\\sum_{i=1}^n y_i = n \\bar{y}\\) \\ Finally \\(\\sum_{i=1}^n (y_i - \\bar{y})^2 = \\sum_{i=1}^n y_i^2 - n \\bar{y}^2\\)\nThese are useful identities that we will use throughout this course.\nLastly, we recall an important application of matrices. An application of matrices: Suppose that we want to solve for \\(x_1, x_2, x_3\\) where they satisfy the following set of linearequations:\n\\[\\begin{eqnarray*}\n2 x_1 + 3 x_2 - 4 x_3 & = & 0 \\\\\n-x_1 + 4 x_2 & = & -1 \\\\\n5 x_1 + x_2 - 2 x_3 & = & 4\n\\end{eqnarray*}\\]\nWe can set it up in matrix form as follows: \\[\n\\left( \\begin{array}{rrr} 2 & 3 & -4 \\\\ -1 & 4 & 0 \\\\ 5 & 1 & -2 \\end{array} \\right)\n\\left( \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\end{array} \\right) =\n\\left( \\begin{array}{c} 0 \\\\ -1 \\\\ 4  \\end{array} \\right)\n\\] Or it can be presented as \\(A x = b\\). If \\(A\\) is not a singular matrix, then \\(x= A^{-1} b\\). Since \\(det(A) = 62\\), it is not a singular matrix. Solving the above equation this using \\(x= A^{-1} b\\) yields that \\(x = (1, 0, 0.5)^\\top\\).\nKeep this in mind, we will see it return in the next chapter.\n\n\n2.3.3 Homework stop 3\n\nExercise 2.11 Let \\[\nW =  \\left( \\begin{array}{cc} 3 & 2 \\\\ -4 & 6 \\end{array} \\right)\n\\] and \\(x=(2,1)^\\top\\). Compute \\(W^{-1}\\), \\(xx^\\top\\) and \\(x^\\top W\\). Verify that \\(W W^{-1} = W^{-1} W = I_2\\).\n\n\nProve each of the important identities.\nVerify \\(X^\\top A=(A^\\top X)^\\top\\).\nWhat is the rank of a matrix? Is a matrix’s rank related to whether or not a matrix is invertible? Why?\nDefine a positive definite matrix. When is \\(X^\\top X\\) positive definite?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Review material</span>"
    ]
  },
  {
    "objectID": "Unit_1.html#review-random-vectors",
    "href": "Unit_1.html#review-random-vectors",
    "title": "2  Review material",
    "section": "2.4 Review Random Vectors",
    "text": "2.4 Review Random Vectors\n\n2.4.1 Definition of random vectors\n\nDefinition 2.11 Let \\(Y_1, \\cdots, Y_n\\) be random variables. Then \\[\nY = \\left( \\begin{array}{c} Y_1 \\\\ \\vdots \\\\ Y_n \\end{array} \\right)\n\\] is an \\(n\\)-dimensional random vector.\n\nSimilar to a random variable, a random vector also comes with a probability mass function (if all the \\(Y_i\\) are discrete) or a probability density function (if all the \\(Y_i\\) are continuous), or a ``mixture’’ distribution (if some \\(Y_i\\) are discrete and others are continuous). In general, a random vector is drawn from a multivariate distribution, defined by the PMF or PDF. Just as before, the PMF and PDF range is non-negative, the PMF sums to 1 over all outcomes, and the PDF integrates to 1 over \\(\\mathbb{R}^n\\). One discrete multivariate distribution you have learned in 1131 is the Multinomial distribution. We will learn about the multivariate normal distribution soon.\n\n\n2.4.2 Expected Value and Covariance\n\nDefinition 2.12 Let \\(Y\\) be an \\(n\\)-dimensional random vector, then the mean (expected value) of \\(Y\\) is defined as \\[\n{\\textrm{E}}(Y) = \\left( \\begin{array}{c} {\\textrm{E}}(Y_1) \\\\ \\vdots \\\\ {\\textrm{E}}(Y_n) \\end{array} \\right) =\n\\mu\n\\] and the covariance of \\(Y\\) is defined as \\[\n{\\textrm{cov}}\\left[Y\\right] = {\\textrm{E}}[ (Y - \\mu) (Y - \\mu)^\\top ] =\n(( {\\textrm{cov}}\\left[Y_i, Y_j\\right] \\ )) = \\Sigma.\n\\]\n\nSometimes \\({\\textrm{cov}}\\left[Y\\right]\\) is written as \\({\\textrm{Var}}\\left[Y\\right]\\).\nThe following are some facts about \\(\\Sigma\\):\n\\(\\Sigma\\) is an \\(n \\times n\\) matrix with the diagonal elements being the variances, \\({\\textrm{Var}}\\left[Y_i\\right]\\) for \\(i=1, \\ldots, n\\), and the off-diagonal elements being the covariances, \\({\\textrm{cov}}\\left[(\\right]Y_i, Y_j)\\) for \\(i, j = 1, \\ldots, n\\) and \\(i \\ne j\\). \\(\\Sigma\\) is a symmetric, non-negative definite matrix. In this course, we further restrict it to be a positive definite matrix. \\(\\Sigma\\) is referred to as the covariance matrix.\n\n\n2.4.3 Properties of expected value and covariance\nLet \\(Y\\in \\mathbb{R}^{d}\\) be a random vector with \\(A\\in \\mathbb{R}^d\\) and \\(B\\in \\mathbb{R}^{n\\times d}\\) be matrices. It holds that\n\n\\({\\textrm{E}}(A + B Y) = A + B {\\textrm{E}}(Y)\\)\n\\({\\textrm{cov}}\\left[A + B Y\\right] = B {\\textrm{cov}}\\left[Y\\right] B^\\top\\).\n\n\nExercise 2.12 Let $Y = (Y_1, , Y_n)^$ be a random vector, where \\(Y_i\\) are i.i.d.\nrandom variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\). What are the mean and covariance of \\(Y\\)? Use properties of random vectors to compute the mean and variance of the sample mean.\n\n\nSolution 2.4. First, \\({\\textrm{E}}(Y) = \\mu 1\\) and \\({\\textrm{cov}}\\left[Y\\right] = \\sigma^2 I\\).Note that \\(\\bar{Y} = (Y_1 + \\cdots + Y_n)/n = \\frac{1}{n}1^\\top Y\\). Now, we have \\[\n{\\textrm{E}}(\\bar{Y}) = {\\textrm{E}}\\left( \\frac{1}{n}1^\\top Y \\right) =\n\\frac{1}{n} \\left( 1^\\top {\\textrm{E}}(Y) \\right) = \\frac{1}{n} (n \\mu) = \\mu\n\\] and,\n\\[\\begin{align*}\n{\\textrm{cov}}\\left[\\bar{Y}\\right] &={\\textrm{cov}}\\left[ \\frac{1}{n}1^\\top Y\\right]\\\\\n&=\n\\left( \\frac{1}{n} \\right)^2\n\\left( 1^\\top {\\textrm{cov}}\\left[Y\\right]  1 \\right) \\\\\n&=\n\\left( \\frac{1}{n} \\right)^2 (n \\sigma^2) = \\frac{\\sigma^2}{n}.\n\\end{align*}\\]\n\n\n\n2.4.4 Multivariate normal distribution\nWe say that a random vector \\(X\\sim \\mathcal{N}_d(\\mu,\\Sigma)\\) follows a multivariate normal distribution if \\(X\\) has PDF: \\[\n\\phi(\\mathbf{x})=\\left(\\frac{1}{2 \\pi}\\right)^{d / 2}|\\Sigma|^{-1 / 2} \\exp \\left\\{-\\frac{1}{2}(\\mathbf{x}-\\mu)^{\\prime} \\Sigma^{-1}(\\mathbf{x}-\\mu)\\right\\}.\n\\]\nIf \\(X\\sim \\mathcal{N}_d(\\mu,\\Sigma)\\) and \\(c\\in \\mathbb{R}^d\\), \\(A\\in \\mathbb{R}^{m\\times d}\\) then:\n\n\\(AX\\sim \\mathcal{N}(A\\mu,A\\Sigma A^\\top)\\).\n\\(c^\\top X\\sim \\mathcal{N}(c^\\top\\mu,c^\\top\\Sigma c)\\).\nAny conditional distribution for a subset of the variables conditional on another subset of variables is a multivariate distribution.\n\nUsing random vectors is a simple way of deriving lots of equations for this course. Working with vectors also allows those who are “geometrically gifted” to view the whole regression concepts geometrically! If not, not to worry!\n\n\n2.4.5 Homework stop 4\n\nExercise 2.13 For a (full-rank) matrix \\(X\\in \\mathbb{R}^{n\\times p}\\) with \\(n&gt;p\\), and random vector \\(Y\\in \\mathbb{R}^{n\\times 1}\\) with mean \\(\\mu\\) and covariance \\(\\Sigma\\), compute the following:\n\nExpected value and covariance of \\((X^\\top X)^{-1}X^\\top Y\\)\nExpected value of \\(Y^\\top Y\\)\nExpected value and covariance of \\(X^\\top X\\)\nExpected value and covariance of \\(X(X^\\top X)^{-1}X^\\top Y\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Review material</span>"
    ]
  },
  {
    "objectID": "Unit_2.html",
    "href": "Unit_2.html",
    "title": "3  Linear Regression",
    "section": "",
    "text": "3.1 Basics of linear regression\nBy the end of this section, you should be able to say what the linear and normal linear regression models are. As well as what it means to assume either of these models.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "Unit_2.html#basics-of-linear-regression",
    "href": "Unit_2.html#basics-of-linear-regression",
    "title": "3  Linear Regression",
    "section": "",
    "text": "3.1.1 The linear regression model\nConsider the following example.\n\nExample 3.1 It is difficult to accurately determine a person’s body fat percentage without immersing them in water. However, we can easily obtain the weight of a person. A researcher would like to know if weight and body fat percentage are related? If so, for a given weight, can the person’s body fat percentage be predicted? If so, how accurate is the prediction? This researcher collected the following data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndividual\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nWeight (lb)\n175\n181\n200\n159\n196\n192\n205\n173\n187\n188\n\n\nBody Fat (%)\n6\n21\n15\n6\n22\n31\n32\n21\n25\n30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndividual\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n\nWeight (lb)\n188\n240\n175\n168\n246\n160\n215\n159\n146\n219\n\n\nBody Fat (%)\n10\n20\n22\n9\n38\n10\n27\n12\n10\n28\n\n\n\nHow can we (as statisticians / data scientists) answer the questions raised by the researcher?\n\nThe first thing we might do is explore the data:\n\n##################### Exploratory analysis\n\n# Make the data frame\nWeight=c(175 , 181 , 200 , 159 , 196 , 192 , 205 , 173 , 187 , 188 , \n         188 , 240 , 175 , 168 , 246 , 160 , 215 , 159 , 146 , 219 )\nBodyFat =c(6 , 21 , 15 , 6 , 22 , 31 , 32 ,21 , 25 , 30 , \n           10 , 20 , 22 , 9 , 38 , 10 , 27 , 12 , 10 , 28 )\n\ndf=data.frame(cbind(Weight=Weight,BodyFat=BodyFat))\n\n# make some histograms\nhist(df$Weight,freq=F)\n\n\n\n\n\n\n\nhist(df$BodyFat,freq=F)\n\n\n\n\n\n\n\n# print summary statistics\nsummary(df)\n\n     Weight         BodyFat     \n Min.   :146.0   Min.   : 6.00  \n 1st Qu.:171.8   1st Qu.:10.00  \n Median :187.5   Median :21.00  \n Mean   :188.6   Mean   :19.75  \n 3rd Qu.:201.2   3rd Qu.:27.25  \n Max.   :246.0   Max.   :38.00  \n\n# There seems to be some relationship here\nplot(df)\n\n\n\n\n\n\n\n# Here is the correlation matrix, notice it is high!\ncor(df)\n\n           Weight   BodyFat\nWeight  1.0000000 0.6966328\nBodyFat 0.6966328 1.0000000\n\n\nWe have observed that there is a relatively strong linear relationship between these two variables. What next? We might ask, what is this relationship precisely?\nIn particular, note that we have observed a sample of vectors \\((Y_1,X_1),\\ldots,(Y_1,X_n)\\). Now, we want to say something about the relationship between \\(X\\) and \\(Y\\) in general. One way to do that is to suppose at the population level that \\[{\\textrm{E}}\\left[Y|X\\right]=f(X).\\] That is, on average, \\(Y\\) is equal to \\(f(X)\\). One way to do that is to assume that \\(Y|X=f(X)+\\epsilon,\\) where \\(\\epsilon\\) is a random variable that satisfies \\({\\textrm{E}}\\left[\\epsilon\\right]=0\\). This assumption means that, for each \\(Y_i\\), given \\(X_i\\), we have that \\(Y_i=f(X_i)+\\epsilon_i\\). Note that we do not observe \\(\\epsilon_i\\), but we can assume it exists. We can read this as \\(Y_i\\) is equal to \\(f(X_i)\\), plus some random, individual error \\(\\epsilon_i\\). The next step is to use the data to determine \\(f\\).\nUsing the data analysis steps from the Introduction we can write out the first few steps:\n\nQuestion about a population: ``How can we use weight to determine body fat percentage?’’\nData: \\((Y_1,X_1),\\ldots,(Y_{20},X_{20})\\), \\((Y_{i},X_{i})\\) are the body fat percentage and weight of individual \\(i\\in [20]\\).\n\nWe have explored the data with graphs and summary statistics. Now, we have posited the model \\(Y|X=f(X)+\\epsilon\\). Letting \\(f\\) be any function is too general. In fact, we can use the data to learn more about what \\(f\\) might be. Recall that earlier, we saw the scatter plot, where it looked like there was a linear relationship, (with some error), between \\(Y\\) and \\(X\\). (We can draw a straight line through the middle of the data.)\nLet’s make some assumptions that make the statistical analysis easier:\n\nAssume that \\(\\forall i\\in[20]\\), it holds that \\[Y_i|X_i=\\beta_0+\\beta_1 X_i+\\epsilon_i.\\] This means that we assume that \\(f\\) is a line.\nNext, we assume \\(\\forall i\\in[20]\\), \\({\\textrm{E}}\\left[\\epsilon_i\\right]=0\\) and \\({\\textrm{Var}}\\left[\\epsilon_i\\right]=\\sigma^2\\). That is, the random error have the same mean and variance for each individual. In addition, the random errors average to 0.\nWe also assume that the individuals’ Body fat percentage, weights and random errors are independent, that is, \\(\\epsilon_i\\perp \\epsilon_j\\) for \\(i\\neq j,\\ i,j\\in [20]\\).\n\nThis is the simple linear regression model. That is, the simple linear regression model is the set of assumptions 1-3 given above.\nIt is often also assumed:\n\n\\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma^2)\\),\n\nbut not always. Including the normality assumption is known as the simple normal linear regression model.\nIn general, a model is a set of assumptions about a population. The particular set of assumptions 1-3 is the simple linear regression model.\nThe following is some terminology used in regression analysis:\n\nHere, \\(Y_i\\) is the response variable, also known as the dependent variable, or the outcome variable.\nHere, \\(X_i\\) is the covariate, also known as the explanatory variable, or the independent variable.\n\nGiven a “question about a population” which involves regression, you should immediately identify the response variable and the covariates.\nNow, how can we interpret this model? That is, what does it mean to assume this model?\nFirst, observe that we assume that \\({\\textrm{E}}\\left[Y|X\\right]\\) is a line. This means there is a linear relationship between the average body fat percentage and weight.\nNext, observe that for any individual, their actual body fat percentage is given by \\(Y={\\textrm{E}}\\left[Y|X\\right]+\\epsilon_i=\\beta_0+\\beta_1 X_i+\\epsilon_i\\). Therefore, their body fat percentage will not fall exactly on the line \\(\\beta_0+\\beta_1 X_i\\). Rather, it will fall above or below the line, depending on \\(\\epsilon_i\\). Furthermore, if we assume that \\(\\epsilon_i\\sim \\mathcal{N}(0,\\sigma^2)\\), then we know from the properties of the Normal distribution that this random error will not exceed \\(2\\sigma\\) with high probability. Therefore, most of the time, an individual’s body fat percentage will fall within \\(2\\sigma\\) of the line.\nThird, notice that this quantity, \\(2\\sigma\\), does not depend on \\(X\\). That is, for any weight, we still expect an individual’s body fat percentage to be within \\(2\\sigma\\) of the line, regardless of the value of weight.\nFourth, if we knew \\(\\beta_0,\\beta_1\\), then given someone’s weight, we could try to predict their body fat percentage given their weight. That is, we could calculate the expected body fat \\({\\textrm{E}}\\left[Y|X\\right]\\). There would still be their individual random error \\(\\epsilon\\), so we would not be able to predict it exactly. However, if \\(\\sigma^2\\) isn’t too big, then we could produce an accurate prediction.\nTherefore, if the model assumptions are correct, we assume there exists some line, around which the body fat percentages are scattered uniformly.\nNext, we will simulate data from the normal simple linear regression model to gain a better understanding of this model. Suppose that \\(\\beta_0=-15\\), \\(\\beta_1=.2\\) and \\(\\sigma=5\\). Then we would observe the following.\n\n############################ Simulation\nset.seed(3252)\n\n# Suppose that beta_0=-15 and beta_1=0.2 and sigma=5, \n# then we would have that the mean function E(Y|X) is given by the following line:\ncurve(-15+.2*x,125,280,lwd=3,xlab=\"Weight\",ylab=\"Expected BF % (E(Y|X))\")\n\n\n\n\n\n\n\n# Next, let's simulate some body weights from the uniform distribution\nWeight2=runif(20,135,250)\n\n# Then, we can simulate the population body fat percentages according to the model as follows:\n\n# Simulating 20 values of the random error, \nepsilons=rnorm(n=20,mean=0,sd=5)\n\n# Computing the simulated Body fat percentages: \nBfs=-15+.2*Weight2+epsilons\n\n# Plot the simulated values, and the mean function\ncurve(-15+.2*x,125,280,lwd=3,xlab=\"Weight\",ylab=\"BF %\")\npoints(Weight2,Bfs,pch=22,bg=2)\n\n\n\n\n\n\n\n\nNotice how the data are scatted around the line uniformly? This is what data from a simple linear regression model looks like. Try changing the value in set.seed() and re-running the code. Notice how the data changes, but it is always scatted around the line uniformly? This is what we expect to see if the data follow a simple linear regression model.\nNotice how the data simulated from our model appears similar to the body fat percentage and weights data we observed? That means this model (set of assumptions) is a good fit for our data.\n\n\n\n\n\n\nCaution\n\n\n\nIn this model, and in regression in general, the response \\(Y\\) is not exactly equal to some function of \\(X\\) given by \\(f(X)\\). The model assumes that on average \\(Y=f(X)\\). Therefore, knowing someones “\\(X\\)” value will not exactly give us their \\(Y\\) value, but it would give us a good guess at it. The error \\(\\epsilon\\) is used to model the fact that someones “\\(X\\)” value will not exactly give us their \\(Y\\) value. Notice above how the actual points are scatted around the line, and not exactly equal to it! This is due to the errors \\(\\epsilon\\).\n\n\n\n\n3.1.2 The multiple linear regression model\nBut what about matrices? Why did we study matrices then? We can write the regression model in terms of matrices and vectors, to make it more compact.\nNow, recall \\[Y_i|X_i=\\beta_0+\\beta_1 X_i+\\epsilon_i,\\] with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma^2)\\). It is more convenient mathematically to let \\(\\mathbf{Y}=(Y_1,\\ldots,Y_n)^\\top\\), \\[\\mathbf{X}=\\begin{bmatrix}\n        1 & X_1\\\\\n        \\vdots & \\vdots\\\\\n        1 & X_n\\\\\n    \\end{bmatrix}=\\left[ 1_n\\ |\\  (X_1,\\ldots,X_n)^\\top\\right],\\] \\(\\beta=(\\beta_0,\\beta_1)^\\top\\) and \\(\\mathbf{\\epsilon}=(\\epsilon_1,\\ldots,\\epsilon_n)^\\top\\). Then we can write \\[\\mathbf{Y}|\\mathbf{X}=\\mathbf{X}\\beta+\\mathbf{\\epsilon}.\\] Often, we overload the notation \\(Y\\), and use \\(Y\\) instead of \\(\\mathbf{Y}\\), and \\(X\\) instead of \\(\\mathbf{X}\\).\nThis form allows us to go beyond one explanatory variable very easily! Just add one column to \\(X\\) and one entry to \\(\\beta\\) for each new variable. Observe the following model: \\[Y_i|(X_{i1},\\ldots,X_{ik})=\\beta_0+\\beta_1 X_{i1}+\\ldots+\\beta_k X_{ik}+\\epsilon_i,\\] with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma^2)\\) and \\(\\epsilon_i\\perp \\epsilon_j\\) for \\(i\\neq j,\\ i,j\\in [n]\\). This is known as the multiple linear regression model (MLR), or just the linear regression model for short. We can write this model in the same for as above: Let \\[\\mathbf{X}=\\begin{bmatrix}\n        1 & X_{11}&X_{1k}\\\\\n        \\vdots & \\vdots &\\vdots\\\\\n        1 & X_{n1}\\ldots &X_{nk}\\\\\n    \\end{bmatrix},\\] and \\(\\beta=(\\beta_0,\\ldots,\\beta_k)^\\top\\). Then we can write the MLR as \\[\\mathbf{Y}|\\mathbf{X}=\\mathbf{X}\\beta+\\mathbf{\\epsilon},\\] where \\({\\textrm{E}}\\left[(\\right]\\epsilon)=0\\) and \\({\\textrm{Var}}\\left[\\epsilon\\right]=\\sigma^2 I\\). Notice how compact this is! As in the simple case, there is also the normal MLR, which further assumes that \\(\\epsilon\\sim\\mathcal{N}(0,\\sigma^2 I)\\).\nWe can then study the mathematical properties of \\[Y|X=X\\beta+\\epsilon\\] for general but fixed \\(k\\), under the normal or vanilla MLR, which will cover many models.\n\n\n3.1.3 Homework stop 1\n\nExercise 3.1 Try adjusting the parameters \\(\\beta_0,\\beta_1,\\sigma\\) in the simulation, what happens to the data? What happens to the line?\n\n\nExercise 3.2 Is \\(\\beta\\) an estimate or a population parameter? Why?\n\n\nExercise 3.3 Come up with another possible form of \\(f\\) that is not linear. Adjust the simulation to include this form of \\(f\\).\n\n\nExercise 3.4 Write down the assumptions of the MLR and the normal MLR. What is the difference between the two models?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "Unit_2.html#least-squares",
    "href": "Unit_2.html#least-squares",
    "title": "3  Linear Regression",
    "section": "3.2 Least Squares",
    "text": "3.2 Least Squares\nNow that we have settled on a model for the population, the next step is to use the data to estimate the model parameters. In particular, we need to estimate \\(\\beta\\). That will allow us to estimated \\({\\textrm{E}}\\left[Y|X\\right]\\) for any value of \\(X\\).\nRecall that we want to study the population model: \\[Y|X=X\\beta+\\epsilon.\\]\n\n3.2.1 Notation\nFor the model \\(Y|X=X\\beta+\\epsilon\\), we have\n\n\\(Y\\in \\mathbb{R}^n\\) is the response variable (a continuous random variable).\n\\(X\\in \\mathbb{R}^{n\\times p}\\) is the covariate matrix (Note that the first column is often \\(1_n\\)).\n\\(X_i\\in \\mathbb{R}^p\\) is the \\(i^{th}\\) observed explanatory variable \\((i=1, \\ldots, n)\\) (not a random variable, in the sense that we condition on it).\n\\(\\beta\\in \\mathbb{R}^{p\\times 1}\\) is the coefficient vector .\n\\(\\epsilon\\in\\mathbb{R}^n\\) is the random error (continuous random variable) .\n\nWe may also refer to the actual observed values (versus the abstract mathematical concept of a random variable) as follows:\n\n\\(y=(y_1,\\ldots,y_n)^\\top\\in \\mathbb{R}^n\\) is the observed response variable (fixed/observed)\n\\(x_{ij}\\) is the \\(i^{th}\\) observation of the \\(j^{th}\\) explanatory variable (fixed/observed) Data:\n\n\n\n\nObservation\nObserved data point\n\n\n\n\n1\n\\((y_1, x_{11}, x_{12}, \\ldots x_{1p})\\)\n\n\n2\n\\((y_2, x_{21}, x_{22}, \\ldots x_{2p})\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\nn\n\\((y_n, x_{n1}, x_{n2}, \\ldots x_{np})\\)\n\n\n\nWe posit that \\[Y|X=X\\beta+\\epsilon,\\]\nwhere we assume that\n\n\\(\\forall i\\in[n]\\), \\({\\textrm{E}}\\left[\\epsilon_i\\right]=0\\).\n\\(\\forall i\\in[n]\\), \\({\\textrm{Var}}\\left[\\epsilon_i\\right]=\\sigma^2\\) (constant variance and is also known as homogeneity.)\nWe also would assume that \\(\\epsilon_i\\perp \\epsilon_j\\) for \\(i\\neq j,\\ i,j\\in [n]\\).\n\\(\\beta\\in \\mathbb{R}^{p\\times 1}\\) is the unknown, population coefficient vector.\n\\(X\\in \\mathbb{R}^{n\\times p}\\) is a covariate matrix.\n\nLet’s talk about \\(\\beta\\). How do we interpret \\(\\beta\\)? Suppose we know \\(\\beta\\). Then:\nNote that \\[{\\textrm{E}}\\left[Y_i|X_{i}\\right]={\\textrm{E}}\\left[\\beta^\\top X_i+\\epsilon\\right]=\\beta^\\top X_i=\\beta_1X_{1,1}+\\ldots+\\beta_pX_{i,p}\\]\nWhat does each \\(\\beta_j\\) mean? Suppose that \\(X_j\\) is a continuous covariate.\nWe can interpret (\\(\\beta_j\\)) as follows:\nHolding \\(X_{i,1},\\ldots,X_{i,j-1},X_{i,j+1},\\ldots, X_{i,p}\\) constant, a one unit increase in \\(X_{i,j}\\) causes, on average, a \\(\\beta_j\\) unit increase in \\(Y_i\\).\nFrom another angle, we have that \\(\\partial {\\textrm{E}}\\left[Y\\right]/\\partial X=\\beta\\), therefore, the rate of change with respect to the \\(j^{th}\\) covariate is \\(\\beta_j\\).\n\n\n\n\n\n\nCaution\n\n\n\nThe “on average” and “holding other covariates constant” are very important components of the interpretation. First, the on average acknowledges the random error \\(\\epsilon\\). In other words, a one unit increase in \\(X_{i,j}\\) will not certainly increase \\(Y_i\\), but it will on average. Next, the “holding other covariates constant” is used to mention how correlations between covariates are handled by the model. Some of the covariates in the model may be correlated, so increases in a given covariate may often be associated with changes in another covariate. This is not accounted for in the coefficient vectors \\(\\beta\\). That is why we must specify “holding other covariates constant”.\nFor instance, if a model includes a terms for years of education attained and income, we know that as the number of years of education increase we expect to see a rise in income levels. As a result, to interpret the effect of coefficient on income, we must “hold years of education constant”, comparing what is expected with income changes but education does not.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nFor now, we can assume that all of the covariates \\(X_j\\) are continuous variables. Later in the course, there may be categorical covariates. In this case, the \\(\\beta_j\\) corresponding to the categorical covariates have a different interpretation. We will return to this later.\n\n\nRecall Example 3.1. We assume \\(\\forall i\\in[20]\\), it holds that \\[Y_i|X_i=\\beta^\\top X_i+\\epsilon_i,\\] with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma^2)\\), \\(\\epsilon_i\\perp \\epsilon_j\\) for \\(i\\neq j,\\ i,j\\in [20]\\). A one unit increase in weight causes, on average, a \\(\\beta_2\\) unit increase in body fat percentage. Since \\(\\beta_1\\) is the intercept, it has a special interpretation. \\(\\beta_1\\) is the average value of \\(Y_i\\) given \\(X_i=0\\). It is also helpful to note that \\({\\textrm{cov}}(Y)=\\sigma^2 I\\).\n\n\n3.2.2 Least squares estimation\nOkay, but we don’t know \\(\\beta\\)! Just like we estimate the population mean with the sample mean, we need to estimate \\(\\beta\\). We would like an estimate \\(\\hat\\beta\\), so that we can predict body fat percentage from weight. What is our best guess at \\(\\beta\\), given the data? One way to answer this, is through the method of least squares.\nReturning to our example, recall that:\n\nplot(df)\n\n\n\n\n\n\n\n\nFor example, suppose we want to determine if \\(\\beta\\) is more likely to be \\((-35,0.3)^\\top\\) or \\((-22,0.2)^\\top\\) . How can we say which line is a better to our data? One way is to graph them on top of the data and determine which one looks better. Let’s plot these lines.\n\nplot(df)\n# plot Y=-35+0.3X\nabline(-35,0.3,lwd=2)\n# plot Y=-25+0.2X\nabline(-22,0.2,col='red',lwd=2)\n\n\n\n\n\n\n\n\nIts not clear which one fits the data better. Even if it was clear, obviously, we cannot plot all possible lines. So how can we determine which line fits the data the “best”?\nTo do this, we have to define what “best” means quantitatively. For instance, one might ask which line minimizes the sum of the squared distances of the observed data points to the line? This line is then said to be the “best” line. Mathematically, given a proposed value of \\(\\beta\\), say \\(\\beta_0\\in \\mathbb{R}^p\\), the signed distance to the hyperplane \\(X\\beta_0\\) is \\(\\epsilon_0=Y-X\\beta_0\\). The squared distances to the hyperplane \\(X\\beta_0\\) is then \\(\\epsilon_0^\\top\\epsilon_0 =(Y-X\\beta_0)^\\top (Y-X\\beta_0)\\). We can then formulate this as a math problem: Which \\(\\beta_0\\in \\mathbb{R}^p\\) minimizes \\(\\epsilon_0^\\top\\epsilon_0\\)? i.e., \\(\\hat\\beta =\\mathop{\\mathrm{argmin}}_{\\beta_0\\in\\mathbb{R}^p}\\epsilon_0^\\top\\epsilon_0.\\) It is more convenient to just write \\[\\hat\\beta =\\mathop{\\mathrm{argmin}}_{\\beta\\in\\mathbb{R}^p}(Y-X\\beta)^\\top (Y-X\\beta).\\]\nIn this framework, the “best” estimate is given by \\[\\hat\\beta =\\mathop{\\mathrm{argmin}}_{\\beta\\in\\mathbb{R}^p}(Y-X\\beta)^\\top (Y-X\\beta).\\] Note best is in the sense of minimizing the average squared distance to the hyperplane/line. We could also define best in terms of some other metric, such as average absolute distance to the hyperplane/line. For now, we will stick with this metric.\nThe next step is to solve: \\[\\hat\\beta =\\mathop{\\mathrm{argmin}}_{\\beta\\in\\mathbb{R}^p}(Y-X\\beta)^\\top (Y-X\\beta).\\]\nHow do we minimize a function???\nRECALL in calculus, to find the minimum of a function we:\n\nObtain the first two derivatives of the function.\nSet the first derivative to zero and solve for the critical value.\nUse the second derivative to verify the critical value minimized the function.\n\nGoal: Compute \\(\\hat\\beta\\) – Minimize \\(g(\\beta)=(Y-X\\beta)^\\top (Y-X\\beta)\\). (It may be useful to review taking derivatives with respect to vectors here.\nStep 1a: \\[\\begin{align*}\n\\frac{\\partial g}{\\partial\\beta}&= &\\frac{\\partial g}{\\partial\\beta}(Y-X\\beta)^\\top (Y-X\\beta)\\\\\n&= &\\frac{\\partial g}{\\partial\\beta}\\left[Y^\\top Y-2(X\\beta)^\\top Y+(X\\beta)^\\top X\\beta\\right] \\tag{Transpose and distribute}\\\\\n&= &-2\\frac{\\partial g}{\\partial\\beta}\\beta^\\top X^\\top Y+\\frac{\\partial g}{\\partial\\beta}\\beta^\\top X^\\top X\\beta \\tag{$(AB)^\\top=B^\\top A^\\top$}\\\\\n&= &-2 X^\\top Y+2X^\\top X\\beta \\tag{$\\frac{\\partial }{\\partial x}x^\\top Ax=2 A$ if $A$ symmetric, $\\frac{\\partial }{\\partial x}x^\\top a=a$ }\\\\\n&= &-2X^\\top (Y-X\\beta ).\n\\end{align*}\\]\nStep 1b: (Do this for homework) \\[\\frac{\\partial^2 g}{\\partial\\beta\\partial\\beta^\\top}=2X^\\top X.\\]\nStep 2: We now need \\(X^\\top X\\) to be invertible, so we will assume that \\(X\\) is full rank and \\(n\\geq p\\). \\[\\begin{align*}\n    -2X^\\top (Y-X\\beta )=0\\\\\n    &\\implies X^\\top Y=X^\\top X\\beta\\\\\n    &\\implies \\beta=(X^\\top X)^{-1}X^\\top Y.\n\\end{align*}\\]\nStep 3:\nRecall that if the Hessian matrix is positive definite at a critical point, then that critical point is a local minimum. Since we have assumed \\(X\\) is full rank, this implies that \\(X^\\top X\\) is positive definite.\nTo summarize, the steps have proceeded as follows:\n\nStep 1a: \\(\\frac{\\partial g}{\\partial\\beta}=-2X^\\top (Y-X\\beta )\\)\nStep 1b: \\(\\frac{\\partial^2 g}{\\partial\\beta\\partial\\beta^\\top}=2X^\\top X\\) (Do this for homework)\nStep 2: \\(-2X^\\top (Y-X\\beta )=0\\implies X^\\top Y=X^\\top X\\beta\\) \\(\\implies \\beta=(X^\\top X)^{-1}X^\\top Y\\)\nStep 3: \\(2X^\\top X\\) is positive definite, and so \\[\\hat\\beta=(X^\\top X)^{-1}X^\\top Y.\\]\n\nThe estimate \\(\\hat\\beta\\) is known as the least squares estimate of the regression coefficients.\n\nDefinition 3.1 The least squares estimate of the regression coefficients is \\[\\hat\\beta=(X^\\top X)^{-1}X^\\top Y.\\]\n\n\n\n3.2.3 Example\n\nExample 3.2 In the body weight example Example 3.1, write down \\(X\\), \\(Y\\) and compute \\(\\hat\\beta\\). Interpret \\(\\hat\\beta\\).\n\nFirst, we have that \\[Y=\\left( 6   , 21  , 15  , 6   , 22  , 31  , 32  , 21  , 25  , 30  , 10  , 20  , 22  , 9   , 38  , 10  , 27  , 12  , 10  , 28  \\right)^\\top\\] \\[X=\\left[1_{20}\\ | \\left( 175 , 181 , 200 , 159 , 196 , 192 , 205 , 173 , 187 , 188 ,\n188 , 240 , 175 , 168 , 246 , 160 , 215 , 159 , 146 , 219  \\right)^\\top\\right]\\]\n\n\n\n\n\n\nNote\n\n\n\nFor matrices \\(A,B\\) which have the same number of rows, \\(C=[A | B]\\) is horizontal concatenation of \\(A\\) and \\(B\\). This notation indicates that the matrix \\(C\\) is formed by placing \\(A\\) an \\(B\\) side by side, joining them horizontally. Therefore, \\(X\\) is the matrix whose first column is made up of ones, and second column is made up of the body weights.\n\n\nLet’s use R to compute \\(\\hat\\beta\\).\n\n#Define X and Y\nX=cbind(rep(1,nrow(df)), df$Weight)\nY=df$BodyFat\n\n# cast to column vec\nY=matrix(Y,ncol=1)\n\n#X'X \nX_p_X=t(X)%*%X\n\n#X'X inverse\nX_p_X_inverse=solve(X_p_X)\n\n\n#LS\nbeta_hat= X_p_X_inverse%*%t(X)%*%Y\nbeta_hat\n\n            [,1]\n[1,] -27.3762623\n[2,]   0.2498741\n\n# We can also use Rs lm() function to do this: \n# This code is essential for the course. \n# The first argument is the formula \nmodel=lm(BodyFat ~ Weight, data=df)\n\n#The summary function prints the model output. \n\nsummary(model)\n\n\nCall:\nlm(formula = BodyFat ~ Weight, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.5935  -5.7904   0.6536   5.2731  10.4004 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -27.37626   11.54743  -2.371 0.029119 *  \nWeight        0.24987    0.06065   4.120 0.000643 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.049 on 18 degrees of freedom\nMultiple R-squared:  0.4853,    Adjusted R-squared:  0.4567 \nF-statistic: 16.97 on 1 and 18 DF,  p-value: 0.0006434\n\n# The least squares estimates are given in the Estimate column of the summary. \n\nThe lm() function is used to fit multiple linear regression models in R. The basic usage involves specifying a formula and a data frame. The syntax is given by lm(formula, data, ...).\nThe data argument should be the dataframe which contains your data. The formula argument is used to specify the model to be fitted. It provides a symbolic description of the model, indicating the response variable and the predictors/covariates, as well as the relationships between them. The left-hand side should be the name of your response variable, as it is named in your dataframe. To see the names of your variables use the names() function, e.g., names(df). The right-hand side contains the covariates you want to include in your model. For instance, above, the formula is given by BodyFat ~ Weight . Note that BodyFat is the response and Weight is the covariate.\nWe now list some important properties of the least squares estimator.\n\nExercise 3.5 Compute \\({\\textrm{E}}\\left[\\hat\\beta\\right]\\) and \\({\\textrm{cov}}(\\hat\\beta)\\).\n\n\nSolution 3.1. It holds that \\({\\textrm{E}}\\left[\\hat\\beta\\right]=\\beta\\) and \\({\\textrm{cov}}(\\hat\\beta)=\\sigma^2 I\\).\n\nRecall that an estimator is unbiased if its expectation equals the population parameter it is trying to estimate. After completing Exercise 3.5 you will see that \\(\\hat\\beta\\) is unbiased for the parameter \\(\\beta\\).\nThe least squares estimator is also the “best linear unbiased estimator”, or the BLUE. This is known as the Gauss–Markov theorem. This means that under the assumptions of the linear regression model, over any unbiased estimator of \\(\\beta\\) we can construct, which is a linear combination of \\(Y_1,\\ldots,Y_n\\), the estimator \\(\\hat\\beta\\) has the smallest variance (and therefore, the smallest mean squared error. Recall that for an estimator \\(\\hat\\alpha\\), the mean squared error is given by \\({\\textrm{E}}\\left[||\\beta-\\hat\\alpha||^2\\right]\\).)\nThe Gauss–Markov theorem does not require the random error to be normally distributed. If we are willing to assume that \\(\\epsilon\\sim \\mathcal{N}(0,\\sigma^2 I)\\), then \\(\\hat\\beta\\) is also the maximum likelihood estimator and the “uniformly minimum-variance unbiased estimator”, or UMVUE. This means that \\(\\hat\\beta\\) has lower variance than any other unbiased estimator, no matter what the true value of \\(\\beta\\) is.\nOne might ask, how can we use \\(\\hat\\beta\\) to predict body fat percentage given weight? The estimate \\(\\hat\\beta\\) gives us a best guess at the coefficients. Therefore, our best guess at someones body fat is given by \\[Best\\ Guess=-27.3762623+ 0.2498741 \\times Weight.\\] For instance, for someone who is 170 pounds, we would guess that their body fat percentage is \\(-27.3762623+ 0.2498741 \\times 170=\\) 15.1023347.\n\n\n3.2.4 Homework stop 2\n\nExercise 3.6 Why do we need \\(\\hat\\beta\\), why not use \\(\\beta\\)?.\n\n\nExercise 3.7 Is \\(\\hat\\beta\\) an estimate or a population parameter? What about \\(\\beta\\)?\n\n\nExercise 3.8 Compute, \\(X\\), \\(Y\\) and \\(\\hat\\beta\\) in the following real data example:\nIt is challenging to assess a student’s understanding of a subject without administering an exam. However, we can easily record the number of hours a student studies. A researcher would like to know if the number of hours studied and exam scores are related. This researcher collected the following data:\n\n\n\nStudent\nHours Studied\nExam Score (%)\n\n\n\n\n1\n5\n55\n\n\n2\n8\n65\n\n\n3\n12\n78\n\n\n4\n6\n58\n\n\n5\n10\n72\n\n\n6\n9\n68\n\n\n7\n15\n85\n\n\n8\n7\n60\n\n\n9\n11\n74\n\n\n10\n13\n80\n\n\n11\n14\n82\n\n\n12\n20\n90\n\n\n13\n5\n55\n\n\n14\n6\n59\n\n\n15\n18\n88\n\n\n16\n7\n62\n\n\n17\n16\n86\n\n\n18\n4\n50\n\n\n19\n3\n45\n\n\n20\n19\n89\n\n\n\nTo help you, here is some R code the dataset:\n\n# Data\nstudy_data &lt;- data.frame(\n  Student = 1:20,\n  Hours_Studied = c(5, 8, 12, 6, 10, 9, 15, 7, 11, 13, 14, 20, 5, 6, 18, 7, 16, 4, 3, 19),\n  Exam_Score = c(55, 65, 78, 58, 72, 68, 85, 60, 74, 80, 82, 90, 55, 59, 88, 62, 86, 50, 45, 89)\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "Unit_2.html#least-squares-inference",
    "href": "Unit_2.html#least-squares-inference",
    "title": "3  Linear Regression",
    "section": "3.3 Least squares inference",
    "text": "3.3 Least squares inference\nRecall we estimate the parameter \\(\\beta\\) using least squares:\nRecall that \\(\\hat\\beta=(X^\\top X)^{-1}X^\\top Y\\). We can predict a new weight \\(Y_{new}|X=x\\) with \\(\\hat y_{new}=x^\\top \\hat\\beta\\). We may be interested in the following questions: How good is \\(\\hat y_{new}\\) as a prediction, on average? How will new observations vary about the line? For example, given a specific weight, how will does body fat percentage vary around the regression line? How does \\(\\hat\\beta\\) vary around \\(\\beta\\)? Is there strong evidence that \\(Y\\) has a relationship with \\(X\\)? Is \\(X\\) adding information about \\(Y\\) at all?\nTo answer these questions, we need to look at the variation of our estimates and our data.\n\n3.3.1 Important quantities: Residuals and fitted values\nWe now introduce some very important quantities: We call the estimated values given our observed \\(X\\) the fitted values: \\(\\hat Y=X\\hat\\beta\\). The fitted values are what our model would estimate the vector \\(Y\\) to be. We call \\(\\hat \\epsilon=Y-\\hat Y\\) is the residual vector. The \\(i\\)th entry of \\(\\hat \\epsilon\\), say \\(\\hat \\epsilon_i\\), is the \\(i\\)th residual. The residuals are the signed distances from the response variable to the estimated regression hyperplane. The sum of squared error or sum of squared residuals (SSE) is given by \\(\\hat \\epsilon^\\top \\hat \\epsilon=\\sum_{i=1}^n\\hat \\epsilon_i^2\\). Note that since we estimated \\(\\beta\\) using the least squares method, \\(\\hat \\epsilon^\\top \\hat \\epsilon\\) is minimized (with respect to varying \\(\\beta\\)).\n\nExample 3.3 Recall Example 3.1. What is the residual of individual 3? How can we interpret this value?\n\n\nresiduals=Y-X%*%beta_hat; residuals\n\n             [,1]\n [1,] -10.3517117\n [2,]   3.1490434\n [3,]  -7.5985652\n [4,]  -6.3537255\n [5,]   0.4009314\n [6,]  10.4004279\n [7,]   8.1520641\n [8,]   5.1480365\n [9,]   5.6497986\n[10,]  10.3999245\n[11,]  -9.6000755\n[12,] -12.5935307\n[13,]   5.6482883\n[14,]  -5.6025928\n[15,]   3.9072245\n[16,]  -2.6035997\n[17,]   0.6533228\n[18,]  -0.3537255\n[19,]   0.8946383\n[20,]   0.6538262\n\n# This means that individual 3's body fat is 7.5 percentage points lower than the fitted line\n\nresiduals[3]\n\n[1] -7.598565\n\n# We can go further and and plot all of the residuals\ncurve(beta_hat[1]+beta_hat[2]*x,125,280,lwd=3,xlab=\"Weight\",ylab=\"BF %\")\npoints(Weight,BodyFat,pch=22,bg=1)\nYvals=cbind(BodyFat,model$fitted.values)\nXvals=cbind(Weight,Weight)\nfor(i in 1:nrow(Yvals)){\n  lines(Xvals[i,],Yvals[i,])\n  text(Xvals[i,1]+2,mean(Yvals[i,]),round(residuals[i],2))\n}\n\n\n\n\n\n\n\n# Then, the population body fat percentages, given weights will look like this: \n# \n# Bfs=-15+.2*Weight+rnorm(20,0,sd=5)\n\n\n\n3.3.2 Variation decomposition\nVariance decomposition is a fundamental concept that explains how the total variation in the response variable can be partitioned into different sources. This decomposition is crucial for evaluating the performance of the regression model and understanding the contributions of various factors.\nThe residuals describe one type of variation of the response values. We can also consider the total variation of the response. The total variation of the response, or the sum of squares total/total sum of squares (\\(SST\\)) is given by \\(SST=(n-1)\\hat\\sigma_y=\\sum_{i=1}^n(Y_i-\\bar Y)^2=(Y-\\bar Y 1)^\\top (Y-\\bar Y 1)\\). It can be shown that the \\(SST\\) can be decomposed as follows: \\[SST=(Y-\\bar Y 1)^\\top (Y-\\bar Y 1)=(Y-\\hat Y)^\\top (Y-\\hat Y)+(\\hat Y-\\bar Y)^\\top (\\hat Y-\\bar Y)=\\hat\\epsilon^\\top \\hat\\epsilon+(\\hat Y-\\bar Y)^\\top (\\hat Y-\\bar Y).\\] That is, \\(SST=SSE+SSModel\\) where\n\n\\(SSModel\\), OR \\(SSM\\) measures the total variations of the response explained by the covariates \\(X\\) via the model based on \\(\\hat\\beta\\).\n\\(SSE\\) measures the total variations of the response unexplained by the covariates \\(X\\) via the model based on \\(\\hat\\beta\\).\nNote there are sometimes other names for \\(SSE\\) and \\(SSModel\\), such as \\(SSRegression\\), \\(SSwithin\\) and \\(SSbetween\\), etc.\n\nSo, we have that the total variation in the response can be broken down into that which is explained by the \\(X\\) values, and that which is unexplained.\nAn interesting observation is given as follows: The first column of the \\(X\\) matrix is given by \\(1_n\\), which implies that \\[\\bar Y 1=X\\begin{bmatrix}\n\\bar Y\\\\\n0\n\\end{bmatrix}.\\] This means that if we let \\(\\hat\\beta_*=(\\bar Y,0,\\ldots,0)^\\top\\), then \\((Y-\\bar Y 1)\\) would be the signed distances to (or the residuals of) the regression hyperplane corresponding to \\(\\hat\\beta_*\\). Since \\(\\hat\\beta\\) minimizes the sum of squared residuals, we must have that the hyperplane corresponding to \\(\\hat\\beta\\) has a smaller sum of squared residuals than the regression hyperplane corresponding to \\(\\hat\\beta_*\\). Therefore, we must have that \\(\\hat \\epsilon^\\top \\hat \\epsilon\\leq (Y-\\bar Y 1)^\\top (Y-\\bar Y 1)\\).\nEach of these terms in the decomposition is associated with a certain number of degrees of freedom.\n\nTotal: \\(dfT=n-1\\).\nModel: \\(dfM=\\#\\ \\textbf{non-zero }\\beta-1\\).\nError: \\(dfE=n-\\#\\ \\textbf{non-zero }\\beta\\).\n\nIntuitively, since the \\(SSE\\) is the variance unexplained by the model/covariates, the \\(SSE\\) is related to the error variance \\(\\sigma^2\\). In fact, to estimate \\(\\sigma^2\\), we use \\[\\hat\\sigma^2=MSE=\\frac{SSE}{dfE}.\\]\nThe null model is defined as \\(Y|X=\\beta_0+\\epsilon\\). This is the model where the last \\(p-1\\) terms in the true vector \\(\\beta\\) are \\(0\\). This model says that \\(Y\\) does not depend on \\(X\\). In the null model, we only need to estimate the mean, so \\(df=n-1\\). Therefore, under the null model, \\[\\begin{align*}\n        \\hat\\sigma^2=(n-1)^{-1}SST&=\\hat\\sigma_Y^2\\\\\n        &=(n-1)^{-1}\\sum_{i=1}^n(Y_i-\\bar Y)^2=(n-1)^{-1}(Y-\\bar Y 1)^\\top (Y-\\bar Y 1).\n    \\end{align*}\\] Therefore, in the null model, the estimate of \\(\\sigma^2\\) via the \\(MSE\\) is just the usual estimate of the variance of the response. This is intuitive!\nThe following table can be used to summarize the variation in the response:\n\n\n\nSource\nSS\ndf\nMS\n\n\n\n\nModel\n\\(SSM\\)\n\\(dfM\\)\n\\(MSModel = SSM/dfM\\)\n\n\nResidual\n\\(SSE\\)\n\\(dfE\\)\n\\(MSE = SSE/dfE\\)\n\n\nTotal\n\\(SST\\)\n\\(dfT\\)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is very important to be able to interpret these terms! The derivation is also important. However, we can use a machine to compute anything for us, so memorizing the formula is not helpful.\n\n\n\n\n3.3.3 Coefficients of determination\nA model is a good model if it can explain a fair amount of the variation in the response. (You can think that the model explains “changes” in the response.) In other words, \\(SSModel\\) should be as close to \\(SSTotal\\) as possible; or equivalently, \\(SSError\\) should be as close to 0 as possible. Now, “close” is a relative term, and so we need another value to reference to. This is where the \\(R^2\\) comes in: \\[R^2=\\frac{SSModel}{SST},\\] and is the proportion of variation explained by the model. It is clear that \\(0 \\leq  R^2 \\leq 1\\), and so rescaling the data will not affect \\(R^2\\) (like it would affect the sum of squares terms \\(SST,SSE,SSM\\)). If \\(R^2\\) is close to 1, it is large – “close to 1” is a subjective/area dependent. Generally, the larger the \\(R^2\\), the better the model!\nTo compare different models, we could potentially add different covariates and see if \\(R^2\\) improves. However, every time you add any variable, \\(R^2\\) will always increase. Therefore, it is common to use the adjusted coefficient of determination: \\[\\bar R^2=1-(1-R^2)\\frac{n-1}{n-p}.\\] Thus, the (adjusted) coefficient of determination can be used as a measure of how well the regression model fits the data (how much variance is explained). It could also be used to compare models.\n\n\n3.3.4 The \\(F\\) test\nThe coefficients of determination are summary statistics which give an idea of the fit of the model. We would also like a significance test that tells us whether the covariates explain \\(Y\\), or what we observed was simply due to sampling variation.\nIf \\(\\beta=(\\beta_1,\\ldots,\\beta_p)^\\top\\) then let \\(\\tilde\\beta=(\\beta_2,\\ldots,\\beta_p)^\\top\\). That is \\(\\tilde\\beta\\) is the regression coefficients without the intercept term. Similarly, let \\(\\tilde{\\hat\\beta}=(\\hat\\beta_2,\\ldots,\\hat\\beta_p)^\\top\\). Now, we want to avoid the situation where \\(\\tilde\\beta=0\\) but \\(\\tilde{\\hat\\beta}\\neq 0\\) due to sampling variation.\nTo do this, we perform a significance test: \\[H_0:\\tilde\\beta=0\\qquad vs\\qquad H_1:\\tilde\\beta\\neq 0.\\]\nFirst, we need the normality assumption to perform significance test: Assume \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma^2)\\). With this assumption, the model is then known as the Normal Multiple Linear Regression Model. It is important to note that the least squares method does not require this assumption, and this assumption is required only for the significance test to be valid. To test the hypothesis stated above, we use the overall \\(F\\) test and the observed test statistic is \\(F_{obs} = MSModel/MSE.\\) Why?\nWith the extra normality assumption, we have the following holds:\n\n\\(Y|X\\) is normally distributed.\nWe have that \\(SSM/\\sigma^2\\sim \\chi^2_{dfM}\\) and \\(SSE/\\sigma^2\\sim \\chi^2_{dfE}\\).\nFurthermore, \\(SSM\\perp SSE\\).\n\nRecall that the ratio of two independent \\(\\chi^2\\) distributions divided by their respective degrees of freedom follows an \\(F\\) distribution. Therefore, we have that \\(F_{obs}\\sim F_{dfM,dfE}\\). The corresponding p-value is \\(\\Pr(W &gt; F_{obs})\\) where \\(W\\sim F_{dfM;dfE}\\). We can alternatively reject the null hypothesis if \\(F_{obs}&gt;F_{dfM;dfE,1-alpha}\\), where \\(F_{obs}&gt;F_{dfM;dfE,1-alpha}\\) is the \\(1-\\alpha\\) quantile of the \\(F_{dfM,dfE}\\) distribution.\nWe can now present the complete ANOVA table\n\n\n\n\n\n\n\n\n\n\n\nSource\nSS\ndf\nMS\nF\np-value\n\n\n\n\nModel\n\\(SSM\\)\n\\(dfM\\)\n\\(MSModel = \\frac{SSR}{dfM}\\)\n\\(F = \\frac{MSModel}{MSE}\\)\n\\(\\Pr(W &gt; F_{obs})\\)\n\n\nResidual\n\\(SSE\\)\n\\(dfE\\)\n\\(MSE = \\frac{SSE}{dfE}\\)\n\n\n\n\nTotal\n\\(SST\\)\n\\(dfT\\)\n\n\n\n\n\n\n\nExample 3.4 In Example 3.1, compute and interpret the coefficients of determination. Compute and interpret the ANOVA table. Test whether the regression model is significant. (This means perform the \\(F\\) test.)\n\n\n# recall\nhead(df)\n\n  Weight BodyFat\n1    175       6\n2    181      21\n3    200      15\n4    159       6\n5    196      22\n6    192      31\n\n# The F test results are given in the summary\nsummary(model)\n\n\nCall:\nlm(formula = BodyFat ~ Weight, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.5935  -5.7904   0.6536   5.2731  10.4004 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -27.37626   11.54743  -2.371 0.029119 *  \nWeight        0.24987    0.06065   4.120 0.000643 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.049 on 18 degrees of freedom\nMultiple R-squared:  0.4853,    Adjusted R-squared:  0.4567 \nF-statistic: 16.97 on 1 and 18 DF,  p-value: 0.0006434\n\n# The ANOVA table is given below\n\n# First define the null model object using lm()\n# This line fits a model with only the intercept term\nnull_model=lm(BodyFat~1,data=df)\n\n# This line gets the ANOVA table\nanova(null_model,model)\n\nAnalysis of Variance Table\n\nModel 1: BodyFat ~ 1\nModel 2: BodyFat ~ Weight\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     19 1737.75                                  \n2     18  894.42  1    843.33 16.972 0.0006434 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# We can also do this by hand: \n\n\n# Store sample size\nn=nrow(df)\np=2\n\n# Compute sum of squares\nSST=t(Y-mean(Y)*rep(1,n))%*%(Y-mean(Y)*rep(1,n))\nYhat=X%*%beta_hat\nres=Y-Yhat\nSSE=t(res)%*%res\nSSM=SST-SSE\n\ndfe=n-p\ndfm=p-1\nMSM=SSM/dfm\n\n\nMSE=SSE/dfe\n\nFv=MSM/MSE\n\np.val=1-pf(Fv,dfm,dfe)\n\n# ANOVA Table:\nANOVA_Table=rbind(c(SSM,dfe,MSM,Fv,p.val),c(SSE,dfe,MSE,NA,NA),c(SST,n-1,NA,NA,NA))\nrownames(ANOVA_Table)=c(\"Model\",\"Error\",\"Total\")\ncolnames(ANOVA_Table)=c(\"SS\",\"df\",\"MS\",\"F\",\"p-value\")\nANOVA_Table\n\n             SS df        MS        F      p-value\nModel  843.3252 18 843.32521 16.97164 0.0006434484\nError  894.4248 18  49.69027       NA           NA\nTotal 1737.7500 19        NA       NA           NA\n\n\n\n\n3.3.5 Homework stop 3\n\nExercise 3.9 In the following real data example: Compute and interpret the coefficient of determination, the adjusted coefficient of determination and perform the \\(F\\) test for model significance. Including printing the ANOVA table, the null and alternative hypothesis, an interpretation of the p-value and the conclusion of the test.\nIt is challenging to assess a student’s understanding of a subject without administering an exam. However, we can easily record the number of hours a student studies. A researcher would like to know if the number of hours studied and exam scores are related. This researcher collected the following data:\n\n\n\nStudent\nHours Studied\nExam Score (%)\n\n\n\n\n1\n5\n55\n\n\n2\n8\n65\n\n\n3\n12\n78\n\n\n4\n6\n58\n\n\n5\n10\n72\n\n\n6\n9\n68\n\n\n7\n15\n85\n\n\n8\n7\n60\n\n\n9\n11\n74\n\n\n10\n13\n80\n\n\n11\n14\n82\n\n\n12\n20\n90\n\n\n13\n5\n55\n\n\n14\n6\n59\n\n\n15\n18\n88\n\n\n16\n7\n62\n\n\n17\n16\n86\n\n\n18\n4\n50\n\n\n19\n3\n45\n\n\n20\n19\n89\n\n\n\nTo help you, here is some R code the dataset:\n\n# Data\nstudy_data &lt;- data.frame(\n  Student = 1:20,\n  Hours_Studied = c(5, 8, 12, 6, 10, 9, 15, 7, 11, 13, 14, 20, 5, 6, 18, 7, 16, 4, 3, 19),\n  Exam_Score = c(55, 65, 78, 58, 72, 68, 85, 60, 74, 80, 82, 90, 55, 59, 88, 62, 86, 50, 45, 89)\n)\n\n\n\nExercise 3.10 Write down the interpretations of: \\(SSE,\\ MSE,\\ R^2,\\ \\bar R^2,\\ SSM\\).\n\n\nExercise 3.11 What is the interpretation of the p-value in the ANOVA table?\n\n\nExercise 3.12 What extra assumption is needed to perform the \\(F\\)-test?\n\n\n\n3.3.6 Significance of one variable\nSo far, we have learned that the least squares method yields the following estimate of \\(\\hat\\beta = (X^\\top X)^{-1} X^\\top Y\\) with \\({\\textrm{E}}\\left[\\hat\\beta\\right] = \\beta\\) and \\({\\textrm{cov}}(\\hat\\beta) = (X^\\top X)^{-1} \\sigma^2\\). Moreover, we use \\(MSE\\) to estimate \\(\\sigma^2\\). Next, we learned that we can summarize the \\(SS,\\ df\\), and \\(MS\\) in an ANOVA table. We used the \\(F\\) test and the coefficient of determination to evaluate the quality of the model, i.e., to see the amount of information \\(X\\) provides about \\(Y\\).\nWhen the model is a significant model, then, at least one of the individual explanatory variables is useful in explaining the response. We may be interested in whether a specific covariate, or set of covariates is useful in explaining the response variable. We now learn how we can test for the significance of each individual explanatory variable separately and how we can test for the significance of a subset of explanatory variables. Note that these tests also require that the random error is normally distributed.\nTo test for significance and compute confidence intervals of a single variate, we have to compute the distribution of \\(\\hat\\beta_j\\). We first compute the mean and variance of \\(\\hat\\beta_j\\). First, given that \\({\\textrm{E}}(\\hat\\beta) = \\beta\\), we have \\({\\textrm{E}}(\\hat\\beta_j) = \\beta_j\\). Next, \\({\\textrm{Var}}\\left[\\hat\\beta_j\\right]\\) is the \\((j, j)^{th}\\) entry of \\({\\textrm{cov}}(\\hat\\beta)\\). In addition, we have derived that \\({\\textrm{cov}}(\\hat\\beta) = (X^\\top X)^{-1} \\sigma^2\\).\nNow, recall that if \\(Z\\) is multivariate normal, i.e., \\(Z\\sim\\mathcal{N}(\\mu,\\Sigma)\\), then \\(b+AZ\\sim\\mathcal{N}(b+A\\mu,A\\Sigma A^\\top)\\), i.e., \\(b+AZ\\) is also multivariate normal. Therefore, since we have assumed that \\(\\epsilon\\sim \\mathcal{N}_n(0,\\sigma^2 I)\\) and that \\(Y|X=X\\beta+\\epsilon\\), it follows that \\(Y|X\\sim \\mathcal{N}_n(X\\beta,\\sigma^2 I)\\). Next, we may recall that \\(\\hat\\beta=(X^\\top X)^{-1} XY\\). Let \\(A=(X^\\top X)^{-1} X\\). Then \\(\\hat\\beta=AY\\). It follows that \\(\\hat\\beta\\) is also multivariate normal! Putting everything together, we have that \\(\\hat\\beta\\sim \\mathcal{N}_p(\\beta,(X^\\top X)^{-1} \\sigma^2)\\).\n\nTheorem 3.1 Under the assumptions of the normal linear regression model it holds that \\(\\hat\\beta\\sim \\mathcal{N}_p(\\beta,(X^\\top X)^{-1} \\sigma^2)\\).\n\nNow that we have the distribution of \\(\\hat\\beta\\), we can use it to compute the confidence intervals for \\(\\beta_j\\)s.\nRecall from introductory statistics (MATH 1131) that you learned that if we want to compute a confidence interval for the sample mean and the sample variance was unknown, we had to estimate the variance. Similarly, here, the variance of \\(\\hat\\beta_j\\) contains \\(\\sigma\\), an unknown parameter. Recall that, we estimate \\(\\sigma^2\\) by \\(MSE\\), and so we can estimate the variance of \\(\\hat\\beta_j\\) by \\(\\hat{\\textrm{Var}}\\left[\\hat\\beta_j\\right] = (X^\\top X)_{j,j}^{-1} MSE\\).\nIt can be shown that \\(\\hat\\beta\\perp MSE\\). Therefore, we have that \\[\\frac{\\hat\\beta_j - \\beta_j}{\\sqrt{\\widehat{var}(\\hat\\beta_j)}}\\sim t_{dfE}.\\]\nNow that we know the distribution of \\(\\hat\\beta_j\\), we can perform significance testing and compute confidence intervals.\nIf we want to test \\[H_0\\colon \\beta_j=\\beta_j^0\\qquad vs \\qquad \\beta_j\\neq\\beta_j^0\\] we can do the following.\nThe observed test statistic is \\(TS=\\frac{\\hat\\beta_j - \\beta_j^0}{\\sqrt{\\widehat{var}(\\hat\\beta_j)}}\\). Note that, under the null hypothesis, we have that \\(\\frac{\\hat\\beta_j - \\beta_j^0}{\\sqrt{\\widehat{var}(\\hat\\beta_j)}}\\sim t_{dfE}\\). Thus, the corresponding \\(p\\)-value is obtained based on the \\(t_{dfE}\\) distribution. Specifically, we can compute the p-value \\(\\Pr\\left(-|TS|&lt;Z\\right)+\\Pr\\left(|TS|&gt;Z\\right)=2*\\Pr\\left(|TS|&gt;Z\\right)\\), where \\(Z\\sim t_{dfE}\\).\nThe test proceeds as follows:\n\nState the hypotheses \\[H_0\\colon \\beta_j=\\beta_j^0\\qquad vs \\qquad H_1:\\ \\beta_j\\neq\\beta_j^0.\\]\nCompute the test statistic \\(\\frac{\\hat\\beta_j - \\beta_j^0}{\\sqrt{\\widehat{var}(\\hat\\beta_j)}}\\) and the p-value.\nInterpret the p-value, and use it to decide whether you reject the null hypothesis.\n\nOften, one may choose a threshold \\(\\alpha\\), and reject the null hypothesis if the p-value falls below that threshold. Other times, we use the p-value as a description of evidence against the null. If it is larger than 0.05, but still small, then that still constitutes some evidence against the null hypothesis.\nLet’s now discuss one-sided hypotheses. First, consider: \\[H_0\\colon \\beta_j\\leq  \\beta_j^0\\qquad vs \\qquad H_1:\\ \\beta_j&gt; \\beta_j^0\\] Then, if the alternative hypothesis is true, we expect \\(TS\\) to be positive. The p-value is given by \\(\\Pr\\left(TS&gt;Z\\right)\\), where \\(Z\\sim t_{dfE}\\). Notice that the p-value is measuring how extremely positive \\(TS\\) is. Using the threshold method, we can also check if \\(TS&gt; t_{dfE,1-\\alpha}\\). Next, if we want to test \\[H_0\\colon \\beta_j\\geq  \\beta_j^0\\qquad vs \\qquad H_1:\\ \\beta_j&lt; \\beta_j^0,\\] then if the alternative hypothesis is true, we expect \\(TS\\) to be negative. The p-value is given by \\(\\Pr\\left(TS&lt;Z\\right)\\), where \\(Z\\sim t_{dfE}\\). Notice that the p-value is measuring how extremely negative \\(TS\\) is. Using the threshold method, we can also check if \\(TS&lt; t_{dfE,\\alpha}\\).\n\n\n\n\n\n\nNote\n\n\n\nWe use \\(t_{k, p}\\) to denote the \\(p\\)th quantile of the \\(t\\) distribution with \\(k\\) degrees of freedom. For \\(p=0.025\\) and large \\(k\\), this is approximately equal to 2.\n\n\n\nIn Example 3.1, test if the coefficient for weight is not equal to 1. Next, test if the coefficient for weight is greater than 1. Lastly, test if the coefficient for weight is not equal to 0.\n\nFirst, we have that \\[H_0\\colon \\beta_1= 15\\qquad vs \\qquad H_1:\\ \\beta_1\\neq 15.\\] Now, let’s execute the test:\n\n#changing matrix to scalar\nMSE=c(MSE)\nhvar_beta=solve(t(X)%*%X)*MSE\n\n\nTS=beta_hat[2]/sqrt(hvar_beta[2,2])\n\n# not equal\n# pt(x,df) is the CDF of a t distributed RV with df degrees of freedom at x. \np_val=2*(1-pt(abs(TS),dfe))\np_val\n\n[1] 0.0006434484\n\n# We can also use the model object to test if it is not equal to 0: \n# The test statistic and the pvalue are given in the t value and Pr(&gt;|t|)  columns, respectively. \nsummary(model)\n\n\nCall:\nlm(formula = BodyFat ~ Weight, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.5935  -5.7904   0.6536   5.2731  10.4004 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -27.37626   11.54743  -2.371 0.029119 *  \nWeight        0.24987    0.06065   4.120 0.000643 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.049 on 18 degrees of freedom\nMultiple R-squared:  0.4853,    Adjusted R-squared:  0.4567 \nF-statistic: 16.97 on 1 and 18 DF,  p-value: 0.0006434\n\n\nBased on the concepts that you have learned in 1131, and what we have reviewed in previous lectures, it also follows from the above analysis that a \\((1-\\alpha)100%\\) confidence interval for \\(\\beta_j\\) is\n\\[\n\\hat\\beta_j \\pm t_{dfE, \\alpha/2} \\sqrt{\\widehat{var}(\\hat\\beta_j)}.\n\\] :::{#exm-3-4-2} In Example 3.1, compute a 99% and a 95% confidence interval for the coefficient for weight. Which one is longer? Why? Interpret these intervals. :::\n\n# By hand\nbeta_hat[2]+c(-1,1)*qt(0.975,dfe)*sqrt(hvar_beta[2,2])\n\n[1] 0.1224448 0.3773035\n\nbeta_hat[2]+c(-1,1)*qt(0.995,dfe)*sqrt(hvar_beta[2,2])\n\n[1] 0.07528522 0.42446306\n\n# Auto software/using lm:\nconfint(model,level=0.95)\n\n                  2.5 %     97.5 %\n(Intercept) -51.6365090 -3.1160157\nWeight        0.1224448  0.3773035\n\nconfint(model,level=0.99)\n\n                   0.5 %    99.5 %\n(Intercept) -60.61484736 5.8623227\nWeight        0.07528522 0.4244631\n\n\nIf we took many samples of size 20 and computed a 95% (99%) confidence interval for each sample, then 95% (99%) of then would contain the true coefficient for the weight variable. We can conclude that with 95% (99%) confidence, the true coefficient for weight likely falls within (0.12, 0.38) ((0.08,0.42)).\n\n\n\n\n\n\nCaution\n\n\n\nThe key to understanding a confidence interval is to realize that the end points of the interval depend on the sample, and are therefore, random. On the other hand, the population parameter is not random, it is fixed. Therefore, if we drew a different sample, the interval would move, and there is a \\((1-100\\alpha )\\)% chance that that interval catches the population parameter. Most of the time it will contain the parameter, but not always.\n\n\nRecall that the point of computing a confidence interval is to report the uncertainty in our estimate that resulted from drawing a sample. We expect the true parameter to be somewhere in that range, and our best guess at the parameter is given by the center of the interval.\n\n\n3.3.7 Inference for the mean response and prediction intervals\nWe may wish to estimate the average response at a specific set of the covariates \\(x\\). Given \\(x\\), the theoretical mean response is \\(x^\\top \\beta\\). Given \\(x\\), we can estimate the mean response as \\(x^\\top \\hat\\beta\\). For instance, what is the average body fat percentage at 160 pounds? How accurate is our estimate? We can use a confidence interval to answer this question.\nNote that the expectation and variance of the estimate of the mean response are given by \\({\\textrm{E}}\\left[x^\\top \\hat\\beta\\right]=x^\\top \\beta\\) and \\({\\textrm{Var}}\\left[x^\\top \\hat\\beta\\right]=x^\\top(X^\\top X)^{-1}x\\sigma^2\\). Again, we must estimate \\(\\sigma\\) and we can write \\(\\hat{\\textrm{Var}}\\left[x^\\top \\hat\\beta\\right]=x^\\top(X^\\top X)^{-1}x MSE\\).\n\nExercise 3.13 Under the assumptions of the normal linear regression model, show that for a fixed covariate vector \\(x\\in\\mathbb{R}^p\\), \\(x^\\top \\hat\\beta\\) has a multivariate normal distribution and find it’s mean and variance. Argue that \\(\\frac{x^\\top\\hat\\beta - x^\\top \\beta}{\\sqrt{\\hat{\\textrm{Var}}\\left[x^\\top \\hat\\beta\\right]}}\\sim t_{dfE}\\).\n\nIt can be shown that a (\\(1-\\alpha)100\\)% confidence interval for the mean response \\({\\textrm{E}}\\left[Y|X=x\\right]\\) is \\[x^\\top\\hat\\beta\\pm t_{dfE,\\alpha/2}\\sqrt{\\hat{\\textrm{Var}}\\left[x^\\top \\hat\\beta\\right]}.\\]\nSimilarly, if we want to test \\[H_0\\colon {\\textrm{E}}\\left[Y|X=x\\right]=\\mu_0\\qquad vs \\qquad {\\textrm{E}}\\left[Y|X=x\\right]\\neq \\mu_0\\] we can do the following:\nThe observed test statistic is \\(TS(x,\\mu_0)=\\frac{x^\\top\\hat\\beta - \\mu_0}{\\sqrt{\\hat{\\textrm{Var}}\\left[x^\\top \\hat\\beta\\right]}}\\). Observe that under the null hypothesis, we have that \\(TS(x,\\mu_0)\\sim t_{dfE}\\). Therefore, the p-value is given by \\(2*\\Pr\\left(|TS(x,\\mu_0)|&gt;Z\\right)\\).\nSimilar to the previous section, we can also perform one-sided tests:\n\nRight-sided test \\((H_1\\colon x^\\top \\beta&gt;\\mu_0\\ )\\): p-value \\(\\Pr\\left(TS(x,\\mu_0)&gt;Z\\right)\\).\nLeft-sided test \\((H_1\\colon x^\\top \\beta&lt;\\mu_0\\ )\\): p-value \\(\\Pr\\left(TS(x,\\mu_0)&lt;Z\\right)\\).\n\nWe may also wish to predict what the response will be, given a new set of covariates. On top of that, we may again wish to quantify how much error there is in our prediction. For instance, what is the predicted body fat percentage of someone who is 160 pounds? Note that this differs from the previous section. In the previous section, we were interested in the average body fat percentage of someone who is 160 pounds. Here, we are interested in predicting the body fat percentage of a single, specific person, and not the average of the whole population.\nSpecifically, suppose that we have a subject whose covariates are given by \\(z\\), but we do not know the value of the subjects response, which we can denote by \\(Y_{new}\\). Then the true response is \\((Y_{new}|Z=z)\\ =z^\\top\\beta+\\epsilon_{new}\\).\nSuppose we want to predict \\(Y_{new}\\) and give an idea of how much error is in our prediction. The predicted response is known, and is given by \\({\\textrm{E}}\\left[Y_{new}|Z=z\\right]=z^\\top\\hat\\beta\\). We have \\({\\textrm{Var}}\\left[Y_{new}|Z=z\\right]={\\textrm{Var}}\\left[z\\hat\\beta\\right]+{\\textrm{Var}}\\left[\\epsilon_{new}\\right]=z^\\top(X^\\top X)^{-1}z\\sigma^2+\\sigma^2\\). Therefore, the variation in a new response is the variation in our estimate of \\(\\beta\\) plus the inherent population variation, \\(\\sigma^2\\). We have that this can be estimated with: \\(\\hat{\\textrm{Var}}\\left[Y_{new}|Z=z\\right]=z^\\top(X^\\top X)^{-1}zMSE+MSE\\).\n\nExercise 3.14 Under the assumptions of the normal linear regression model, show that for a fixed covariate vector \\(z\\in\\mathbb{R}^p\\), \\(Y_{new}|Z=z\\) has a multivariate normal distribution and find it’s mean and variance. Argue that given \\(Z=z\\), \\[\\frac{Y_{new}-z^\\top\\beta}{\\sqrt{\\hat{\\textrm{Var}}\\left[Y_{new}\\right]}} \\sim t_{dfE}.\\]\n\nTherefore, the (\\(1-\\alpha)100\\)% prediction interval for \\(Y_{new}\\) is given by: \\[z\\hat\\beta\\pm t_{dfE,\\alpha/2}\\sqrt{z^\\top(X^\\top X)^{-1}zMSE+MSE}.\\]\nNote that the prediction interval is wider than that of the mean response interval for the same covariate vector \\(z\\). That is because it is more difficult to predict the response for a specific person than it is to estimate a mean of a population. Furthermore, the interpretation of a prediction interval is different. A (\\(1-\\alpha)100\\)% prediction interval can be interpreted it as follows. Given a (\\(1-\\alpha)100\\)% prediction interval for \\(Y_{new}|Z=z\\), say \\((a,b)\\), we say that the probability \\(Y_{new}\\) is in \\((a,b)\\) is (\\(1-\\alpha)100\\)%. Note that this differs substantially from a confidence interval!\n\nExample 3.5 In Example 3.1, execute the following: What is a 95% confidence interval for the mean of someone who weighs 165 pounds? What is a 95% confidence interval for predicted BF% of someone who weighs 165 pounds? Interpret these intervals.\n\n\n# Intervals are given as follows:\n\nz &lt;- data.frame(Weight=165)\npredict(model, newdata = z, interval = 'confidence')\n\n       fit      lwr      upr\n1 13.85297 9.379675 18.32627\n\npredict(model, newdata = z, interval = 'prediction')\n\n       fit       lwr      upr\n1 13.85297 -1.617547 29.32349\n\n\nWe are 95% confident the mean body fat of a person who weighs 165 pounds is in 13.8529704, 9.3796749, 18.3262658. There is a 95% probability that the body fat of a person who weights 165 pounds is in 13.8529704, -1.6175473, 29.323488 . Note that the prediction interval is wider!\n\n\n3.3.8 Homework stop 4\n\nExercise 3.15 What is the difference between a prediction interval and an interval for the mean response ?\n\n\nExercise 3.16 Code the confidence intervals for the mean response and prediction interval without using the predict function.\n\n\nExercise 3.17 Do the chapter 3 practice problems from the problem list.\n\n\n\n3.3.9 Partial testing\nWe may be interested in executing the following hypothesis test: \\[H_0\\colon (\\beta_1,\\ldots,\\beta_k)=0\\qquad vs \\qquad (\\beta_1,\\ldots,\\beta_k)\\neq 0.\\] This amounts to testing whether the subset of variables \\((\\beta_1,\\ldots,\\beta_k)\\) adds anything to the model beyond \\((\\beta_{k+1},\\ldots,\\beta_p)\\). For example, you may be interested in whether location related covariates affect the price of Airbnb. The overall idea is to compare the reduced (null) model with \\(p-k\\) covariates to the complete (saturated, full) model (which contains all covariates).\nLet’s first review the \\(F\\)-test. We learned about the \\(F\\) test, which compares the following models: \\[Y|X=\\beta^\\top X+\\epsilon \\qquad vs \\qquad  Y|X=\\beta_1+\\epsilon.\\] Here, the complete model is given by \\(Y|X=\\beta^\\top X+\\epsilon\\) and the reduced model is given by \\(Y|X=\\beta_1+\\epsilon\\). Recall that the test statistic is given by \\[\\frac{SSM/dfM}{SSE/dfE}=\\frac{( SST-SSE)/(dfT-dfE)}{SSE/dfE},\\] where the degrees of freedom are in terms of the full model (not the null model). We could then rewrite this test statistic as \\[\\frac{SSM_C/dfM_C}{SSE_C/dfE_C}=\\frac{( SST_C-SSE_C)/(dfT_C-dfE_C)}{SSE_C/dfE_C},\\] where \\(C\\) stands for the complete model. (All that has changed is the notation, we added a \\(C\\) subscript.)\nNow, note that \\(SST=\\sum_{i=1}^n(Y_i-\\bar Y)^2\\) has nothing to do with what covariates are in the model. In other words, \\(SST\\) is always the same, not matter what covariates are in the model. Therefore, \\(SST_C=SST_R=SST\\), where \\(SST_R\\) stands for the “sum of squares total” in the reduced model. In our example of the \\(F\\) test, the least squares estimate of \\(\\beta_1\\) in the reduced model is \\(\\hat\\beta_1=\\bar Y\\) and the associated residual vector is given by \\(\\hat\\epsilon=Y-\\bar Y 1_n\\). But wait, observe that in this case, we have that \\(\\hat\\epsilon^\\top \\hat\\epsilon=SST!\\) Therefore, putting everything together, in this example, we have that \\(SSM_C=SST_C-SSE_C=SSE_R-SSE_C\\). That is, the model sum of squares for the complete model is the difference between the sum-squared error in the reduced model and the sum-squared error in the complete model. We can then rewrite the test statistic as \\[\\frac{( SSE_R-SSE_C)/(dfT_C-dfE_C)}{SSE_C/dfE_C}.\\] The difference \\(SSE_R-SSE_C\\) can be interpreted as the extra information gained from adding the covariates into the model OR total explained variations lost by going from the full model to the reduced model.\nThis idea can be generalized to develop a general method for testing hypotheses of the type: \\[H_0\\colon (\\beta_2,\\ldots,\\beta_k)=0\\qquad vs \\qquad (\\beta_2,\\ldots,\\beta_k)\\neq 0.\\] We complete the test as follows. Given a full model (which contains \\(\\beta_1,\\ldots,\\beta_p\\)) and reduced model (which contains \\(\\{\\beta_1,\\beta_{k+1},\\ldots,\\beta_p\\}\\)), define:\n\n\\(SSE_R-SSE_C=SSdrop\\)\n\\(dfE_R-dfE_C=dfdrop\\)\n\\(MSdrop=SSdrop/dfdrop\\)\n\nThen the test statistic and p-value are given by: \\(TS=MSdrop/MSE_C\\) and \\(\\Pr\\left(F_{dfdrop,dfE_C}\\geq TS\\right)\\), respectively.\nWe can interpret \\(SSE_R-SSE_C\\) as the extra info gained from adding the extra covariates into the model OR total explained variations lost by going from the full model to the reduced model. In addition, \\(dfE_R-dfE_C=k-1\\), or the number of covariates dropped from the full model to obtain the reduced model.\n\n\n\n\n\n\nNote\n\n\n\nIf you take \\(k=1\\), then this is equivalent to the \\(t\\)-test!\n\n\n\n\n3.3.10 Partial coefficient of determination\nWe can define the partial coefficient of determination as follows:\n\\[\\begin{align*}\n    R^2(X_1,\\ldots,X_{k-1}|X_{k},\\ldots,X_p)&=(SSE_R-SSE_C)/SSE_R\\\\\n    &=SSdrop/SSE_R.\n\\end{align*}\\] You might also see the partial correlation coefficient: \\[R(X_1,\\ldots,X_{k-1}|X_{k},\\ldots,X_p)=\\sqrt{ R^2(X_1,\\ldots,X_{k-1}|X_{k},\\ldots,X_p)}.\\]\nThis quantity is the extra proportion of variation explained from adding the covariates \\(X_1,\\ldots,X_{k-1}\\) to the model which already contains \\(X_{k},\\ldots,X_p\\).\n\nExample 3.6 A researcher ran an experiment to see if YouTube, Facebook and newspaper ads would improve sales. Run the partial \\(F\\) test to see how online advertising affects sales. Compute and interpret the following quantities:\n\n\\(SSE_R - SSE_C = SSdrop\\)\n\\(dfE_R - dfE_C = dfdrop\\)\n\\(MSdrop = SSdrop/dfdrop\\)\nTest stat: \\(TS = MSdrop/MSE_C\\)\np-value: \\(\\Pr (F_{dfdrop,dfE_C} \\geq TS)\\)\nPartial coefficient of determination\n\n\n\n# install.packages('datarium')                         \ndata(\"marketing\", package = \"datarium\")\n#printing out first few rows\nhead(marketing, 4)\n\n  youtube facebook newspaper sales\n1  276.12    45.36     83.04 26.52\n2   53.40    47.16     54.12 12.48\n3   20.64    55.08     83.16 11.16\n4  181.80    49.56     70.20 22.20\n\nplot(marketing)\n\n\n\n\n\n\n\n#setting n to be a variable (sample size)\nn=nrow(marketing)\n\n\n# Estimation: How to get an estimate $\\hat\\beta$ of $\\beta$?\n# lm( sales~   , data= marketing)\nfull_model&lt;- lm(sales ~ youtube+facebook+newspaper, data = marketing)\nsummary(full_model)\n\n\nCall:\nlm(formula = sales ~ youtube + facebook + newspaper, data = marketing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.5932  -1.0690   0.2902   1.4272   3.3951 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.526667   0.374290   9.422   &lt;2e-16 ***\nyoutube      0.045765   0.001395  32.809   &lt;2e-16 ***\nfacebook     0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.023 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\nsumm=summary(full_model)\n\nfull_model$coefficients\n\n (Intercept)      youtube     facebook    newspaper \n 3.526667243  0.045764645  0.188530017 -0.001037493 \n\nMSE=var(full_model$residuals); MSE\n\n[1] 4.029288\n\nMSE=summ$sigma^2\n\nSSE_C=sum(summ$residuals^2)\n\n\n\n# Inference: What is the error of $\\hat\\beta$? Is $f$ degenerate? I.e., is $\\beta=0$? \n\n#regular ANOVA\nsummary(full_model)\n\n\nCall:\nlm(formula = sales ~ youtube + facebook + newspaper, data = marketing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.5932  -1.0690   0.2902   1.4272   3.3951 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.526667   0.374290   9.422   &lt;2e-16 ***\nyoutube      0.045765   0.001395  32.809   &lt;2e-16 ***\nfacebook     0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.023 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n#confidence intervals for beta coefficients\nconfint.lm(full_model)\n\n                  2.5 %     97.5 %\n(Intercept)  2.78851474 4.26481975\nyoutube      0.04301371 0.04851558\nfacebook     0.17154745 0.20551259\nnewspaper   -0.01261595 0.01054097\n\n#Partial F Test\nmodel_red=lm(sales ~ newspaper, data = marketing)\nsum_reduced=summary(model_red)\nMSER=sum_reduced$sigma^2\nSSE_R=sum(sum_reduced$residuals^2)\n\nSSdrop=SSE_R-SSE_C\n\nMSEdrop=SSdrop/2\nFstat=MSEdrop/MSE\n\n1-pf(Fstat,2,196)\n\n[1] 0\n\npart_test=anova(model_red,full_model); part_test\n\nAnalysis of Variance Table\n\nModel 1: sales ~ newspaper\nModel 2: sales ~ youtube + facebook + newspaper\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    198 7394.1                                  \n2    196  801.8  2    6592.3 805.71 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npartial_c_det=SSdrop/SSE_R\n\n\n\nSSER=sum(model_red$residuals*model_red$residuals); SSER\n\n[1] 7394.119\n\ndfer=model_red$df.residual; dfer\n\n[1] 198\n\nSSEC=sum(full_model$residuals*full_model$residuals); SSEC\n\n[1] 801.8284\n\ndfeC=full_model$df.residual; dfeC\n\n[1] 196\n\nSSdrop=SSER-SSEC; SSdrop\n\n[1] 6592.29\n\ndfdrop=dfer-dfeC\n\nMSdrop=SSdrop/dfdrop; MSdrop\n\n[1] 3296.145\n\nR_online=SSdrop/SSER; R_online\n\n[1] 0.8915586\n\npart_test$F\n\n[1]       NA 805.7141\n\npart_test$`Pr(&gt;F)`\n\n[1]           NA 2.812622e-95\n\n# Prediction: Predict any values if necessary. \n# What if we have a 300$ budget and we only can pick one advertising method?\nnew_data=marketing[1:3,1:3]\nnew_data[1:3,]=diag(300,3)\npredict(full_model,new_data)\n\n        1         2         3 \n17.256061 60.085672  3.215419 \n\n# It's best to put our money in FB... meta?\n\n# What about intervals?\n\npredict(full_model,new_data, interval = 'confidence')\n\n        fit         lwr       upr\n1 17.256061 16.56191879 17.950203\n2 60.085672 55.25061022 64.920734\n3  3.215419 -0.09445737  6.525296\n\n\nIt’s a good time to stop and do another example to review the topics covered so far.\n\nExample 3.7 In the dataset mtcars we have the following variables:\n\nmpg: Miles/(US) gallon\ncyl: Number of cylinders\ndisp: Displacement (cu.in.)\nhp: Gross horsepower\ndrat: Rear axle ratio\nwt: Weight (1000 lbs)\nqsec: 1/4 mile time\nvs: V/S\nam: Transmission (0 = automatic, 1 = manual)\ngear: Number of forward gears\ncarb: Number of carburetors\n\nThe data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models). Overall, we would like to investigate the relationship between mpg and the following variables: cyl, disp, hp, drat, wt, qsec, gear, carb. Let’s investigate the following questions:\n\nAssume the normal MLR model. Store the covariate matrix and response in a variable. Fit a normal MLR model to the data. – That is use lm() to fit the model.\nWhat are the least squares estimates? What is the \\(MSE\\)?\nGenerate the ANOVA table. Is the model significant?\nTest if drat contributes anything to the model, adjusting for the other covariates. Test if drat is related to mpg, without adjusting for the other covariates.\nTest if the subset of variables gear, carb contribute to the model jointly, adjusting for the remaining covariates. What is the partial coefficient of determination? Interpret the partial coefficient of determination. Test if the subset of variables gear, carb contribute to the model jointly, without adjusting for the remaining covariates.\nCompute a confidence interval for the mean mpg of cars with the following set of covariate values rmtcars[1,-1]*1.1. Compute a prediction interval for thempg` of a car with the above set of covariate values.\nCompute a confidence interval for the coefficient for disp.\nCompute and interpret the coefficient of determination.\n\n\n\ndata(\"mtcars\")\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\ndim(mtcars)\n\n[1] 32 11\n\n# 1. \n# response~all variables minus the two variables we will not include\nmodel=lm(mpg~.-vs-am,data=mtcars)\nsumm=summary(model)\nsumm\n\n\nCall:\nlm(formula = mpg ~ . - vs - am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0230 -1.6874 -0.4109  0.9640  5.4400 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 17.88964   17.81996   1.004   0.3259  \ncyl         -0.41460    0.95765  -0.433   0.6691  \ndisp         0.01293    0.01758   0.736   0.4694  \nhp          -0.02085    0.02072  -1.006   0.3248  \ndrat         1.10110    1.59806   0.689   0.4977  \nwt          -3.92065    1.86174  -2.106   0.0463 *\nqsec         0.54146    0.62122   0.872   0.3924  \ngear         1.23321    1.40238   0.879   0.3883  \ncarb        -0.25510    0.81563  -0.313   0.7573  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.622 on 23 degrees of freedom\nMultiple R-squared:  0.8596,    Adjusted R-squared:  0.8107 \nF-statistic:  17.6 on 8 and 23 DF,  p-value: 4.226e-08\n\nX=model.matrix(model)\nY=mtcars$mpg\nX[1:5,]\n\n                  (Intercept) cyl disp  hp drat    wt  qsec gear carb\nMazda RX4                   1   6  160 110 3.90 2.620 16.46    4    4\nMazda RX4 Wag               1   6  160 110 3.90 2.875 17.02    4    4\nDatsun 710                  1   4  108  93 3.85 2.320 18.61    4    1\nHornet 4 Drive              1   6  258 110 3.08 3.215 19.44    3    1\nHornet Sportabout           1   8  360 175 3.15 3.440 17.02    3    2\n\nY\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n# 2. \nLSE=coef(model)\nLSE\n\n(Intercept)         cyl        disp          hp        drat          wt \n17.88963741 -0.41459575  0.01293240 -0.02084886  1.10109551 -3.92064847 \n       qsec        gear        carb \n 0.54145693  1.23321026 -0.25509911 \n\nMSE=summ$sigma^2\nMSE\n\n[1] 6.874941\n\n# 3. \nnull_model=lm(mpg~1,data=mtcars)\nanova(null_model,model)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ 1\nModel 2: mpg ~ (cyl + disp + hp + drat + wt + qsec + vs + am + gear + \n    carb) - vs - am\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     31 1126.05                                  \n2     23  158.12  8    967.92 17.599 4.226e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 4. \n# Notice the p value is 0.5 , not sign. \nsumm$coefficients['drat',]\n\n  Estimate Std. Error    t value   Pr(&gt;|t|) \n 1.1010955  1.5980601  0.6890201  0.4977032 \n\ndrat=lm(mpg~drat,,data=mtcars)\n# Notice the p value is 1.78e-05 , sig! explain this difference!\nsummary(drat)\n\n\nCall:\nlm(formula = mpg ~ drat, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.0775 -2.6803 -0.2095  2.2976  9.0225 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -7.525      5.477  -1.374     0.18    \ndrat           7.678      1.507   5.096 1.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.485 on 30 degrees of freedom\nMultiple R-squared:  0.464, Adjusted R-squared:  0.4461 \nF-statistic: 25.97 on 1 and 30 DF,  p-value: 1.776e-05\n\n# 5. \nred_model=lm(mpg~.-vs-am-gear-carb,data=mtcars)\nanova(red_model,model)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ (cyl + disp + hp + drat + wt + qsec + vs + am + gear + \n    carb) - vs - am - gear - carb\nModel 2: mpg ~ (cyl + disp + hp + drat + wt + qsec + vs + am + gear + \n    carb) - vs - am\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     25 163.48                           \n2     23 158.12  2    5.3532 0.3893 0.6819\n\nob=anova(red_model,model)\nob$`Sum of Sq`[2]/ob$RSS[1]\n\n[1] 0.0327457\n\n# 3.7% of the variation in mpg is explained from adding the covariate gear and carb to the model which contains cyl, disp, hp, drat, wt, qsec. \n\n# 6. \nnew_ob=c(6.6,176,121,4.29,2.882,18.106,0,1.1,4.4,4.4)\nnew_ob=matrix(new_ob,nrow=1,ncol=length(new_ob))\ncolnames(new_ob)=names(mtcars[1,-1])\nnew_ob=data.frame(new_ob)\npredict(model,new_ob, interval = 'confidence')\n\n       fit      lwr      upr\n1 22.43839 18.49468 26.38211\n\npredict(model,new_ob, interval = 'prediction')\n\n       fit     lwr      upr\n1 22.43839 15.7322 29.14459\n\n# 7. \nconfint(model)\n\n                   2.5 %      97.5 %\n(Intercept) -18.97375462 54.75302945\ncyl          -2.39565252  1.56646102\ndisp         -0.02343129  0.04929609\nhp           -0.06371601  0.02201829\ndrat         -2.20474377  4.40693480\nwt           -7.77195651 -0.06934042\nqsec         -0.74362628  1.82654014\ngear         -1.66782660  4.13424711\ncarb         -1.94235037  1.43215215\n\n#8. \nsumm$r.squared\n\n[1] 0.8595764\n\n# 85% of the variation in mpg is explained by cyl, disp, hp, drat, wt, qsec, gear and carb\n\n\nExercise 3.18 Interpret all of the above quantites.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "Unit_2.html#checking-model-assumptions",
    "href": "Unit_2.html#checking-model-assumptions",
    "title": "3  Linear Regression",
    "section": "3.4 Checking model assumptions",
    "text": "3.4 Checking model assumptions\nWe learned how to test significance of one or multiple variables, compute confidence intervals for the estimated coefficients, mean response, and predicted response. All the methods rely on the assumptions! Recall that we assume 1. The relationship is linear \\(Y|X=X\\beta+\\epsilon,\\) 2. \\(\\forall i\\in[n]\\), \\(\\epsilon_i\\sim \\mathcal{N}(0,\\sigma^2)\\) 3. \\(\\epsilon_i\\perp \\epsilon_j\\) for \\(i\\neq j,\\ i,j\\in [n]\\).\nWe now briefly discuss how to use the data to check if these assumptions are appropriate. We will cover this in more detail in the next chapter.\n\n3.4.1 Checking normality\nWe do not know \\(\\epsilon\\), however, we do know \\(\\hat\\epsilon\\), which is our best proxy for the true random error vector \\(\\epsilon\\). To check if the true random error vector is normally distributed we can use histograms and quantile-quantile plots. More specifically, if the histogram of the residuals looks more or less bell-shaped, with tails similar to the normal PDF, then the assumption of normality is valid.\nRecall that a qq-plot compares the quantiles of the sample to the quantiles of the theoretical normal distribution. The x-axis represents the theoretical quantiles. The y-axis represents the sample quantiles. If the sample follows a normal distribution, the points in the qq-plot will approximately lie on a line.\nInterpretation:\n\nStraight Line: If the points lie on or near the straight line, the sample appears normal.\nHeavy Tails: Points deviating upwards or downwards at the ends suggest the sample has heavier or lighter tails than the normal distribution.\nS-Shape: Points forming an S-shape indicate the sample has lighter tails and a heavier center than the normal distribution.\n\nSee below for an example:  Note that you will always have some deviation at the ends of the line in the qq-plot.\n\nExample 3.8 In examples Example 3.1 and Example 3.6, check that the normality assumption is valid.\n\n\n# Make the data frame\nWeight=c(175 , 181 , 200 , 159 , 196 , 192 , 205 , 173 , 187 , 188 , \n         188 , 240 , 175 , 168 , 246 , 160 , 215 , 159 , 146 , 219 )\nBodyFat =c(6 , 21 , 15 , 6 , 22 , 31 , 32 ,21 , 25 , 30 , \n           10 , 20 , 22 , 9 , 38 , 10 , 27 , 12 , 10 , 28 )\n\ndf=data.frame(cbind(Weight=Weight,BodyFat=BodyFat))\nmodel= lm(BodyFat ~Weight, data = df)\nsummary(model)\n\n\nCall:\nlm(formula = BodyFat ~ Weight, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.5935  -5.7904   0.6536   5.2731  10.4004 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -27.37626   11.54743  -2.371 0.029119 *  \nWeight        0.24987    0.06065   4.120 0.000643 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.049 on 18 degrees of freedom\nMultiple R-squared:  0.4853,    Adjusted R-squared:  0.4567 \nF-statistic: 16.97 on 1 and 18 DF,  p-value: 0.0006434\n\ncar::qqPlot(model$residuals,pch=22)\n\n\n\n\n\n\n\n\n[1] 12  6\n\nhist(model$residuals,breaks=10,xlim=c(-11,11))\n\n\n\n\n\n\n\n# This appears okay!\n\n# Let's do the next example \n# install.packages('datarium')                         \ndata(\"marketing\", package = \"datarium\")\n\n\n# lm( sales~   , data= marketing)\nfull_model&lt;- lm(sales ~ youtube+facebook+newspaper, data = marketing)\nsummary(full_model)\n\n\nCall:\nlm(formula = sales ~ youtube + facebook + newspaper, data = marketing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.5932  -1.0690   0.2902   1.4272   3.3951 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.526667   0.374290   9.422   &lt;2e-16 ***\nyoutube      0.045765   0.001395  32.809   &lt;2e-16 ***\nfacebook     0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.023 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n# Not great. \ncar::qqPlot(full_model$residuals,pch=22)\n\n\n\n\n\n\n\n\n[1] 131   6\n\nhist(full_model$residuals,breaks=10,xlim=c(-11,11))\n\n\n\n\n\n\n\n\n\n\n3.4.2 Checking the other assumptions\nTo check the remaining assumptions (constant variance, independence of residuals, zero mean and linear relationship), we can use some other diagnostic plots.\nOne plot is that of the fitted values \\(\\hat Y\\) (\\(x\\)-axis) against the residuals \\(\\hat\\epsilon\\) (\\(y\\)-axis). If the error depends on \\(\\hat y\\), then the identically distributed assumption on the errors is probably not valid. If the assumptions are valid, we should observe on the plots that at all levels of the response, the mean of the residuals is 0 and the variance remains the same. Thus, we should see a horizontal band centered at 0 containing the observations.\nThis appears to be the case in the body fat example:\n\nplot(model$fitted.values,model$residuals,pch=22,bg=1)\n\n\n\n\n\n\n\n\nObserve that in the marketing example, the residuals admit a pattern. This usually indicates either a non-linear relationship with the covariates, or an important covariate is missing. In this case, we would say the assumption of identically distributed errors is violated.\n\nplot(full_model$fitted.values,full_model$residuals,pch=22,bg=1)\n\n\n\n\n\n\n\n\nPlotting the residuals against the covariates can reveal dependence between the errors. For instance, if time is a covariate, you can plot the residuals over time to see if they have any relationship with time. If there appears to be dependence among the residuals, then the assumptions of the model are violated. That is, in these plots we should also see a horizontal band centered at 0 containing the observations. If not, then the residuals have a relationship with the given covariate.\n\nBe VERY careful about the scale of your plot, as it can affect your interpretation. Zooming out or in too much can make everything look fine. In addition, the \\(y\\)-axis not being centered at 0 can cause you to misinterpret the plot.\n\n\nplot(marketing$youtube,full_model$residuals,pch=22,bg=1)\n\n\n\n\n\n\n\nplot(marketing$facebook,full_model$residuals,pch=22,bg=1)\n\n\n\n\n\n\n\nplot(marketing$newspaper,full_model$residuals,pch=22,bg=1)\n\n\n\n\n\n\n\n\n\nplot(marketing$youtube,full_model$residuals,pch=22,bg=1,ylim=c(-10,10))\n\n\n\n\n\n\n\nplot(marketing$facebook,full_model$residuals,pch=22,bg=1,ylim=c(-10,10))\n\n\n\n\n\n\n\nplot(marketing$newspaper,full_model$residuals,pch=22,bg=1,ylim=c(-10,10))\n\n\n\n\n\n\n\n\nNotice how the newspaper plot changes with the new axis limits. It appears that the variance of the error is changing with the value of the facebook and youtube budgets.\nAnother plot is that of the fitted values against the residuals. This gives an idea of the overall fit of the model. We should observe the points scatters around the line \\(y=x\\).\n\nplot(full_model$fitted.values,marketing$sales,pch=22,bg=1)\nabline(0,1)\n\n\n\n\n\n\n\n\nNotice that the line is slightly curved above the line at the ends. This means that at high and low values, the actual sales are empirically greater than as predicted by the model. Let’s plot the actual data.\n\nplot(marketing,pch=22,bg=1)\n\n\n\n\n\n\n\n\nIn this case, Youtube and facebook spending seems to have a nonlinear relationship with sales. We will see how to remedy this in later chapters.\nAs a final note, observe that we can put the model object in the plot() function to obtain the diagnostic plots.\n\nplot(full_model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will learn in later chapters how to check the assumptions more thoroughly and how to remedy violations of the assumptions.\n\n\n3.4.3 Homework stop 5\nComplete the assigned textbook problems for Chapter 4.\n\nExercise 3.19 List the assumptions for the normal MLR model and the MLR model. Write down how you would check each assumption.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "Unit_2.html#simple-linear-regression",
    "href": "Unit_2.html#simple-linear-regression",
    "title": "3  Linear Regression",
    "section": "3.5 Simple linear regression",
    "text": "3.5 Simple linear regression\nA special case of the multiple linear regression is simple linear regression. A simple linear regression model is a regression model with one explanatory variable: \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\). \\[\ny = \\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right),\nX = \\left( \\begin{array}{cc} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{array} \\right),\n\\beta = \\left( \\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array} \\right),\n\\epsilon =  \\left( \\begin{array}{c} \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n\n\\end{array} \\right).\n\\]\n\n3.5.1 Estimated Coefficients\nIn this case, following some matrix manipulations (verify this for homework), we have \\[\nX^\\top X = \\left( \\begin{array}{cc} n & \\sum_{i=1}^nx_i \\\\ \\sum_{i=1}^nx_i & \\sum_{i=1}^nx_i^2 \\end{array} \\right),\\\nX^\\top y = \\left( \\begin{array}{cc} \\sum_{i=1}^ny_i \\\\ \\sum_{i=1}^nx_i y_i \\end{array} \\right).\n\\] Now, recall if \\[ A = \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix} \\] then \\[ A^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix}\nd & -b \\\\\n-c & a\n\\end{bmatrix}. \\]\nFrom MATH 1131 (or simple algebraic manipulation), we know \\[\\begin{equation*}\n    \\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})  =  \\sum_{i=1}^nx_i y_i - n\\bar{x}\\bar{y} =\n\\sum_{i=1}^nx_i y_i - n^{-1}\\left( \\sum_{i=1}^nx_i \\right) \\left( \\sum_{i=1}^ny_i \\right)\n\\end{equation*}\\] \\[\\begin{equation*}\n\\sum_{i=1}^n(x_i - \\bar{x})^2  =  \\sum_{i=1}^nx_i^2 - n\\bar{x}^2 =\n\\sum_{i=1}^nx_i^2 - n^{-1}\\left( \\sum_{i=1}^nx_i \\right)^2.\n\\end{equation*}\\] Therefore \\[\\begin{eqnarray*}\n(X^\\top X)^{-1} & = & \\frac{1}{n \\sum_{i=1}^nx_i^2 - (\\sum_{i=1}^nx_i)^2}\n\\left( \\begin{array}{cc} \\sum_{i=1}^nx_i^2 & - \\sum_{i=1}^nx_i \\\\ - \\sum_{i=1}^nx_i & n \\end{array} \\right) \\\\\n& = & \\frac{1}{n \\sum_{i=1}^n(x_i - \\bar{x})^2}\n\\left( \\begin{array}{cc} \\sum_{i=1}^nx_i^2 & - \\sum_{i=1}^nx_i \\\\ - \\sum_{i=1}^nx_i & n \\end{array} \\right).\n\\end{eqnarray*}\\]\nTo summarize: \\[\\begin{align*}\nX^\\top X &=  \\begin{bmatrix} n & \\sum_{i=1}^nx_i \\\\ \\sum_{i=1}^nx_i & \\sum_{i=1}^nx_i^2 \\end{bmatrix}\\\\\nX^\\top y&= \\begin{bmatrix}\\sum_{i=1}^ny_i \\\\ \\sum_{i=1}^nx_i y_i \\end{bmatrix}\\\\\n(X^\\top X)^{-1} & =  \\frac{1}{n \\sum_{i=1}^n(x_i - \\bar{x})^2}\n\\begin{bmatrix} \\sum_{i=1}^nx_i^2 & - \\sum_{i=1}^nx_i \\\\ - \\sum_{i=1}^nx_i & n\\end{bmatrix}\n\\end{align*}\\] Now, we have that \\[\\begin{eqnarray*}\n\\hat\\beta & = & \\left( \\begin{array}{c} \\hat\\beta_0 \\\\ \\hat\\beta_1 \\end{array} \\right) =\n(X^\\top X)^{-1} X^\\top y \\\\\n& = & \\frac{1}{n \\sum_{i=1}^n(x_i - \\bar{x})^2}\n\\left( \\begin{array}{c} \\sum_{i=1}^nx_i^2 \\ \\sum_{i=1}^ny_i - \\sum_{i=1}^nx_i \\ \\sum_{i=1}^nx_i y_i \\\\\n- \\sum_{i=1}^nx_i \\ \\sum_{i=1}^ny_i + n \\sum_{i=1}^nx_i y_i \\end{array} \\right).\n\\end{eqnarray*}\\]\nNow, \\[\\hat\\beta_1=\\frac{1}{n \\sum_{i=1}^n(x_i - \\bar{x})^2}\n\\left(- \\sum_{i=1}^nx_i \\ \\sum_{i=1}^ny_i + n \\sum_{i=1}^nx_i y_i\\right).\\]\n\nExercise 3.20 Let’s show that \\[\n\\hat\\beta_1 = \\frac{n \\sum_{i=1}^nx_i y _i - \\sum_{i=1}^nx_i \\ \\sum_{i=1}^ny_i}{n \\sum_{i=1}^n(x_i - \\bar{x})^2}\n= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2}.\n\\]\n\nDoes this look familiar? We see that, \\[\n\\hat\\beta_1 = \\hat{{\\textrm{cov}}}(X,Y)\\frac{\\hat\\sigma_y}{\\hat\\sigma_x},\n\\] where \\(\\hat{{\\textrm{cov}}}(X,Y)\\) is the estimated correlation between \\(X\\) and \\(Y\\). Let’s interpret this:\n\nIf \\(\\hat{{\\textrm{cov}}}(X,Y)\\approx 0\\) then \\(\\hat\\beta_1\\approx 0\\) - low correlation implies an estimated slope close to 0.\nThe estimated coefficient \\(\\hat\\beta_1\\) is the product of the estimated correlation between \\(X\\) and \\(Y\\) and the ratio of the estimated standard deviation of \\(Y\\) to that of \\(X\\).\n\nNow, looking at the intercept term, we have \\[\\hat\\beta_0=\\frac{1}{n \\sum_{i=1}^n(x_i - \\bar{x})^2}\n\\left(\\sum_{i=1}^nx_i^2 \\ \\sum_{i=1}^ny_i - \\sum_{i=1}^nx_i \\ \\sum_{i=1}^nx_i y_i\\right).\\]\n\nExercise 3.21 Show that \\[\\hat\\beta_0=\\bar y - \\hat\\beta_1\\bar x.\n\\]\n\nObserve that the intercept is the mean of \\(Y\\) minus the mean of \\(X\\) times the estimated slope. In essence, it tells us that the intercept \\((\\hat{\\beta}_0)\\) represents the value of \\((X)\\) when \\((X)\\) is at its mean value (\\((\\bar{X})\\)) and that \\((\\bar{X})\\) is adjusted by subtracting the contribution of \\((\\hat{\\beta}_1 \\bar{X})\\).\nThis adjustment ensures that the regression line passes through the point \\(((\\bar{X}, \\bar{X}))\\), which is the point of averages for the data.\n\n\n3.5.2 Inference in SLR\nWe can also simplify the values used for inference in the SLR model. Recall that \\({\\textrm{Var}}\\left[\\hat\\beta\\right] = (X^\\top X)^{-1} \\sigma^2\\), and so we have \\[\\begin{eqnarray*}\n{\\textrm{Var}}\\left[\\hat\\beta_0\\right] & = & \\frac{\\sum_{i=1}^nx_i^2}{n \\sum_{i=1}^n(x_i - \\bar{x})^2} \\sigma^2\n= \\frac{\\sum_{i=1}^nx_i^2 - n \\bar{x}^2 + n \\bar{x}^2}{n \\sum_{i=1}^n(x_i - \\bar{x})^2} \\sigma^2  \\\\\n& = & \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\right] \\sigma^2  \\\\\n{\\textrm{Var}}\\left[\\hat\\beta_1\\right] & = & \\frac{1}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\sigma^2  \\\\\n{\\textrm{cov}}({\\hat\\beta_0, \\hat\\beta_1}) & = & - \\frac{\\bar{x}}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\sigma^2.\n\\end{eqnarray*}\\]\nWe know from previous sections that a \\((1 - \\alpha)100%\\) confidence interval of \\(\\beta_i\\), where \\(i = 0, 1\\), is \\[\n\\hat\\beta_i \\pm t_{df_E, \\alpha/2} \\sqrt{\\widehat{var}(\\hat\\beta_i)}.\n\\] Similarly, let \\(\\beta_i^0\\) be a hypothesized value of \\(\\beta_i\\), for \\(i=0, 1\\). If we want to test whether \\(\\beta_i=\\beta_i^0\\), then the observed test statistic is given by \\[\n\\frac{\\hat\\beta_i - \\beta_i^0}{\\sqrt{\\widehat{var}(\\hat\\beta_i)}},\n\\] and the corresponding \\(p\\)-value is obtained via the \\(t_{dfE}\\) distribution as usual.\n\n\n\n\n\n\nNote\n\n\n\nSimilarly, inference for the mean response and predictions can be obtained. We can also simplify the \\(ANOVA\\) table, \\(R^2\\), etc. For instance, the \\(R^2\\) is the square of the sample correlation coefficient between \\(X\\) and \\(Y\\).\n\n\n\n\n3.5.3 Inference for the correlation coefficient\nIf we are interested in doing a hypothesis test, or constructing confidence intervals for the correlation between two variables, say \\(X\\) and \\(Y\\), we can use the simple linear regression model.\nWe have already derived the relationship between the estimated correlation coefficient and the estimated slope of the simple linear regression model. More specifically, if the estimated correlation coefficient is 0, then the estimated slope of the simple linear regression is 0. One can show that the same relationship holds at the population level: \\(\\beta_1=\\rho \\sigma_y/\\sigma_x\\), where \\(\\rho={\\textrm{corr}}\\left[X,Y\\right]\\).\nNow, suppose that we want to test if \\(H_0: \\rho =0\\) versus \\(H_a: \\rho \\neq 0\\). Using the fact that \\(\\beta_1=\\rho \\sigma_y/\\sigma_x\\), the above test is equivalent to the statement \\(H_0: \\beta_1 = 0\\) versus \\(H_a: \\beta_1 \\neq 0\\). Therefore, we can just test if the slope parameter in the model \\(Y|X=\\beta_0+\\beta_1 X+\\epsilon\\) is 0.\nLetting \\(\\hat\\rho=\\hat{{\\textrm{corr}}}(X,Y)\\) The observed test statistic is then: \\[\n\\frac{\\hat\\beta_1}{\\sqrt{\\widehat{var}(\\hat\\beta_1)}} = \\frac{\\hat\\rho \\sqrt{n-2}}{\\sqrt{1 - \\hat\\rho^2}},\n\\] and the corresponding \\(p\\)-value is obtained based on the \\(t_{dfE}\\) distribution.\nHowever, when the hypothesized value for \\(\\rho\\) is non-zero, the problem becomes very complicated. The exact distribution of \\(\\hat\\rho\\) is extremely difficult to obtain under the null hypothesis. The following procedure gives an approximation of the distribution of a function of \\(\\hat\\rho\\) under the null hypothesis. In particular, Fisher suggested the transformation for \\(\\rho \\in (0, 1)\\), \\[\n\\theta = \\frac{1}{2} \\log \\frac{1 + \\rho}{1 - \\rho}.\n\\] Then \\[\n\\hat\\theta = \\frac{1}{2} \\log \\frac{1 + \\hat\\rho}{1 - \\hat\\rho},\n\\] is an estimate of \\(\\theta\\), where \\(\\hat\\theta\\) is approximately distributed as normal with mean \\(\\theta\\) and variance \\(\\frac{1}{n-3}\\). Hence, an approximate \\((1 - \\alpha)100%\\) confidence interval of \\(\\theta\\) is \\[\n\\hat\\theta \\pm z_{\\alpha/2} \\sqrt{\\frac{1}{n-3}},\n\\] and the corresponding confidence interval of \\(\\rho\\) can be obtained by the inverse transformation. Similarly, if the hypothesized value of \\(\\rho\\) is \\(\\rho_0\\), then the hypothesized value of \\(\\theta\\) is \\(\\theta_0 = \\frac{1}{2} \\log \\frac{1 + \\rho_0}{1-\\rho_0}\\). The observed test statistic can be obtained and the corresponding \\(p\\)-value can be obtained based on the standard normal distribution.\n\nExample 3.9 In Example 3.1 test if the correlation between body fat and weight is 0. Next, test if the correlation is greater than 1/2. Construct a 95% CI for \\(\\rho.\\)\n\n\n############################ Exploratory\nWeight=c(175 , 181 , 200 , 159 , 196 , 192 , 205 , \n         173 , 187 , 188 , 188 , 240 , 175 , 168 , \n         246 , 160 , 215 , 159 , 146 , 219 )\nBodyFat =c(6 , 21 , 15 , 6 , 22 , 31 , 32 ,21 , 25 , \n           30 , 10 , 20 , 22 , 9 , 38 , 10 , 27 , 12 , 10 , 28 )\n\ndf=data.frame(cbind(Weight=Weight,BodyFat=BodyFat))\n\n\ncor(df)\n\n           Weight   BodyFat\nWeight  1.0000000 0.6966328\nBodyFat 0.6966328 1.0000000\n\nhist(df$Weight,freq=F)\n\n\n\n\n\n\n\nhist(df$BodyFat,freq=F)\n\n\n\n\n\n\n\nsummary(df)\n\n     Weight         BodyFat     \n Min.   :146.0   Min.   : 6.00  \n 1st Qu.:171.8   1st Qu.:10.00  \n Median :187.5   Median :21.00  \n Mean   :188.6   Mean   :19.75  \n 3rd Qu.:201.2   3rd Qu.:27.25  \n Max.   :246.0   Max.   :38.00  \n\ncor(df)[1,2]\n\n[1] 0.6966328\n\nX=cbind(rep(1,nrow(df)), df$Weight)\nY=df$BodyFat\n\nbeta_hat= solve(t(X)%*%X)%*%t(X)%*%Y\nbeta_hat\n\n            [,1]\n[1,] -27.3762623\n[2,]   0.2498741\n\nmodel=lm(BodyFat~ Weight,df)\nmodel\n\n\nCall:\nlm(formula = BodyFat ~ Weight, data = df)\n\nCoefficients:\n(Intercept)       Weight  \n   -27.3763       0.2499  \n\nsummary(model)\n\n\nCall:\nlm(formula = BodyFat ~ Weight, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.5935  -5.7904   0.6536   5.2731  10.4004 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -27.37626   11.54743  -2.371 0.029119 *  \nWeight        0.24987    0.06065   4.120 0.000643 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.049 on 18 degrees of freedom\nMultiple R-squared:  0.4853,    Adjusted R-squared:  0.4567 \nF-statistic: 16.97 on 1 and 18 DF,  p-value: 0.0006434\n\ncor(df)[1,2]^2\n\n[1] 0.4852972\n\ncor(df)[1,2]\n\n[1] 0.6966328\n\na=function(x){\n  (exp(2*x)-1)/(exp(2*x)+1)\n}\na(1.36)\n\n[1] 0.8763931\n\n\n\n\n3.5.4 Homework stop 6\n\nComplete the Chapter 2 questions in the textbook.\n\n\nExercise 3.22 For \\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\] \\[\ny = \\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right),\nX = \\left( \\begin{array}{cc} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{array} \\right),\n\\beta = \\left( \\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array} \\right),\n\\epsilon =  \\left( \\begin{array}{c} \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n\n\\end{array} \\right)\n\\]\n\nCompute \\(\\hat\\beta\\), \\({\\textrm{Var}}\\left[\\hat\\beta_1\\right]\\), \\({\\textrm{Var}}\\left[\\hat\\beta_0\\right]\\), \\({\\textrm{cov}}\\left[(\\right]\\hat\\beta_0,\\hat\\beta_1)\\)\nShow \\(\\hat\\beta_1=r\\frac{\\hat\\sigma_y}{\\hat\\sigma_x}\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "Unit_2.html#additional-concepts-examples",
    "href": "Unit_2.html#additional-concepts-examples",
    "title": "3  Linear Regression",
    "section": "3.6 Additional concepts & examples",
    "text": "3.6 Additional concepts & examples\nHere we touch on a few important examples and notes about the MLR.\n\n3.6.1 Beware scatter plots in MLR\nSometimes, scatter plots are misleading for determining the relationship between \\(Y\\) and a collection of \\(p\\) covariates. In the following example, it appears that \\(X1\\) and \\(Y\\) do not have a relationship, when in fact they do. Generally, this phenomena goes away with higher sample sizes.\n\n# Scatter diagram beware?\n# x1=c(2,3,4,1,5,6,7,8)\n# x2=c(2,3,4,1,5,6,7,8)\n# x=c(2,3,4,1,5,6,7,8)\n\n# x1=1:8\n# x2=c(2,1:6,4)\n# y=8-5*x1+12*x2+rnorm(8,0,2)\n\nset.seed(445)\n\n\n\nn=8\nx1=rnorm(n,5,5)\nx2=rnorm(n,3,5)\ny=8-5*x1+12*x2+rnorm(n,0,2)\n\ndf=data.frame(cbind(y,x1,x2))\n\nplot(df)\n\n\n\n\n\n\n\n\nNext, we do an example from the textbook, which uses the NFL data. Specifically, we try to evaluate the relationship between number of wins and several explanatory variables.\n\nExample 3.10 Using the following NFL data, complete 3.1-3.4, 4.1 and 4.2 in the textbook.\n\n################## NFL example #################\n# This gives you the data sets used in the textbook\n# install.packages('MPV')\ndf=MPV::table.b1\n# Note for more information, run ?MPV::table.b1\n\nhead(df)\n\n   y   x1   x2   x3   x4 x5  x6   x7   x8   x9\n1 10 2113 1985 38.9 64.7  4 868 59.7 2205 1917\n2 11 2003 2855 38.8 61.3  3 615 55.0 2096 1575\n3 11 2957 1737 40.1 60.0 14 914 65.6 1847 2175\n4 13 2285 2905 41.6 45.3 -4 957 61.4 1903 2476\n5 10 2971 1666 39.2 53.8 15 836 66.1 1457 1866\n6 11 2309 2927 39.7 74.1  8 786 61.0 1848 2339\n\n# names too long\nnames(df)\n\n [1] \"y\"  \"x1\" \"x2\" \"x3\" \"x4\" \"x5\" \"x6\" \"x7\" \"x8\" \"x9\"\n\n# rename to make it easier\nnames(df)=c(\"Wins\",\"RushY\",\"PassY\",\"PuntA\",\"FGP\",\"TurnD\",\"PenY\",\"PerR\",\"ORY\",\"OPY\")\nnames(df)\n\n [1] \"Wins\"  \"RushY\" \"PassY\" \"PuntA\" \"FGP\"   \"TurnD\" \"PenY\"  \"PerR\"  \"ORY\"  \n[10] \"OPY\"  \n\n\n\n\n# Wins~ beta_1+beta_2Passing yrds+beta_3per_rush+beta_4ORY+epsilon\n# summary(df)\n# plot(df)\n# run the model \nregression_model=lm(  Wins ~ PassY+PerR+ORY  ,data= df )\nsummary(regression_model)\n\n\nCall:\nlm(formula = Wins ~ PassY + PerR + ORY, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0370 -0.7129 -0.2043  1.1101  3.7049 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.808372   7.900859  -0.229 0.820899    \nPassY        0.003598   0.000695   5.177 2.66e-05 ***\nPerR         0.193960   0.088233   2.198 0.037815 *  \nORY         -0.004816   0.001277  -3.771 0.000938 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.706 on 24 degrees of freedom\nMultiple R-squared:  0.7863,    Adjusted R-squared:  0.7596 \nF-statistic: 29.44 on 3 and 24 DF,  p-value: 3.273e-08\n\n# model=lm(Wins~PassY+PerR+ORY,data=df)\n# get the confidence intervals. \nconfint(regression_model)\n\n                    2.5 %       97.5 %\n(Intercept) -18.114944410 14.498200293\nPassY         0.002163664  0.005032477\nPerR          0.011855322  0.376065098\nORY          -0.007451027 -0.002179961\n\n\nWhat conclusions can you make from this output? - All variables seem important! For isntance, we sdee that for every 1% increase in percentage rushing, there is a 0.193960 increase in number of wins, on average, holding passing yards and opponent rushing yards constant.\n\n#### CI\n# mean response of z'\\beta , z=(2000,60,1900)'\nnew_data=data.frame(  matrix(c(2000,60,1900),ncol=3) )\nnames(new_data)\n\n[1] \"X1\" \"X2\" \"X3\"\n\nnames(new_data)=c( 'PassY','PerR','ORY' )\n\npredict(regression_model, new_data  ,   interval = 'confidence')\n\n       fit      lwr      upr\n1 7.875942 7.072672 8.679213\n\npredict(regression_model, new_data  ,   interval = 'predict')\n\n       fit      lwr     upr\n1 7.875942 4.263986 11.4879\n\n## ANOVA \n\nregression_model_reduced=lm(  Wins ~ 1  ,data= df )\nsummary(regression_model_reduced)\n\n\nCall:\nlm(formula = Wins ~ 1, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9643 -2.9643 -0.4643  3.0357  6.0357 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.9643     0.6576   10.59 4.09e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.48 on 27 degrees of freedom\n\nanova(regression_model_reduced,regression_model)\n\nAnalysis of Variance Table\n\nModel 1: Wins ~ 1\nModel 2: Wins ~ PassY + PerR + ORY\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     27 326.96                                  \n2     24  69.87  3    257.09 29.437 3.273e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# subset test\n\nregression_model_reduced=lm(  Wins ~ PassY ,data= df )\nanova(regression_model_reduced,regression_model)\n\nAnalysis of Variance Table\n\nModel 1: Wins ~ PassY\nModel 2: Wins ~ PassY + PerR + ORY\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     26 250.77                                  \n2     24  69.87  2     180.9 31.069 2.189e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# summary(df)\n\nsummary(regression_model)\n\n\nCall:\nlm(formula = Wins ~ PassY + PerR + ORY, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0370 -0.7129 -0.2043  1.1101  3.7049 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.808372   7.900859  -0.229 0.820899    \nPassY        0.003598   0.000695   5.177 2.66e-05 ***\nPerR         0.193960   0.088233   2.198 0.037815 *  \nORY         -0.004816   0.001277  -3.771 0.000938 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.706 on 24 degrees of freedom\nMultiple R-squared:  0.7863,    Adjusted R-squared:  0.7596 \nF-statistic: 29.44 on 3 and 24 DF,  p-value: 3.273e-08\n\n# anova(regression_model)\nsumm=summary(regression_model)\nsumm$r.squared\n\n[1] 0.7863069\n\nsumm$adj.r.squared\n\n[1] 0.7595953\n\nregression_model2=lm(Wins~PassY+ORY,data=df)\n\n\nSSER=sum(regression_model2$residuals*regression_model2$residuals); SSER\n\n[1] 83.9382\n\ndfer=regression_model2$df.residual; dfer\n\n[1] 25\n\nSSEC=sum(regression_model$residuals*regression_model$residuals); SSEC\n\n[1] 69.87\n\ndfeC=regression_model$df.residual; dfeC\n\n[1] 24\n\nSSdrop=SSER-SSEC; SSdrop\n\n[1] 14.06819\n\ndfdrop=dfer-dfeC\n\nMSdrop=SSdrop/dfdrop; MSdrop\n\n[1] 14.06819\n\nR_prp=SSdrop/SSER; R_prp\n\n[1] 0.1676018\n\nMSdrop\n\n[1] 14.06819\n\n1-pf(MSdrop,dfdrop,dfeC)\n\n[1] 0.000986662\n\ncor(regression_model$fitted.values , df$Wins)^2\n\n[1] 0.7863069\n\nconfint(regression_model2)\n\n                   2.5 %       97.5 %\n(Intercept)  9.321778092 20.103571885\nPassY        0.001654121  0.004568143\nORY         -0.008797465 -0.004819085\n\nnew_data=df[1,c(3,8,9)]\nnew_data[1,]=c(2300 , 56 , 2100)\npredict(regression_model2,new_data,interval = 'confidence')\n\n     fit      lwr      upr\n1 7.5709 6.814662 8.327138\n\n#########  check the fit #######\nMSE=summ$sigma^2\nqqnorm(regression_model2$residuals/summ$sigma,pch=22,bg=1)\nabline(0,1)\n\n\n\n\n\n\n\nhist(regression_model2$residuals,breaks=6)\n\n\n\n\n\n\n\nplot(regression_model2$fitted.values,regression_model$residuals,pch=22,bg=1)\nabline(h=0)\n\n\n\n\n\n\n\nn=nrow(df)\nplot(1:n,regression_model2$residuals,pch=22,bg=1)\nabline(h=0)\n\ntime=(1:n)\nres=lm(regression_model2$residuals~time)\nsummary(res)\n\n\nCall:\nlm(formula = regression_model2$residuals ~ time)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.36425 -1.04520 -0.07845  1.16457  2.40353 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.8479     0.5610   3.294 0.002852 ** \ntime         -0.1274     0.0338  -3.771 0.000848 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.445 on 26 degrees of freedom\nMultiple R-squared:  0.3535,    Adjusted R-squared:  0.3286 \nF-statistic: 14.22 on 1 and 26 DF,  p-value: 0.0008481\n\nlines(time,res$fitted.values)\n\n\n\n\n\n\n\nregression_model3=lm(Wins~PerR+ORY,data=df)\nsumm3=summary(regression_model3)\nsumm3\n\n\nCall:\nlm(formula = Wins ~ PerR + ORY, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7985 -1.5166 -0.5792  1.9927  4.5248 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 17.944319   9.862484   1.819  0.08084 . \nPerR         0.048371   0.119219   0.406  0.68839   \nORY         -0.006537   0.001758  -3.719  0.00102 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.432 on 25 degrees of freedom\nMultiple R-squared:  0.5477,    Adjusted R-squared:  0.5115 \nF-statistic: 15.13 on 2 and 25 DF,  p-value: 4.935e-05\n\nsumm3$r.squared\n\n[1] 0.5476628\n\nsumm3$adj.r.squared\n\n[1] 0.5114759\n\nconfint(regression_model)\n\n                    2.5 %       97.5 %\n(Intercept) -18.114944410 14.498200293\nPassY         0.002163664  0.005032477\nPerR          0.011855322  0.376065098\nORY          -0.007451027 -0.002179961\n\nconfint(regression_model3)\n\n                  2.5 %       97.5 %\n(Intercept) -2.36784828 38.256485319\nPerR        -0.19716429  0.293906022\nORY         -0.01015637 -0.002916818\n\nnew_data3=df[1,c(8,9)]\nnew_data3[1,]=c(56 , 2100)\npredict(regression_model3,new_data3,interval = 'confidence')\n\n       fit      lwr      upr\n1 6.926243 5.828643 8.023842\n\npredict(regression_model2,new_data,interval = 'confidence')\n\n     fit      lwr      upr\n1 7.5709 6.814662 8.327138\n\n\n\nBe careful about extrapolating beyond the region containing the original observations. It is very possible that a model that fits well in the region of the original data will perform poorly outside that region. It is easy to inadvertently extrapolate, since the levels of the regressors jointly define a region containing the data which is impossible to visualize in its entirety beyond 2 dimensions. Ideally, we want to make inferences which lie inside the convex hull of the regressors.\nWe can use the diagonal of the hat matrix \\(H=X(X^\\top X)^{-1}X^\\top\\). In general, the point that has the largest value of \\(h_{ii}\\), say \\(h_{max}\\), will lie on the boundary of the convex hull in a region of the \\(x\\)-space where the density of the observations is relatively low. Points that lie in the set \\(\\{x^\\top (X^\\top X)^{-1} x\\leq h_{max}\\}\\) enclose the convex hull. Thus, for a value we are interested in predicting, say \\(y\\), we can check if we are extrapolating with \\(y^\\top (X^\\top X)^{-1} y\\leq h_{max}\\).\n\n\nA serious problem that may dramatically impact the usefulness of a regression model is multicollinearity, or near - linear dependence among the regression variables. Multicollinearity implies near - linear dependence among the regressors. The regressors are the columns of the \\(X\\) matrix, so clearly an exact linear dependence would result in a singular \\(X^\\top X\\). This will impact our ability to estimate \\(\\beta\\).\nWe can check for this dependence with the variance inflation factor (VIF). The variance inflation factor can be written as \\((1-R^2_j)^{-1}\\), where \\(R^2_j\\) is the coefficient of determination obtained from regressing \\(X_j\\) on the other regressor variables. If VIF is large, say \\(&gt;3\\), then you will likely need to make some changes to your regression model.\n\n\nSometimes, you may observe that regression coefficients have the a sign that is unexpected, or contradicts nature. This is likely due to one of the following:\n\nThe range of some of the regressors is too small – if the range of some of the regressors is too small, then the variance of \\(\\hat\\beta\\) is high.\nImportant regressors have not been included in the model.\nMulticollinearity is present.\nComputational errors have been made.\n\n\nWe close this Chapter with the following statement. Recall the modelling overview from Chapter 1:\n\nPosit the model: What is the linear regression model – what are all the assumptions of the linear regression model?\nEstimation: How can we estimate parameters of the linear regression model?\nInference: How can we compute confidence intervals and run hypothesis tests associated with the linear regression model?\nFit: Does our fitted line match up with the data? What about the normality assumption? Do the errors appear normal? Do the errors seem independent? Is the variance constant? How much variability is explained by our model?\nPrediction: How can we predict a new \\(Y\\)? What is the error of this prediction\n\nIf you have learned the concepts of this chapter, you should be able to complete all of these steps! In the following chapters, we will discuss different problems that can arise in regression modelling and how to remedy them.\n\n\n3.6.2 Homework questions\n\nExercise 3.23 Show \\({\\textrm{Var}}\\left[\\hat Y|X\\right]=\\sigma^2 H\\).\n\n\nExercise 3.24 Check for multicolinearity in our past examples.\n\n\nExercise 3.25 Complete the problem sets from Chapter’s 2, 3 and 4!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "Unit_3.html",
    "href": "Unit_3.html",
    "title": "4  Residual analysis",
    "section": "",
    "text": "4.1 Properties of residuals\nThe following are some properties of the residual vector. First, the sample mean of the residuals is zero: \\(\\sum_{i=1}^n\\hat\\epsilon_i/n=\\hat\\epsilon 1_n \\cdot 1/n=0\\). We also have that \\({\\textrm{E}}\\left[\\hat\\epsilon\\right]=0\\). Next, the sample variance of the residual vector is approximately the MSE: \\(\\frac{1}{n-1}\\sum_{i=1}^n\\hat\\epsilon_i^2=\\frac{n-p}{n-1}MSE\\). Lastly, unlike the random error \\(\\epsilon_i\\), the residuals are not independent. Sometimes we say that they are “approximately independent” if \\(p&lt;&lt;n\\), which we will touch on later.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Residual analysis</span>"
    ]
  },
  {
    "objectID": "Unit_3.html#types-of-residuals",
    "href": "Unit_3.html#types-of-residuals",
    "title": "4  Residual analysis",
    "section": "4.2 Types of residuals",
    "text": "4.2 Types of residuals\nWe will refer to \\(\\hat\\epsilon_i\\) as simply the residuals, or ordinary residuals when we need to be extra clear.\nThe standardized residual is given by \\[d_i=\\hat\\epsilon_i/\\sqrt{MSE}.\\] This is an approximate \\(Z\\)-score for the residuals, since the residuals have 0 mean, the \\(MSE\\) is approximately the variance of the random error and the residuals approximate the random error. We say that large \\(d_i\\) (\\(&gt;3\\)) indicates an outlier, though, we may want to use a more robust measure of the variance. We will generally prefer to use a different type of residual, which we now present.\nWe now introduce the hat matrix: \\(H=X(X^\\top X)^{-1}X^\\top\\). Note that \\(H\\) is symmetric and idempotent. The hat matrix appears often in regression analysis, and you should remember this quantity. It is called the hat matrix because \\(\\hat Y=H Y\\).\nNote that the eigenvalues of \\(H\\), and any idempotent matrix \\(A\\) are either \\(0\\) or 1: \\[ \\lambda x=A x=A^2 x=A\\lambda x=\\lambda^2 x,\\] which implies that \\(\\lambda\\in\\{0,1\\}\\).\n\nExercise 4.1 Verify that \\(H\\) is symmetric and idempotent and that \\(\\hat Y=H Y\\), where one recalls that a matrix \\(A\\) is idempotent if \\(AA=A\\).\n\nNow, note that: \\[\\hat\\epsilon=(I-H)Y=(I-H)\\epsilon.\\]\n\nExercise 4.2 Verify that \\(\\hat\\epsilon=(I-H)Y=(I-H)\\epsilon.\\).\n\nUsing this identity, we have that \\({\\textrm{Var}}\\left[\\hat\\epsilon\\right]=(I-H)\\epsilon(I-H)^\\top=(I-H)\\sigma^2.\\)\nThe fact that \\(H\\) is symmetric and idempotent implies that its diagonal elements are between 0 and 1. It follows that the elements on the diagonal of \\((I-H)\\) are also between 0 and 1. Therefore, the \\(MSE\\) overestimates the variance of the residuals: the variance of residual \\(i\\) is given by \\((1-h_{ii})\\sigma^2&lt;\\sigma^2\\approx MSE\\). Here, \\(h_{ii}\\) denotes the \\(i\\)th diagonal element of the matrix \\(H\\).\nFurthermore, \\(h_{ii}\\) is a measure of the location of the \\(i\\)th point in \\(x\\)-space, \\({\\textrm{Var}}\\left[\\hat\\epsilon_i\\right]\\) depends on where \\(X_i\\) lies. Points near the center of the \\(x\\)-space have larger variance than residuals at more remote locations. What do you think about this?\nNow, intuitively, violations of model assumptions would be more likely to occur at remote points. However, the variance of the ordinary residuals is lower at these points. Therefore, these violations may be hard to detect from inspection of the ordinary residuals because their residuals will often be smaller.\nTherefore, we will call points that are outlying in the \\(x\\)-space leverage points. We will refer to influence points as points that are not only remote in terms of the \\(x\\)-space, but also the observed response for that point is not consistent with the response that would be predicted for that point, using only the other data points.\nIn the example below, observe that the right-most point is both a leverage and influential point.\n\n########## \n# Simulate some data\nset.seed(330)\nx=c(rnorm(6),2.5)\ny=x*2+3\ny[7]=y[7]+7\n\n# Plot the data and fitted lines\nplot(x,y,pch=22,bg=1)\na=lm(y~x)\ncurve(a$coefficients[1]+x*a$coefficients[2],add=T,lwd=3)\ncurve(x*2+3,add=T,col=2,lwd=3)\n\n\n\n\n\n\n\na$coefficients\n\n(Intercept)           x \n   2.048937    3.979977 \n\n\nLet \\(\\hat Y^*_n\\) be the estimate of \\(Y_n\\) based on the other data and let \\(\\delta_n=Y_n-\\hat Y^*_n\\). Note that one can show that \\(\\hat Y_n=\\hat Y^*_n+h_{nn}\\delta_n\\). Next, we know that if \\(X_n\\) is outlying, i.e., \\(||X_n||\\) is large, then \\(h_{nn}\\approx 1\\). This implies that \\(\\hat Y_n\\approx Y_n\\), which means that the regression line is dragged to pass through \\((X_n,Y_n)\\).\nTo detect these types of outlying points, it makes sense to then define the studentized residuals: \\[r_i=\\frac{\\hat\\epsilon_i}{\\sqrt{MSE(1-h_{ii})}}.\\]\nThe studentized residuals in the simple linear regression model reduce to \\[\nr_i=\\frac{\\hat\\epsilon_i}{\\sqrt{MSE}\\left[1-\\left(\\frac{1}{n}+\\frac{\\left(X_i-\\bar{X}\\right)^2}{\\sum (X-\\bar X)^2}\\right)\\right]}.\n\\] Observe that as \\(X_i\\) grows large, we have that \\(\\frac{\\left(X_i-\\bar{X}\\right)^2}{\\sum (X-\\bar X)^2}\\to 1\\), which implies that \\(\\left[1-\\left(\\frac{1}{n}+\\frac{\\left(X_i-\\bar{X}\\right)^2}{\\sum (X-\\bar X)^2}\\right)\\right]\\to 0\\) and \\(r_i\\to \\infty\\). On the other hand, as \\(\\hat\\epsilon_i\\) grows large, we have that \\(r_i\\) grows large. Therefore, the studentized residual will be large for observations with large ordinary residuals, and for leverage observations.\nEarlier, we presented \\(\\delta_i\\), the difference between the response of the \\(i\\)th observation and the predicted response based on the observations with the \\(i\\)th points removed. These are known as the PRESS residuals. This seems hard computationally, but one can show that\n\\[\n\\delta_i=\\frac{\\hat\\epsilon_i}{1-h_{ii}} .\n\\] Note that when \\(h_{ii}\\) is large, this indicates a highly influential point. Observe that a large PRESS residual \\(\\delta_i\\), but small ordinary residual \\(\\hat\\epsilon_i\\), indicates that the model fit without \\((X_i,Y_i)\\) predicts \\(Y_i\\) poorly.\n\nExercise 4.3 Show that standardizing the PRESS residual, that is, dividing the PRESS residual by its standard deviation, results in \\(\\hat\\epsilon_i/\\sqrt{\\sigma^2(1-h_{ii})}\\). Compare this to the studentized residual.\n\nLastly, if we believe that \\((X_i,Y_i)\\) is outlying, then we can also leave \\((X_i,Y_i)\\) out in the MSE calculation. This results in the R-studentized residuals: \\[\n\\tilde r_i=\\frac{\\hat\\epsilon_i}{\\sqrt{\\widetilde{MSE}_i(1-h_{ii})}},\n\\] where \\(\\widetilde{MSE}_i\\) is the mean squared error computed from the regression model with \\((X_i,Y_i)\\) excluded: \\[\\widetilde{MSE}_{i}=\\frac{(n-p+1) MSE-\\hat \\epsilon_i^2 /\\left(1-h_{i i}\\right)}{n-p}.\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Residual analysis</span>"
    ]
  },
  {
    "objectID": "Unit_3.html#revisiting-checking-model-assumptions",
    "href": "Unit_3.html#revisiting-checking-model-assumptions",
    "title": "4  Residual analysis",
    "section": "4.3 Revisiting checking model assumptions",
    "text": "4.3 Revisiting checking model assumptions\nRecall from Checking model assumptions that we plot the residuals to check various assumptions. In this case, we can now use our upgraded residuals to make these plots. In general, any of the residuals that incorporate the values \\(h_{ii}\\) are acceptable. We will generally use the studentized residuals.\nRecall that we may want to plot:\n\nQQplot of the studentized residuals\nHistogram of the studentized residuals\nPlot of studentized residuals against the fitted Values\nStudentized residuals against the covariates\nStudentized residuals against covariates that are not currently in the model\nStudentized residuals against time in some contexts\n\n\nExample 4.1 Here, this data contains delivery times, the number of products in the delivery and the distance of the delivery. Perform a residual analysis on the model which regresses delivery times against the number of products in the delivery and the distance of the delivery. Compute all the different types of residuals.\n\n\n########## Delivery Time\n\n\n# Load and inspect the data \ndata(delivery, package=\"robustbase\")\ndf=delivery\nn=nrow(df)\nhead(df)\n\n  n.prod distance delTime\n1      7      560   16.68\n2      3      220   11.50\n3      3      340   12.03\n4      4       80   14.88\n5      6      150   13.75\n6      7      330   18.11\n\n# Fit the model \nmodel=lm(delTime~.,data=df)\ns=summary(model); s\n\n\nCall:\nlm(formula = delTime ~ ., data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7880 -0.6629  0.4364  1.1566  7.4197 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.341231   1.096730   2.135 0.044170 *  \nn.prod      1.615907   0.170735   9.464 3.25e-09 ***\ndistance    0.014385   0.003613   3.981 0.000631 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 22 degrees of freedom\nMultiple R-squared:  0.9596,    Adjusted R-squared:  0.9559 \nF-statistic: 261.2 on 2 and 22 DF,  p-value: 4.687e-16\n\n# X matrix\nX=model.matrix(model)\n\n# Hat matrix\nhat=X%*%solve(t(X)%*%X)%*%t(X)\n\n# Compute h_ii\nhii=diag(hat)\nhii\n\n         1          2          3          4          5          6          7 \n0.10180178 0.07070164 0.09873476 0.08537479 0.07501050 0.04286693 0.08179867 \n         8          9         10         11         12         13         14 \n0.06372559 0.49829216 0.19629595 0.08613260 0.11365570 0.06112463 0.07824332 \n        15         16         17         18         19         20         21 \n0.04111077 0.16594043 0.05943202 0.09626046 0.09644857 0.10168486 0.16527689 \n        22         23         24         25 \n0.39157522 0.04126005 0.12060826 0.06664345 \n\nmax(hii)\n\n[1] 0.4982922\n\n# Notice 9 is large\n\n###### ordinary residuals\nregular_residuals=model$residuals\n# or \n\n\n# standardized residuals\nstand_res=model$residuals/s$sigma\n\n# studentized residuals\nstudent_res=rstudent(model)\n\n\n#PRESS residuals\npress=model$residuals/(1-hii)\n\n# Get the MSE_is\nMSE_i=((n-2)*(s$sigma)^2-regular_residuals^2/(1-hii))/(n-3)\n\n#r studentized residuals\nRstudent_res=model$residuals/sqrt(MSE_i)\n\n# Plot them all and compare\npar(mfrow=c(2,3))\nplot(regular_residuals,main=\"ordinary\")\nplot(stand_res,main=\"Standardized\")\nplot(student_res,main=\"Studentized\")\nplot(press,main=\"PRESS\")\nplot(Rstudent_res,main=\"student\")\n\n# Notice 9 is much more outlying in the last 3 graphs. \n\n# Reset plotting\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\n# 9 is largest\nwhich.max(student_res)\n\n9 \n9 \n\n# Notice the standardized is half as large as the studentized. \nstudent_res[9]\n\n      9 \n4.31078 \n\nstand_res[9]\n\n       9 \n2.276351 \n\npar(mfrow=c(2,2))\n\n# Notice the difference !!!\nqqnorm(regular_residuals,pch=22,bg=1,main=\"Ordinary\")\n\nqqnorm(student_res,pch=22,bg=1,main=\"Studentized\")\nabline(0,1)\n\n# Compare all \n\npar(mfrow=c(2,2))\n\n\n\n\n\n\n\nqqnorm(student_res,pch=22,bg=1,ylim=c(-5,5),main=\"Studentized\")\nabline(0,1)\n# hist(student_res)\n\nqqnorm(Rstudent_res,pch=22,bg=1,ylim=c(-3,3),main=\"R Studentized\")\nqqline(Rstudent_res,pch=22,bg=1,ylim=c(-10,10))\n# abline(0,1)\n\nqqnorm(stand_res,pch=22,bg=1,ylim=c(-3,3),main=\"Standardized\")\nabline(0,1)\n\nqqnorm(press,pch=22,bg=1,ylim=c(-10,10),main=\"PRESS\")\nqqline(press,pch=22,bg=1,ylim=c(-10,10))\n#careful of the scale!\n\npar(mfrow=c(3,2))\nqqline(model$residuals,pch=22,bg=1,main=\"Ordinary\")\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nqqnorm(student_res,pch=22,bg=1,main=\"Studentized\")\nabline(0,1)\nqqnorm(Rstudent_res,pch=22,bg=1,main=\"R Studentized\")\nabline(0,1)\nqqnorm(stand_res,pch=22,bg=1,main=\"Standardized\")\nabline(0,1)\nqqnorm(press,pch=22,bg=1,main=\"PRESS\")\nabline(0,1)\n\n\n\n\n\n\n\n# Now we plot the fitted values against the R studentized residuals\npar(mfrow=c(1,1),pch=22)\nplot(model$fitted.values,Rstudent_res,bg=1)\nabline(h=0)\n\n\n\n\n\n\n\n# Now we plot the number of products against the R studentized residuals\n# There is one moderately large delivery!\nplot(df$n.prod,Rstudent_res,bg=1)\nabline(h=0)\n\n\n\n\n\n\n\n# Care for the scale\nplot(df$n.prod,Rstudent_res,bg=1,xlim=c(0,12))\nabline(h=0)\n\n\n\n\n\n\n\n# There is one very far delivery!\nplot(df$distance,Rstudent_res,bg=1)\nabline(h=0)\n\n\n\n\n\n\n\n# Care for the scale\nplot(df$distance,Rstudent_res,bg=1,xlim=c(0,850))\nabline(h=0)\n\n\n\n\n\n\n\n# What happens to the model when we remove this outlying observation (the far distance delivery)\ndf2=df[-which.max(df$distance),]\n\n\n# refit the model\nmodel=lm(delTime~.,data=df2)\ns=summary(model); s\n\n\nCall:\nlm(formula = delTime ~ ., data = df2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0325 -1.2331  0.0199  1.4730  4.8167 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.447238   0.952469   4.669 0.000131 ***\nn.prod      1.497691   0.130207  11.502 1.58e-10 ***\ndistance    0.010324   0.002854   3.618 0.001614 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.43 on 21 degrees of freedom\nMultiple R-squared:  0.9487,    Adjusted R-squared:  0.9438 \nF-statistic: 194.2 on 2 and 21 DF,  p-value: 2.859e-14\n\n# X matrix\nX=model.matrix(model)\n\n# Hat matrix\nhat=X%*%solve(t(X)%*%X)%*%t(X)\n\n# Compute h_ii\nhii=diag(hat)\nhii\n\n         1          2          3          4          5          6          7 \n0.11083391 0.07741039 0.09998709 0.10097319 0.08066357 0.04290146 0.10024969 \n         8         10         11         12         13         14         15 \n0.06537738 0.20438000 0.14675966 0.11367920 0.06437975 0.08033747 0.04661503 \n        16         17         18         19         20         21         22 \n0.21115081 0.06254612 0.10128434 0.11992977 0.18537865 0.16642759 0.55671434 \n        23         24         25 \n0.04687996 0.13894064 0.07620000 \n\nmax(hii)\n\n[1] 0.5567143\n\n###### ordinary residuals\nregular_residuals=model$residuals\n# or \n\n\n# standardized residuals\nstand_res=model$residuals/s$sigma\n\n# studentized residuals\nstudent_res=rstudent(model)\n\n\n#PRESS residuals\npress=model$residuals/(1-hii)\n\n# Get the MSE_is\nMSE_i=((n-2)*(s$sigma)^2-regular_residuals^2/(1-hii))/(n-3)\n\n#r studentized residuals\nRstudent_res=model$residuals/sqrt(MSE_i)\n\n# Plot them all and compare - much better\npar(mfrow=c(2,3))\nplot(regular_residuals,main=\"ordinary\")\nplot(stand_res,main=\"Standardized\")\nplot(student_res,main=\"Studentized\")\nplot(press,main=\"PRESS\")\nplot(Rstudent_res,main=\"student\")\n\n\n# Notice that these graphs are fine now... \npar(mfrow=c(1,1))\n\n\n\n\n\n\n\nqqnorm(student_res,pch=22,bg=1,main=\"Studentized\")\nabline(0,1)\n\n\n\n\n\n\n\n# Now we plot the fitted values against the R studentized residuals\npar(mfrow=c(1,1),pch=22)\nplot(model$fitted.values,Rstudent_res,bg=1)\nabline(h=0)\n\n\n\n\n\n\n\n# Now we plot the number of products against the R studentized residuals\nplot(df2$n.prod,Rstudent_res,bg=1)\nabline(h=0)\n\n\n\n\n\n\n\n# Care for the scale\nplot(df2$n.prod,Rstudent_res,bg=1,xlim=c(0,12))\nabline(h=0)\n\n\n\n\n\n\n\nplot(df2$distance,Rstudent_res,bg=1)\nabline(h=0)\n\n\n\n\n\n\n\n# Care for the scale\nplot(df2$distance,Rstudent_res,bg=1,xlim=c(0,850))\nabline(h=0)\n\n\n\n\n\n\n\n\nNow, we have introduced different types of residuals and the appropriate graphs to examine when checking for violations of the assumptions. When we observe violations of the assumptions - what do we do? That will be the topic of the next section.\nSome of these remedies include: - Transformations of the response - Transformations of certain regressors - Robust methods/outlier removal - Inclusion of new regressors",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Residual analysis</span>"
    ]
  },
  {
    "objectID": "Unit_3.html#homework-stop",
    "href": "Unit_3.html#homework-stop",
    "title": "4  Residual analysis",
    "section": "4.4 Homework stop",
    "text": "4.4 Homework stop\nDo the Chapter 4 questions from the textbook.\n\nExercise 4.4 In the context of a regression model, do you think a point outlying in the \\(x\\)-space is more problematic than a point outlying in the \\(y\\)-space?\n\n\nExercise 4.5 Make a table describing the differences between each type of residual.\n\n\nExercise 4.6 Perform a residual analysis on the marketing data from Example Example 3.6.\n\n\nExercise 4.7 Perform a residual analysis on the data from Example Example 3.7.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Residual analysis</span>"
    ]
  },
  {
    "objectID": "Unit_4.html",
    "href": "Unit_4.html",
    "title": "5  Transformations",
    "section": "",
    "text": "5.1 Variance-stabilizing transformations\nRecall that we assume that \\(\\forall i\\in[n]\\), \\(\\epsilon_i\\sim \\mathcal{N}(0,\\sigma^2)\\). A common reason for a violation of this assumption is for \\(Y\\) to have a distribution in which the variance is related to its mean. For example, if the response \\(Y\\) is a Poisson random variable, i.e., \\[Y|X\\sim Pois(X\\beta),\\] then we have that \\({\\textrm{E}}\\left[Y\\right]=X\\beta\\), then \\({\\textrm{E}}\\left[Y\\right]={\\textrm{Var}}\\left[Y\\right]=X\\beta\\). In this case, the simple linear regression assumptions are violated. In particular, the variance is not the same for each observation. Here, it happens that taking the response to be roughly \\(\\sqrt{Y}\\) fixes the problem. That is, performing the regression analysis with \\(\\sqrt{Y}\\) as the response variable instead of \\(Y\\), ensures that the regression assumptions are (approximately) satisfied. This example gives rise to the idea of transformations. If our data do not satisfy the assumptions for the MLR or the normal MLR, we might ask if there is some transformation of either the response, some of the covariates, or both that make the data suitable for a MLR analysis. Note that the assumptions are important. For instance, if the variance is not homogeneous, the OLS estimator will still be unbiased, but they will no longer have BLUE property. That means that some other estimator will work better for such data!\nWhich transformation should we choose? Sometimes, we can use prior experience or theoretical considerations to guide us in selecting an appropriate transformation. Other times, we must choose it empirically, i.e., based on the data. Often, the square root and the logarithm are popular choices. If you response is between 0 and 1, and the data appear to be “football shaped”, then you may like to take the \\(\\arcsin(\\sqrt{Y})\\).\nWe now demonstrate what one of these relationships looks like in simple linear regression. We now simulate a dataset where \\(\\sigma^2\\propto {\\textrm{E}}\\left[Y|X\\right]\\), and plot \\(X\\) against \\(Y\\). We use the Poisson example discussed previously.\nset.seed(2352)\nn=1000\nX=runif(n,5,10)*2\nY=20*rpois(n,X)+2\nplot(X,Y)\n# Performing a regression analysis yields: \nmodel=lm(Y~X)\n# Notice the intercept is poorly estimated! \nsummary(model)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-250.289  -53.392   -2.127   48.795  270.716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -9.5199    13.0735  -0.728    0.467    \nX            20.7241     0.8599  24.101   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 77.61 on 998 degrees of freedom\nMultiple R-squared:  0.3679,    Adjusted R-squared:  0.3673 \nF-statistic: 580.9 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n# Notice the fan shape in the residuals against the fitted values?\nplot(model$fitted.values,model$residuals)\n\n\n\n\n\n\n\n# Let's perform the transformations\n\nmodel2=lm(sqrt(Y)~X)\n\nplot(model2$fitted.values,model2$residuals)\n\n\n\n\n\n\n\nmodel2=lm(log(Y)~X)\n\nplot(model2$fitted.values,model2$residuals)\nHowever, these transformations do not always work. Suppose we have that \\(Y\\sim\\mathcal{N}(X,4*X^2).\\) We then have that \\(\\sigma=2*X=2*{\\textrm{E}}\\left[Y|X\\right]\\). Notice how the spread of the points is increasing with \\(X\\)? This is a symptom of non-homogeneous variance. However, the proposed transformations do not work.\nset.seed(2352)\n# \\sigma^2\\propto \\E{Y}\nY=20*rnorm(n,X,X*2)+2\n\nplot(X,Y)\n\n\n\n\n\n\n\n# Performing a regression analysis yields: \nmodel=lm(Y~X)\n\n# notice the intercept is poorly estimated. \nsummary(model)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2610.8  -375.3    -3.5   384.8  3460.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   76.844    103.345   0.744   0.4573  \nX             13.367      6.797   1.966   0.0495 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 613.5 on 998 degrees of freedom\nMultiple R-squared:  0.00386,   Adjusted R-squared:  0.002861 \nF-statistic: 3.867 on 1 and 998 DF,  p-value: 0.04953\n\n# Notice the fan shape in the residuals against the fitted values?\nplot(model$fitted.values,model$residuals)\n\n\n\n\n\n\n\n# Let's perform the transformation\n\n# Performing a regression analysis yields: \nmodel2=lm(sqrt(Y)~X)\n\nWarning in sqrt(Y): NaNs produced\n\n# Notice the fan shape in the residuals against the fitted values?\nplot(model2$fitted.values,model2$residuals)\n\n\n\n\n\n\n\n# Performing a regression analysis yields: \nmodel2=lm(log(Y+3000)~X)\n\n# Notice the fan shape in the residuals against the fitted values?\nplot(model2$fitted.values,model2$residuals,ylim=c(-3,3))\nIn general, a good transformation to correct violated assumptions can improve estimates and test accuracy.\nLet’s expand on this. It is a good time to recall that in general, for a real function \\(f\\), we have that \\({\\textrm{E}}\\left[f(X)\\right]\\neq f({\\textrm{E}}\\left[X\\right])\\). For instance, for many random variables \\(Z\\), we would have that \\({\\textrm{E}}\\left[Z^2\\right]\\neq {\\textrm{E}}\\left[Z\\right]^2\\), \\({\\textrm{E}}\\left[\\log Z\\right]\\neq \\log {\\textrm{E}}\\left[Z\\right]\\) etc. .\nIn a transformed regression model, we fit the following model: \\[f(Y)=X\\beta+\\epsilon.\\] If we are interested in predicting the value of \\(Y\\) given \\(z\\), then it seems natural to take the predictions for \\(f(Y)\\) given \\(z\\), which are given by \\(\\beta^\\top z\\) and apply the inverse transformation \\(f^{-1}\\). For instance, to predict \\(Y|Z=z\\), we may compute: \\(f^{-1}(\\beta^\\top z)\\). It turns out, this prediction is biased, and we should use a different method instead.\nTo see why it’s biased, observe that the predictions from the model \\(f(Y)=X\\beta+\\epsilon\\) for a new set of covariates \\(z\\) are given by \\(\\hat f(Y)=\\hat\\beta^\\top z\\approx{\\textrm{E}}\\left[f(Y)|Z=z\\right]\\). Now, we have that \\[f^{-1}(\\hat\\beta^\\top z)\\approx f^{-1}({\\textrm{E}}\\left[f(Y)|Z=z\\right])\\neq {\\textrm{E}}\\left[f^{-1}(f(Y))|Z=z\\right]= {\\textrm{E}}\\left[\\hat f(Y)|Z=z\\right].\\]\nThe solution to this problem is to adjust for the bias. For the log transform, we can multiply the resulting inverse transformed predictions by \\(\\exp(\\hat\\sigma^2/2)\\). For the square root transformation, we add \\(\\hat\\sigma^2\\) to the resulting inverse transformed predictions. See (Miller 1984) for more information.\nOne can also use confidence and prediction intervals to predict the value of \\(Y\\) given \\(z\\). Confidence or prediction intervals may be directly converted from one metric to another – such interval estimates are percentiles of a distribution which are unaffected by the transformation. They can be converted back to the original units via the inverse transformation and the interpretation will remain the same. Optimal intervals are intervals with the shortest average interval length for a given confidence level, under a given set of assumptions. However, it may be that the resulting intervals may not be “optimal”. One way to get a prediction in the original units, is to apply the inverse transformation to the prediction interval computed from the transformed model and take the midpoint of that interval. This does not always work well - and should be checked against the original data.\nset.seed(2352)\n# Simulate the stock prices\nn=100\nday=seq(1:n)\nlog_return=rnorm(n,0.000001+0.000005*day,0.01)\n# log_return=rnorm(n,0.000001+0.000005*day,0.0005)\nstock_prices=c(50,50*exp(cumsum(log_return)))\n# exp(log_return)[1:10]\n\nplot(c(0,day), stock_prices,type='l')\ndf=data.frame(cbind(\"day\"=c(0,day),\"sp\"=stock_prices))\n\n\n# Now, suppose this is our starting dataset\nhead(df)\n\n  day       sp\n1   0 50.00000\n2   1 49.93624\n3   2 49.97369\n4   3 49.29229\n5   4 48.90210\n6   5 49.66743\n\n# Notice that the pattern is not great... but we can regress on the transformed response\nplot(df)\n\n\n\n\n\n\n\n# Fitting the model \n\n# Compute the log returns\ndf$lr=NA\ndf$lr[2:(n+1)]=log(df$sp[-1]/df$sp[-(n+1)])\n\n# Sanity Check\n# log_return[1:5]\n# df$lr[2:6]\n\nmodel=lm(lr~day,df)\nsummary(model)\n\n\nCall:\nlm(formula = lr ~ day, data = df)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0249054 -0.0064851  0.0000518  0.0075839  0.0208044 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  1.824e-03  1.926e-03   0.947    0.346\nday         -7.771e-06  3.312e-05  -0.235    0.815\n\nResidual standard error: 0.00956 on 98 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.0005615, Adjusted R-squared:  -0.009637 \nF-statistic: 0.05506 on 1 and 98 DF,  p-value: 0.815\n\nplot(c(0,day), stock_prices,type='l',lwd=2)\nlines(50*exp(cumsum(fitted.values(model))),col='red',lty=2,lwd=2)\n\n# Intervals for the mean at each time point \nintervals=predict(model,interval ='prediction')[,2:3]\n\nWarning in predict.lm(model, interval = \"prediction\"): predictions on current data refer to _future_ responses\n\nmidpoint=ret=rep(0,n)\nfor(i in 1:n){\n  if(i==1){\n    midpoint[i]=50*exp(intervals[i,1])/2+50*exp(intervals[i,2])/2\n  }\n  else\n    midpoint[i]=midpoint[i-1]*(exp(intervals[i,1])+exp(intervals[i,2]))/2\n}\nlines(midpoint,col=\"green\",lty=3,lwd=2)\n\nlegend(\"topleft\",legend=c(\"obs. rolling avg\",\"inverse predictions\",\"Pred int 95%\",\"Bias Correct\"), lty=c(1,2,3),col=c('black','red','green'),lwd=2)\nA second example…\nset.seed(2352)\n# Simulate data\nn=100\nX=runif(n,5,10)\nlogs=rnorm(n,1+0.2*X,0.5)\nY=exp(logs)\nplot(X,Y)\n\n\ndf=data.frame(cbind(\"X\"=X,\"Y\"=Y))\ndf=df[order(X),]\n\n# Fitting the model\nmodel=lm(log(Y)~X,data=df)\nsummary(model)\n\n\nCall:\nlm(formula = log(Y) ~ X, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.13484 -0.31721 -0.02877  0.29765  0.85929 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.00060    0.21048   4.754 6.85e-06 ***\nX            0.19333    0.02721   7.106 1.94e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4113 on 98 degrees of freedom\nMultiple R-squared:  0.3401,    Adjusted R-squared:  0.3333 \nF-statistic:  50.5 on 1 and 98 DF,  p-value: 1.937e-10\n\ns=summary(model)$sigma\n# Rolling average\n\n\n\nzb=zoo::zoo(x=df$Y,df$X)\n\n\n\n\n\n\n\nrm=zoo::rollmean(zb,25)\n\nplot(attributes(rm)$index,rm,lty=1,lwd=3,type='l')\n# plot(X,Y)\n\nzb=zoo::zoo(x=exp(fitted.values(model)),df$X)\nrm=zoo::rollmean(zb,25)\nlines(attributes(rm)$index,rm,col=2,lty=2,lwd=3)\n\nlines(attributes(rm)$index,rm*exp(s^2/2),col=6,lty=2,lwd=3)\n\n# Intervals for the mean at each time point \nnd=data.frame(\"X\"=df$X)\nivs=predict(model, newdata = nd,interval = 'prediction')[,2:3]\nintervals=rowMeans(exp(ivs))\nzb=zoo::zoo(x=intervals,df$X)\nrm=zoo::rollmean(zb,25)\nlines(attributes(rm)$index,rm,col=3,lty=3,lwd=4)\n\n\n# Intervals for the mean at each time point - notice when we lower the level the performance increases... \nnd=data.frame(\"X\"=df$X)\nivs=predict(model, newdata = nd,interval = 'prediction', level = 0.8)[,2:3]\nintervals=rowMeans(exp(ivs))\nzb=zoo::zoo(x=intervals,df$X)\nrm=zoo::rollmean(zb,25)\nlines(attributes(rm)$index,rm,col=7,lty=3,lwd=4)\n\n\n\n\n# Intervals for the mean at each time point using confidence intervals\n# ivs=predict(model, newdata = nd,interval = 'confidence', level = 0.8)[,2:3]\n# intervals=rowMeans(exp(ivs))\n# zb=zoo::zoo(x=intervals,df$X)\n# rm=zoo::rollmean(zb,25)\n# lines(attributes(rm)$index,rm,col=6,lty=3,lwd=3)\n\nlegend(\"topleft\",legend=c(\"obs. rolling avg\",\"inverse predictions\",\"Pred int 95%\",\"Pred int 80%\",\"Bias cor.\"), lty=c(1,2,3,3,2),col=c(1,2,3,7,6),lwd=2)\nset.seed(2352)\n\n# Simulate data\n\nn=1000\nX=runif(n,2,15)\nsq=rnorm(n,10+2*X,7)\nY=sq^2\nplot(X,Y)\n\n\n\n\n\n\n\ndf=data.frame(cbind(\"X\"=X,\"Y\"=Y))\ndf=df[order(X),]\n\n# Fitting the model\nmodel=lm(sqrt(Y)~X,data=df)\nsummary(model)\n\n\nCall:\nlm(formula = sqrt(Y) ~ X, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-24.2975  -4.5612  -0.0315   4.7083  20.2670 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  9.94439    0.54223   18.34   &lt;2e-16 ***\nX            1.99810    0.05897   33.88   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.919 on 998 degrees of freedom\nMultiple R-squared:  0.535, Adjusted R-squared:  0.5345 \nF-statistic:  1148 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\ns=summary(model)$sigma\n# plot(X,Y)\n# Rolling average\n\n\n\nzb=zoo::zoo(x=df$Y,df$X)\nrm=zoo::rollmean(zb,50)\n\nplot(attributes(rm)$index,rm,col=1,lty=3,lwd=3,type='l')\nzb=zoo::zoo(fitted.values(model)^2,df$X)\nrm=zoo::rollmean(zb,25)\nlines(attributes(rm)$index,rm,col=2,lty=2,lwd=3)\nlines(attributes(rm)$index,rm+s^2,col=6,lty=2,lwd=3)\n\n\n# Intervals for the mean at each time point \nintervals=rowMeans(predict.lm(model,interval = 'prediction')[,2:3]^2)\n\nWarning in predict.lm(model, interval = \"prediction\"): predictions on current data refer to _future_ responses\n\nzb=zoo::zoo(intervals,df$X)\nrm=zoo::rollmean(zb,25)\nlines(attributes(rm)$index,rm,col=3,lty=3,lwd=3)\nlegend(\"topleft\",legend=c(\"obs. rolling avg\",\"inverse predictions\",\"Pred int 95%\",\"Bias Core.\"), lty=c(1,2,3,2),col=c(1,2,3,6),lwd=rep(3,4))\nWe see that the bias correction is the best performing method. However, this involves working out the bias for each transformation. For a complicated transformation, this may be quite difficult. For common transformations, this has already been completed for us.\nLet’s do an example with some real data. The following example is taken from the textbook:\n# Electric Utility Data\n\ndf&lt;- data.frame(\n    Customer = c(1:53),\n    x_kWh = c(679, 292, 1012, 493, 582, 1156, 997, 2189, 1097, 2078, 1818, 1700, 747, 2030, 1643, 414, 354, 1276, 745, 435, 540, 874, 1543, 1029, 710, 1434, 837, 1748, 1381, 1428, 1255, 1777, 370, 2316, 1130, 463, 770, 724, 808, 790, 783, 406, 1242, 658, 1746, 468, 1114, 413, 1787, 3560, 1495, 2221, 1526),\n    y_kW = c(0.79, 0.44, 0.56, 0.79, 2.70, 3.64, 4.73, 9.50, 5.34, 6.85, 5.84, 5.21, 3.25, 4.43, 3.16, 0.50, 0.17, 1.88, 0.77, 1.39, 0.56, 1.56, 5.28, 0.64, 4.00, 0.31, 4.20, 4.88, 3.48, 7.58, 2.63, 4.99, 0.59, 8.19, 4.79, 0.51, 1.74, 4.10, 3.94, 0.96, 3.29, 0.44, 3.24, 2.14, 5.71, 0.64, 1.90, 0.51, 8.33, 14.94, 5.11, 3.85, 3.93)\n  )\n\n\ndf\n\n   Customer x_kWh  y_kW\n1         1   679  0.79\n2         2   292  0.44\n3         3  1012  0.56\n4         4   493  0.79\n5         5   582  2.70\n6         6  1156  3.64\n7         7   997  4.73\n8         8  2189  9.50\n9         9  1097  5.34\n10       10  2078  6.85\n11       11  1818  5.84\n12       12  1700  5.21\n13       13   747  3.25\n14       14  2030  4.43\n15       15  1643  3.16\n16       16   414  0.50\n17       17   354  0.17\n18       18  1276  1.88\n19       19   745  0.77\n20       20   435  1.39\n21       21   540  0.56\n22       22   874  1.56\n23       23  1543  5.28\n24       24  1029  0.64\n25       25   710  4.00\n26       26  1434  0.31\n27       27   837  4.20\n28       28  1748  4.88\n29       29  1381  3.48\n30       30  1428  7.58\n31       31  1255  2.63\n32       32  1777  4.99\n33       33   370  0.59\n34       34  2316  8.19\n35       35  1130  4.79\n36       36   463  0.51\n37       37   770  1.74\n38       38   724  4.10\n39       39   808  3.94\n40       40   790  0.96\n41       41   783  3.29\n42       42   406  0.44\n43       43  1242  3.24\n44       44   658  2.14\n45       45  1746  5.71\n46       46   468  0.64\n47       47  1114  1.90\n48       48   413  0.51\n49       49  1787  8.33\n50       50  3560 14.94\n51       51  1495  5.11\n52       52  2221  3.85\n53       53  1526  3.93\n\n#  changing the plot aesthetics\npar(pch=22,lwd=2)\n\n\n# Explore\n\nplot(df[,2:3])\n\n\n\n\n\n\n\nsummary(df)\n\n    Customer      x_kWh           y_kW       \n Min.   : 1   Min.   : 292   Min.   : 0.170  \n 1st Qu.:14   1st Qu.: 679   1st Qu.: 0.790  \n Median :27   Median :1029   Median : 3.250  \n Mean   :27   Mean   :1153   Mean   : 3.413  \n 3rd Qu.:40   3rd Qu.:1543   3rd Qu.: 4.880  \n Max.   :53   Max.   :3560   Max.   :14.940  \n\nplot(df)\n\n\n\n\n\n\n\n# Model \n\nmodel=lm(y_kW~x_kWh, df); model\n\n\nCall:\nlm(formula = y_kW ~ x_kWh, data = df)\n\nCoefficients:\n(Intercept)        x_kWh  \n  -0.831304     0.003683  \n\nsumm=summary(model); summ\n\n\nCall:\nlm(formula = y_kW ~ x_kWh, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1399 -0.8275 -0.1934  1.2376  3.1522 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.8313037  0.4416121  -1.882   0.0655 .  \nx_kWh        0.0036828  0.0003339  11.030 4.11e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.577 on 51 degrees of freedom\nMultiple R-squared:  0.7046,    Adjusted R-squared:  0.6988 \nF-statistic: 121.7 on 1 and 51 DF,  p-value: 4.106e-15\n\n# Now do the residual analysis\n# Studentized residuals\nstudent_res=rstudent(model)\nMSE=summ$sigma^2\n\nqqnorm(student_res,pch=22,bg=1)\nabline(0,1)\n\n\n\n\n\n\n\nhist(student_res,breaks=6)\n\n\n\n\n\n\n\nplot(model$fitted.values,student_res,pch=22,bg=1)\nabline(h=0)\nWe see that the residual variance increases with the mean of \\(Y\\). This is easily seen by the fan shape of the residuals in the plot of the residuals against the fitted values.\n######## Let's try the sqrt transformation\n\nmodel2=lm(sqrt(y_kW)~x_kWh, df)\nmodel2\n\n\nCall:\nlm(formula = sqrt(y_kW) ~ x_kWh, data = df)\n\nCoefficients:\n(Intercept)        x_kWh  \n  0.5822259    0.0009529  \n\nsumm2=summary(model2); summ2\n\n\nCall:\nlm(formula = sqrt(y_kW) ~ x_kWh, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.39185 -0.30576 -0.03875  0.25378  0.81027 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.822e-01  1.299e-01   4.481 4.22e-05 ***\nx_kWh       9.529e-04  9.824e-05   9.699 3.61e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.464 on 51 degrees of freedom\nMultiple R-squared:  0.6485,    Adjusted R-squared:  0.6416 \nF-statistic: 94.08 on 1 and 51 DF,  p-value: 3.614e-13\n\nstudent_res2=rstudent(model2)\nMSE2=summ2$sigma^2\nqqnorm(student_res2,pch=22,bg=1)\nabline(0,1)\n\n\n\n\n\n\n\nhist(student_res2,breaks=10,xlim=c(-4,4))\n\n\n\n\n\n\n\nhist(student_res2[-which.max(abs(student_res2))],breaks=10,xlim=c(-2,2))\n\n\n\n\n\n\n\nplot(model2$fitted.values,student_res2,pch=22,bg=1)\nabline(h=0)\n\n\n\n\n\n\n\n# There is one large outlier skewing the previous plot. Let's rescale and remove. \nplot(model2$fitted.values,student_res2,pch=22,bg=1,ylim=c(-3,3))\nabline(h=0)\n\n\n\n\n\n\n\n# Compare!\npar(mfrow=c(1,2))\nplot(model$fitted.values,student_res,pch=22,bg=1,ylim=c(-3,3))\nabline(h=0)\nplot(model2$fitted.values,student_res2,pch=22,bg=1,ylim=c(-3,3))\nabline(h=0)\nWe see that the transformation has solved the problem. Note that sometimes, even though the square-root transformation may be more suitable, the analyst may opt for the logarithm transform. This is because the log transformation gives a nicer interpretation to the coefficients. In this case, that is not working well, see below:\n######## Let's try the log transformation\npar(mfrow=c(1,1))\n\n\nmodel3=lm(log(y_kW)~x_kWh, df)\nmodel3\n\n\nCall:\nlm(formula = log(y_kW) ~ x_kWh, data = df)\n\nCoefficients:\n(Intercept)        x_kWh  \n  -0.558713     0.001172  \n\nsumm3=summary(model3); summ3\n\n\nCall:\nlm(formula = log(y_kW) ~ x_kWh, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.29261 -0.47256  0.08414  0.49628  1.12143 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.5587131  0.2057201  -2.716    0.009 ** \nx_kWh        0.0011716  0.0001555   7.533 7.86e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7347 on 51 degrees of freedom\nMultiple R-squared:  0.5266,    Adjusted R-squared:  0.5174 \nF-statistic: 56.74 on 1 and 51 DF,  p-value: 7.862e-10\n\nstudent_res3=rstudent(model3)\nMSE3=summ3$sigma^2\nqqnorm(student_res3,pch=22,bg=1)\nabline(0,1)\n\n\n\n\n\n\n\nhist(student_res3,breaks=10,xlim=c(-4,4))\n\n\n\n\n\n\n\nhist(student_res3[-which.max(abs(student_res3))],breaks=10,xlim=c(-2,2))\n\n\n\n\n\n\n\nplot(model3$fitted.values,student_res3,pch=22,bg=1,ylim=c(-3,3))\nabline(h=0)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "Unit_4.html#variance-stabilizing-transformations",
    "href": "Unit_4.html#variance-stabilizing-transformations",
    "title": "5  Transformations",
    "section": "",
    "text": "Caution\n\n\n\nIt is often necessary to convert any predicted values back to the original units. Applying the inverse transformation to predicted values gives an estimate of the median of the distribution of the (untransformed) response – instead of the mean. This implies that predictions are generally biased. Prediction and confidence intervals do not suffer this illness. They can be converted back to the original units via the inverse transformation and the interpretation will remain the same.\n\n\n\n\n\n\n\n\nExample 5.1 Let’s simulate what happens when, given the day \\(t\\in [100]\\), we try to estimate the mean stock price \\(P_t\\) for some stock (maybe ?Gamestop?) in a model which regresses the logged rate of return against the day. Note that the logged returns at time \\(t\\) are given by: \\(L=\\log\\left(\\frac{P_t}{P_{t-1}}\\right)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 5.2 An electric utility is interested in developing a model relating peak - hour demand \\(Y\\) to total energy usage during the month \\(X\\). This is an important planning problem because while most customers pay directly for energy usage (in kilowatt - hours), the generation system must be large enough to meet the maximum demand imposed. Data for 53 residential customers for the month of August is given below.\n\n\n\n\n\n\n\n5.1.1 Linearizing the model\nMoving on, we may suspect that the relationship between the regressors and the response is nonlinear, either through empirical evidence or theoretical justification. In some cases a nonlinear function can be linearized by using a suitable transformation. Such nonlinear models are called intrinsically linear. For example, consider the model \\(Y=\\beta_0e^{\\beta_1 X}\\epsilon\\). Taking the log of both sides yields: \\[\\log(Y)=\\log(\\beta_0)+\\beta_1 X+\\log(\\epsilon).\\] Reparameterizing with \\(Z=\\log(Y)\\), \\(\\alpha_0=\\log(\\beta_0)\\) and \\(\\eta=\\log(\\epsilon)\\), we have that\n\\[Z=\\alpha_0+\\beta_1 X+\\eta.\\] If we are willing to assume that \\(\\eta\\) are symmetric about 0 with a constant variance, then we can run a linear regression with the model given above. To get estimates for the original units \\(Y\\), we can transform back as previously discussed. A model is linearizable if there exists some reparameterization which places the model in the form of the MLR.\n\nExercise 5.1 Show the following models are linearizable - that is, find the linear reparameterization of the following models: 1. \\(Y=\\beta_0 X^\\beta_1\\) 2. \\(Y=\\beta_0 X^{\\beta_1X}\\) 3. \\(Y=\\beta_0+ \\log X\\) 4. \\(Y=X/(\\beta_0 X-\\beta_1)\\)\n\n\nExample 5.3 A research engineer is investigating the use of a windmill to generate electricity. He has collected data on the DC output from his windmill and the corresponding wind velocity. See below. Find a well-fitting regression model for this data.\n\n\n########### Windmill data\n\n\n\n\n# Create the data frame\ndf_wind &lt;- data.frame(\n  WindVelocity_mph = c(5.00, 6.00, 3.40, 2.70, 10.00, 9.70, 9.55, 3.05, 8.15, 6.20, \n                       2.90, 6.35, 4.60, 5.80, 7.40, 3.60, 7.85, 8.80, 7.00, 5.45, \n                       9.10, 10.20, 4.10, 3.95, 2.45),\n  DCOutput = c(1.582, 1.822, 1.057, 0.500, 2.236, 2.386, 2.294, 0.558, 2.166, 1.866,\n               0.653, 1.930, 1.562, 1.737, 2.088, 1.137, 2.179, 2.112, 1.800, 1.501,\n               2.303, 2.310, 1.194, 1.144, 0.123)\n)\n\n###############\n\n\npar(mfrow=c(1,1))\nplot(df_wind)\n\n\n\n\n\n\n\nsummary(df_wind)\n\n WindVelocity_mph    DCOutput    \n Min.   : 2.450   Min.   :0.123  \n 1st Qu.: 3.950   1st Qu.:1.144  \n Median : 6.000   Median :1.800  \n Mean   : 6.132   Mean   :1.610  \n 3rd Qu.: 8.150   3rd Qu.:2.166  \n Max.   :10.200   Max.   :2.386  \n\nplot(df_wind,pch=22,bg=1)\n\n\n\n\n\n\n\nmodel=lm(DCOutput~WindVelocity_mph, df_wind)\nmodel\n\n\nCall:\nlm(formula = DCOutput ~ WindVelocity_mph, data = df_wind)\n\nCoefficients:\n     (Intercept)  WindVelocity_mph  \n          0.1309            0.2411  \n\nsumm=summary(model); summ\n\n\nCall:\nlm(formula = DCOutput ~ WindVelocity_mph, data = df_wind)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.59869 -0.14099  0.06059  0.17262  0.32184 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       0.13088    0.12599   1.039     0.31    \nWindVelocity_mph  0.24115    0.01905  12.659 7.55e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2361 on 23 degrees of freedom\nMultiple R-squared:  0.8745,    Adjusted R-squared:  0.869 \nF-statistic: 160.3 on 1 and 23 DF,  p-value: 7.546e-12\n\nstudent_res=rstudent(model)\n\nMSE=summ$sigma^2\nqqnorm(student_res,pch=22,bg=1)\nabline(0,1)\n\n\n\n\n\n\n\nhist(student_res,breaks=10,xlim=c(-4,4))\n\n\n\n\n\n\n\nplot(model$fitted.values,student_res,pch=22,bg=1)\nabline(h=0)\n\n\n\n\n\n\n\n\nThe fit is not good. Looking at the scatterplot, we might initially consider using a quadratic model to account for the pictured curvature. However, the scatterplot suggests that as wind speed increases, DC output approaches an upper limit of approximately 2.5. This is also consistent with the theory of windmill operation. Since the quadratic model will eventually bend downward as wind speed increases, it would not be appropriate for these data. A more reasonable model for the windmill data that incorporates an upper asymptote would be based on \\(1/X\\).\n\nplot(df_wind$DCOutput,1/df_wind$WindVelocity_mph,pch=22,bg=1)\n\n\n\n\n\n\n\n# plot(df$DCOutput,log(df$WindVelocity_mph))\ndf_wind$WindVelocity_mph_inv=1/df_wind$WindVelocity_mph\nmodel2=lm(DCOutput~WindVelocity_mph_inv, df_wind)\nmodel2\n\n\nCall:\nlm(formula = DCOutput ~ WindVelocity_mph_inv, data = df_wind)\n\nCoefficients:\n         (Intercept)  WindVelocity_mph_inv  \n               2.979                -6.935  \n\nsumm=summary(model2); summ\n\n\nCall:\nlm(formula = DCOutput ~ WindVelocity_mph_inv, data = df_wind)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.20547 -0.04940  0.01100  0.08352  0.12204 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            2.9789     0.0449   66.34   &lt;2e-16 ***\nWindVelocity_mph_inv  -6.9345     0.2064  -33.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09417 on 23 degrees of freedom\nMultiple R-squared:   0.98, Adjusted R-squared:  0.9792 \nF-statistic:  1128 on 1 and 23 DF,  p-value: &lt; 2.2e-16\n\nstudent_res=rstudent(model2)\nMSE=summ$sigma^2\nqqnorm(student_res,pch=22,bg=1)\nabline(0,1)\n\n\n\n\n\n\n\nhist(student_res,xlim=c(-4,4))\n\n\n\n\n\n\n\nplot(model2$fitted.values,student_res,pch=22,bg=1,ylim=c(-2,2))\nabline(h=0)\n\n\n\n\n\n\n\n\n\n\n5.1.2 Box Cox Transformations\nOne technique is to use the data to estimate which transformation is best, a popular instance is the Box-Cox transformation. Consider the class of transformations: \\(\\{y^\\lambda\\colon \\lambda\\in\\mathbb{R}\\}\\). The regression model and \\(\\lambda\\) can be estimated simultaneously using the method of maximum likelihood. Recall that we used the method of least squares to estimate the model parameters - maximum likelihood is an alternative estimation strategy. Think of \\(\\lambda\\) like an extra model parameter, on top of \\(\\beta\\) and \\(\\sigma\\) that we need to estimate.\nLet \\[\\begin{align*}\n    \\tilde y&=\\log^{-1}(1/n\\sum_{i=1}^n\\log y_i)\\\\\n    y_\\lambda&=\n\\begin{cases}\n\\dfrac{y^\\lambda-1}{\\lambda\\tilde y^{\\lambda-1}}& \\lambda\\neq 0\\\\\n\\tilde y\\log y& \\lambda= 0\n\\end{cases}.\n\\end{align*}\\]\nWe then fit the following model \\[y_\\lambda=X\\beta+\\epsilon.\\]\nEven though \\(y^\\lambda \\neq y_\\lambda\\), we use \\(y^\\lambda\\) (or \\(\\log y\\) if \\(\\lambda=0\\)) as the final response - as it is more interpretable. It is entirely acceptable to use \\(y_\\lambda\\) as the response for the final model - this model will have a scale difference and an origin shift in comparison to the model using \\(y^\\lambda\\) (or \\(\\log y\\)). Usually the final \\(\\lambda\\) used in the model is rounded to a nice number for interpretation. A computational procedure is used for estimating \\(\\lambda\\), which we will not cover here. In general, we can compute a confidence interval for \\(\\lambda\\), and if it contains 1 then we may not need to transform.\n\nExample 5.4 Let’s apply the Box-Cox transformation to the two previous examples.\n\nThe R function boxcox from the MASS package can be used to execute the Box-Cox transformation. It requires you to specify a grid of points for \\(\\lambda\\), given below by seq(-2, 2, 1/10). We can set the plotit parameter to TRUE in order to see if this grid is big enough. We should see a peak or mode in the log-likelihood function that is plotted. If we don’t, we can expand the grid on the side which has the largest value of the log-likelihood. Observe below that we need to include points higher than 2 on the grid, as the function is still increasing for at \\(\\lambda=2\\):\n\n# You need the MASS package. \n# install.packages('MASS')\n\nbc=MASS::boxcox(DCOutput~WindVelocity_mph,data=df_wind,    \n       lambda = seq(-2, 2, 1/10), \n       plotit = TRUE,         \n       eps = 1/50,     \n       xlab = expression(lambda), \n       ylab = \"log-Likelihood\") \n\n\n\n\n\n\n\n# bc\n\n#Observe that \n\nbc=MASS::boxcox(DCOutput~WindVelocity_mph,data=df_wind,    \n                lambda = seq(0, 5, 1/10), \n                plotit = TRUE,         \n                eps = 1/50,     \n                xlab = expression(lambda), \n                ylab = \"log-Likelihood\") \n\n\n\n\n\n\n\n# bc\n\n#Seems like we should try lambda=2 \n\nThe confidence interval goes from just below 2 to just below 3. Let’s pick a round number, and try the transformation \\(\\lambda=2\\).\n\n# plot(df$DCOutput,log(df$WindVelocity_mph))\nmodel3=lm(DCOutput^2~WindVelocity_mph, df_wind)\nmodel3\n\n\nCall:\nlm(formula = DCOutput^2 ~ WindVelocity_mph, data = df_wind)\n\nCoefficients:\n     (Intercept)  WindVelocity_mph  \n         -1.3585            0.7107  \n\nsumm3=summary(model3); summ3\n\n\nCall:\nlm(formula = DCOutput^2 ~ WindVelocity_mph, data = df_wind)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.74840 -0.31027  0.05951  0.30793  0.57072 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.35851    0.21239  -6.396 1.58e-06 ***\nWindVelocity_mph  0.71066    0.03211  22.130  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3979 on 23 degrees of freedom\nMultiple R-squared:  0.9551,    Adjusted R-squared:  0.9532 \nF-statistic: 489.7 on 1 and 23 DF,  p-value: &lt; 2.2e-16\n\nplot(df_wind$WindVelocity_mph,df_wind$DCOutput^2,pch=22,bg=1)\n\n\n\n\n\n\n\nstudent_res=rstudent(model3)\nMSE=summ3$sigma^2\nqqnorm(student_res,pch=22,bg=1)\nabline(0,1)\n\n\n\n\n\n\n\nhist(student_res,xlim=c(-4,4))\n\n\n\n\n\n\n\nplot(model3$fitted.values,student_res,pch=22,bg=1,ylim=c(-2,2))\nabline(h=0)\n\n\n\n\n\n\n\n\nThe fit is not bad. The \\(R^2\\) is very high. There is a pattern in the QQplot and a slight pattern in the residuals plot. For knowing nothing about wind velocity, it is not bad.\nThe electricity data clearly points to the square root transformation - matching the analysis we did previously.\n\n## Electricity\n\nmodel=lm(y_kW~x_kWh, df)\n\nbc=MASS::boxcox(y_kW~x_kWh,data=df,    \n                lambda = seq(-2, 2, 1/10), \n                plotit = TRUE,         \n                eps = 1/50,     \n                xlab = expression(lambda), \n                ylab = \"log-Likelihood\") \nabline(v=0.5)\n\n\n\n\n\n\n\n# bc\n\n\n\n5.1.3 Homework stop\nComplete the assigned Chapter 5 questions.\n\n\n\n\nMiller, Don M. 1984. “Reducing Transformation Bias in Curve Fitting.” The American Statistician 38 (2): 124–26. http://www.jstor.org/stable/2683247.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transformations</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Miller, Don M. 1984. “Reducing Transformation Bias in Curve\nFitting.” The American Statistician 38 (2): 124–26. http://www.jstor.org/stable/2683247.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "Appendix_R.html",
    "href": "Appendix_R.html",
    "title": "6  Introduction to R software",
    "section": "",
    "text": "6.1 Some Basics\nR is a Statistical Programming language, it consists of 2 types of objects: data and functions.\n##Data\nx&lt;-2\nprint(x)\n\n[1] 2\n\n##function\nlog(2)\n\n[1] 0.6931472\nData is stored in variables and can take many forms. To store a value in a variable use “&lt;-”, above we set the variable x equal to 2. There are many data types in R, we will go through some of them.\n#real numbers\nnum=29.333\nnum\n\n[1] 29.333\n\n#Some math\n#adding and subtraction\n2+3-2\n\n[1] 3\n\n#multiplying and dividing\nnum&lt;-5*(10/25)\nnum\n\n[1] 2\n\n#Strings\nword&lt;-\"hello\"\nword\n\n[1] \"hello\"\n\nword='hello'",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R software</span>"
    ]
  },
  {
    "objectID": "Appendix_R.html#booleans",
    "href": "Appendix_R.html#booleans",
    "title": "6  Introduction to R software",
    "section": "6.2 Booleans",
    "text": "6.2 Booleans\nBooleans take on either TRUE or FALSE values, and can be very useful in R. You can set booleans to the result of a comparison of two data types, some of the syntax is below:\n\n&lt;,&gt;,&lt;=,&gt;= corresponds to less than, greater than, less than or equal, greater than or equal\n==, != equals, not equals\n&& , written like a&&b where a and b are booleans, it is TRUE is both a and b are TRUE\n|| , written like a||b where a and b are booleans, it is TRUE if at least one of a and b are TRUE\n\n\n#booleans can be initialize in a variety of ways, for example\n#must capitalize the true or false\nFALSE\n\n[1] FALSE\n\nF\n\n[1] FALSE\n\nT\n\n[1] TRUE\n\nmyBoolean&lt;-TRUE\nmyBoolean\n\n[1] TRUE\n\nmyBoolean2&lt;- 3&lt;4\nmyBoolean2\n\n[1] TRUE\n\nmyBoolean3&lt;-\"this\"==\"that\"\nmyBoolean3\n\n[1] FALSE\n\n## && (and) is TRUE if BOTH input booleans are true \n## || (or) is TRUE if AT LEAST one input boolean is true\nmyBoolean4&lt;-myBoolean2&&myBoolean\nmyBoolean4\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R software</span>"
    ]
  },
  {
    "objectID": "Appendix_R.html#vectors",
    "href": "Appendix_R.html#vectors",
    "title": "6  Introduction to R software",
    "section": "6.3 Vectors",
    "text": "6.3 Vectors\nVectors in R are used frequently, they are “lists” or “arrays” of all the same data type.\n\n##vectors are created with c(data,data,data)\nmyVector&lt;-c(2,3,4,5,6,7,8,9,10)\nmyVector\n\n[1]  2  3  4  5  6  7  8  9 10\n\n#a:b is a shortcut for a sequence from a to b adding 1\n#you can create vectors of sequences using seq(), for more type ?seq in the console\nmyVector2&lt;-2:10\nmyVector2\n\n[1]  2  3  4  5  6  7  8  9 10\n\nas.numeric(2:10)\n\n[1]  2  3  4  5  6  7  8  9 10\n\nas.double(2:10)\n\n[1]  2  3  4  5  6  7  8  9 10\n\nmyVector2&lt;-rep(NA,l=20)\n\n\n#These do not have to be numbers, they can be vectors, Strings, booleans...\nmyVector&lt;-c(myVector,myVector)\nmyVector\n\n [1]  2  3  4  5  6  7  8  9 10  2  3  4  5  6  7  8  9 10\n\nmyVector3&lt;-c(\"this\",\"is\",\"a\",\"vector\",\"of\",\"strings\")\nmyVector3\n\n[1] \"this\"    \"is\"      \"a\"       \"vector\"  \"of\"      \"strings\"\n\n#access elements with square brackets []\nmyVector[1]\n\n[1] 2\n\n#more advanced accesssing\n#access elements 1 to 5\nmyVector[1:5]\n\n[1] 2 3 4 5 6\n\n#access elements 1, 4 and 6\nmyVector[c(1,4,6)]\n\n[1] 2 5 7\n\n#access elements that are greater than 2\nmyVector[myVector&gt;2]\n\n [1]  3  4  5  6  7  8  9 10  3  4  5  6  7  8  9 10\n\nmyVector[-c(1,4,6)]\n\n [1]  3  4  6  8  9 10  2  3  4  5  6  7  8  9 10\n\n\nWe can perform mathematical operations and comparisons on vectors\n\nx&lt;-1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n#adds 1 to every element\nx+1\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n#this works for comparisons\nx&lt;4\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nx[x&lt;4]\n\n[1] 1 2 3\n\n#multiplies element 1 to element 1 of second vectors\nx*-(1:10)\n\n [1]   -1   -4   -9  -16  -25  -36  -49  -64  -81 -100\n\n#beware repetition\nx-c(1,2)\n\n [1] 0 0 2 2 4 4 6 6 8 8\n\n# mathematical operations on the vector apply to each element\n\n#squares each element\nx^2\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n#log each element\nlog(x)\n\n [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n [8] 2.0794415 2.1972246 2.3025851\n\n#Example: Dot Product\nx&lt;-c(1,2,3)\ny&lt;-c(2,5,8)\n#sum adds the elements of the vector together\nsum(x*y)\n\n[1] 36",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R software</span>"
    ]
  },
  {
    "objectID": "Appendix_R.html#matrices",
    "href": "Appendix_R.html#matrices",
    "title": "6  Introduction to R software",
    "section": "6.4 Matrices",
    "text": "6.4 Matrices\nYou can also use matrices in R.\n\n#you can create a matrix with matrix(vector of data,nrow=number of rows,ncol=number of columns)\n#You can see it will fill in the data down the columns first\nmyMatrix&lt;-matrix(1:9,nrow=3,ncol=3); myMatrix\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\nmyMatrix\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n#rbind and cbind add a row or column repectively to the matrix\n#you can create matrices with rbind(rowvector1,rowvector2,...), or with cbind(column vector 1, column vector 2,...)\n\nmyMatrix&lt;-rbind(c(2,3,4),c(3,4,5),c(1,2,3))\nmyMatrix\n\n     [,1] [,2] [,3]\n[1,]    2    3    4\n[2,]    3    4    5\n[3,]    1    2    3\n\nmyMatrix2&lt;-cbind(c(1,2,3),c(4,5,6),c(7,8,9))\nmyMatrix2\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\nmyMatrix3&lt;-cbind(myMatrix2,c(10,11,12))\nmyMatrix3\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nmyMatrix3&lt;-cbind(c(10,11,12),myMatrix2)\n\nWe can also do Matrix math:\n\n#again math functions apply to every element\nmyMatrix^2\n\n     [,1] [,2] [,3]\n[1,]    4    9   16\n[2,]    9   16   25\n[3,]    1    4    9\n\n#multiply with '%*%\nmyMatrix2%*%myMatrix\n\n     [,1] [,2] [,3]\n[1,]   21   33   45\n[2,]   27   42   57\n[3,]   33   51   69\n\n#we can find the inverse with 'solve()\nX&lt;-matrix(c(1,0,1,-2,3,0,1,4,2),nrow=3)\nX\n\n     [,1] [,2] [,3]\n[1,]    1   -2    1\n[2,]    0    3    4\n[3,]    1    0    2\n\nsolve(X)\n\n     [,1] [,2] [,3]\n[1,] -1.2 -0.8  2.2\n[2,] -0.8 -0.2  0.8\n[3,]  0.6  0.4 -0.6\n\n#check dimension\ndim(X)\n\n[1] 3 3\n\n#We can also transpose with t()\nt(X)\n\n     [,1] [,2] [,3]\n[1,]    1    0    1\n[2,]   -2    3    0\n[3,]    1    4    2\n\n#Some times to multiply vectors we have to trun them into matrix types\nmyVector&lt;-c(1,2,3)\nnewM&lt;-matrix(myVector,ncol=1)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R software</span>"
    ]
  },
  {
    "objectID": "Appendix_R.html#functions",
    "href": "Appendix_R.html#functions",
    "title": "6  Introduction to R software",
    "section": "6.5 Functions",
    "text": "6.5 Functions\nFunctions are objects that take an input and transform it into some output, just like in mathematics. We have already seen some, such as log().\nThey are called with this format output&lt;-functionName(input).\n\nThe input is called parameters, and there can be many parameters\nparameters are usually described in the documentation\nthe output is what the function returns\nfunctions can only return 1 object, but this includes a list… so it could return many objects in the form of a list object\n\nR has many, many functions, to learn more about a function type ?functionName and the documentation will come up.\n\n#A simple function\n#here the function log is called, with the parameter 2, and the output is stored in the variable x\nx&lt;-log(2)\nx\n\n[1] 0.6931472\n\n#A more complicated function\n#What are the parameters? \n#not rep(a,n) gives a vector of size n where all elements are a\ns&lt;-sample(x=1:10,size=4,replace=TRUE,prob=rep(1/10,10))\ns\n\n[1] 10  9  9  2\n\n\nWe have seen other people’s functions but we can also make our own! Let’s see an example first:\n\n#recall the dot product example...\ndotProd=function(a,b){\n  value&lt;-sum(a*b)\n  return(value)\n}\n#calling our function\ndotProd(x,y)\n\n[1] 10.39721\n\n\nWhat exactly does this code say?\n\nWe stored the function in the variable dotProd\nto tell the compiler we are creating a function, we use the keyword function\nwe specify the parameters in round brackets ()\nwe put the names of the parameters in the () only, not what data type we expect them to be\ninside curly brackets, we put the code that the function will run when it is called\nreturn() ends the function, and sends back the variable in the brackets\n\nBack to built in functions… R is a statistical software, what does that mean? It already includes many common statistical functions! For most common distributions there are functions for the pdf, cdf, inverse cdf as well as one to get a sample from that distribution. The syntax is in the format: dDistName(x,parameters), pDistName(x,parameters), qDistName(x,parameters) and rDistName(x,parameters) respectively. This will make more sense in the example below…\n\n#The normal distribution, sd is the standard deviation\n#pdf\ndnorm(c(2,3,5),mean=0,sd=1)\n\n[1] 5.399097e-02 4.431848e-03 1.486720e-06\n\n#cdf\npnorm(c(2,3,5),mean=0,sd=1)\n\n[1] 0.9772499 0.9986501 0.9999997\n\n#inverse cdf\nqnorm(c(0.2,.5,.3),mean=0,sd=1)\n\n[1] -0.8416212  0.0000000 -0.5244005\n\n#random sample of size 10\nrnorm(10,mean=0,sd=1)\n\n [1]  1.4965981  2.2993653  1.2375494  0.6349582 -1.4013610 -0.1496314\n [7] -0.8880282  0.5829058  0.3974302  0.5118981",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R software</span>"
    ]
  },
  {
    "objectID": "Appendix_R.html#plotting",
    "href": "Appendix_R.html#plotting",
    "title": "6  Introduction to R software",
    "section": "6.6 Plotting",
    "text": "6.6 Plotting\nR is very good for plotting! There are many types of plots in R, here are some useful plotting functions, this list is not exhaustive…\n\nplot(x,y,...) produces a scatter plot.\nabline(a=intercept,b=slope,...)\ncurve(expr,...) evaluates an expression along a grid to create a curve\nhist(data) creates a histogram\n\nPlot functions have many parameters, some include col which changes the color and add which should be set to TRUE if the plot should be added to the existing plot. The best way to learn plots is with examples, I have included a regression example below.\n\n#simulate errors\nepsilon&lt;-rnorm(100)\nx&lt;-rexp(100)\ny&lt;-9+2*x+epsilon\n\n#scatter plot with true line\nplot(x,y)\nabline(a=9,b=2,col=\"blue\")\n\n#least squares line\nlmm&lt;-lm(y~x)\nsummary(lmm)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.91155 -0.66094  0.05223  0.65269  2.55617 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.1876     0.1279   71.83   &lt;2e-16 ***\nx             1.9500     0.0948   20.57   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.897 on 98 degrees of freedom\nMultiple R-squared:  0.8119,    Adjusted R-squared:   0.81 \nF-statistic: 423.1 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\nabline(lmm$coefficients[1],lmm$coefficients[2],col=\"red\",lty=2)\n\n\n\n\n\n\n\n#histogram of residuals\nhist(epsilon,freq = F)\n#x is what you want to evaluate the grid along\ncurve(dnorm(x),add=T,col=\"blue\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R software</span>"
    ]
  },
  {
    "objectID": "Appendix_R.html#if-statements",
    "href": "Appendix_R.html#if-statements",
    "title": "6  Introduction to R software",
    "section": "6.7 If Statements",
    "text": "6.7 If Statements\nIf statements are essential in programming, and they are a form of ‘Control Structure’. They take the form if(boolean variable){some task}.\nWhen the computer runs through the code, it checks if the boolean value is TRUE, and if it is, it executes the code in the curly brackets, code in curly brackets is called a block. A simple example…\n\njim&lt;-\"nice\"\n\nif(jim==\"nice\"){\n  alice=\"nice\"\n}\n\nPlacing an else{some code} after the if statement will execute the code in it’s block if the code in the above if statement was not executed. The if and else must be in the same block so I have surrounded them in curly brackets.\n\njim&lt;-\"nice\"\n##same block\n{\nif(jim==\"nice\"){\n  alice=\"nice\"\n}\nelse{\n  alice=\"not nice\"\n}\n  alice\n}\n\n[1] \"nice\"\n\njim&lt;-\"mean\"\n##same block\n{\nif(jim==\"nice\"){\n  alice=\"nice\"\n}\nelse{\n  alice=\"not nice\"\n}\n  alice\n}\n\n[1] \"not nice\"\n\n\nYou may also use else if(boolean){block}, which executes it’s block if the above (else) if statement(s) did not execute. See below:\n\njim&lt;-\"okay\"\n##same block\n{\nif(jim==\"nice\"){\n  alice=\"nice\"\n}\nelse if(jim==\"okay\"){\n  alice=\"okay\"\n}\n  #Here if jim is not okay or nice, then we check if he is neutral.\nelse if(jim==\"neutral\"){\n  alice=\"neutral\"\n}\nelse{\n  alice=\"not nice\"\n}\n  alice\n}\n\n[1] \"okay\"\n\n\nLastly you may put if statements inside of other if statements, called ‘nested ifs’.\n\njim&lt;-\"nice\"\n##same block\n\nif(jim==\"nice\"){\n  alice=sample(c(\"nice\",\"not nice\"),1)\n  if(alice==\"nice\"){\n    print(alice)\n  }\n  else{\n    print(alice)\n  }\n}\n\n[1] \"nice\"",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R software</span>"
    ]
  },
  {
    "objectID": "Appendix_R.html#loops",
    "href": "Appendix_R.html#loops",
    "title": "6  Introduction to R software",
    "section": "6.8 Loops",
    "text": "6.8 Loops\nLoops execute operations within their blocks repeatedly. There are 2 types of loops you will generally use, for loops and while loops. For loops repeat the block a set number of times, while while loops repeat until a condition is satisfied. You can also nest loops, like if statements.\n\n#calculate 2 to the power of ten\nx&lt;-1\n#this reads for i in 1 to 10, this can be any vector that i loops through, not just a sequential one\nfor(i in 1:10){\n  x&lt;-x*2\n}\n\nx\n\n[1] 1024\n\nfor(i in 1:10){\n  x&lt;-x+i\n}\n\nvec=2:5\n\nfor(i in vec){\n  x&lt;-x+i\n}\n\n\n#calculate power of 2 less that 1000\nx&lt;-1\nwhile(2*x&lt;1000){\n  x&lt;-x*2\n}\nx\n\n[1] 512\n\n#nested loop\nfor(i in c(10,9,8,7,6,5,4,3,2,1)){\n  v&lt;-NULL\n  for(j in 1:i){\n    v&lt;-c(v,\"*\")\n  }\n  print(v)\n}\n\n [1] \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\"\n[1] \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\"\n[1] \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\"\n[1] \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\"\n[1] \"*\" \"*\" \"*\" \"*\" \"*\" \"*\"\n[1] \"*\" \"*\" \"*\" \"*\" \"*\"\n[1] \"*\" \"*\" \"*\" \"*\"\n[1] \"*\" \"*\" \"*\"\n[1] \"*\" \"*\"\n[1] \"*\"\n\n\nYou can also use the replicate function, which replicates a line of code a specified number of times. This gives a 10 by 5 matrix.\n\nreplicate(5,rnorm(10))\n\n            [,1]       [,2]        [,3]        [,4]         [,5]\n [1,] -0.8124341 -1.1438418  1.51771799  0.16457554  0.570530634\n [2,]  1.7470399 -2.0659195  1.77182159  0.98812214 -0.226077825\n [3,] -0.5859069 -0.2295778 -0.44750379  0.05092225  1.271742385\n [4,]  1.8662270 -0.3705188 -0.77888904 -3.96642977  0.003072567\n [5,]  0.6590239  0.5062104  0.09059537  0.04088317  0.659927854\n [6,] -1.2298358 -0.1828197  0.81723351  0.46448719  0.281320459\n [7,] -1.0249364 -0.7730835 -2.22220287  0.42346590 -0.138299241\n [8,]  1.3698698  0.5598986  2.15457152  0.04585078  0.855571648\n [9,]  1.3584260 -0.4201375  0.16771473  0.68766537 -0.281028906\n[10,] -0.6700234  0.3107051  1.01051316 -1.22069931  0.432353974\n\n\nSimilar functions include sapply() and apply(). sapply(X,FUN,...) applies the function that the parameter FUN is set to to individual elements of a vector. apply(X,MARGIN,FUN,...) applies FUN to the rows or columns depending on what MARGIN is set to, 1 for rows and 2 for columns.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R software</span>"
    ]
  },
  {
    "objectID": "Appendix_R.html#coverage-probability-example",
    "href": "Appendix_R.html#coverage-probability-example",
    "title": "6  Introduction to R software",
    "section": "6.9 Coverage Probability Example",
    "text": "6.9 Coverage Probability Example\nHere we generate 10000 samples of size 100 from the exponential distribution, with \\(\\lambda=2\\). We calculate 10000 confidence intervals for \\(1/\\lambda\\) with $=$1%, using the normal approximation: \\[\\sqrt{n}(\\bar{X}-1/\\lambda)\\sim N(0,1/\\lambda^{2})\\] and interval: \\[(\\bar{X}-t_{99}(0.005)*S/\\sqrt{n},\\bar{X}+t_{99}(0.005)*S/\\sqrt{n})\\] We then check the proportion of intervals that contain the true value of \\(1/\\lambda\\).\n\n#10000 samples, each of size 100 from the exponential distribution\nx&lt;-replicate(10000,rexp(100,rate=2))\n#x is 100 by 10000, each column is a sample\ndim(x)\n\n[1]   100 10000\n\n#calculate sample variances\nS_Vector&lt;-apply(x,2,sd); \n\n# S_Vector\n\n#get the t value\ntval&lt;-qt(1-0.005,99)\n#calculate the means\n\nmeans&lt;-apply(x,2,mean); length(means)\n\n[1] 10000\n\n# lower and upper bounds\nlower&lt;-means-S_Vector*tval/10\nupper&lt;-means+S_Vector*tval/10\nintervals&lt;-rbind(lower,upper)\n#example interval\nintervals[,1]\n\n    lower     upper \n0.3120019 0.5172486 \n\n#we now check each interval to see if it contains the mean\nsuccesses&lt;-0\nfor(i in 1:ncol(intervals)){\n  #if 0.5 is in the interval, add 1\n  if((intervals[1,i]&lt;0.5)&&(intervals[2,i]&gt;0.5))\n    successes&lt;-successes+1\n}\n#here is the coverage probability...\ncoverage.prob&lt;-successes/ncol(intervals)\ncoverage.prob\n\n[1] 0.9813\n\n\nSomething more advanced…\n\n#Vectorzing the function changes the way the function calculates when it is a passed a vector as a parameter...\n#it will run the function once per element if it is vectorized instead of passing the vector as a parameter and running once\ngetCovProb&lt;-Vectorize(function(alpha){\n#10000 samples, each of size 100 from the exponential distribution\n  x&lt;-replicate(10000,rexp(100,rate=2))\n#x is 100 by 10000, each column is a sample\n  dim(x)\n#calculate sample variances\n  S_Vector&lt;-apply(x,2,sd)\n#get the t value\n  tval&lt;-qt(1-alpha/2,99)\n#calculate the means\n  means&lt;-apply(x,2,mean)\n# lower and upper bounds\n  lower&lt;-means-S_Vector*tval/10\n  upper&lt;-means+S_Vector*tval/10\n  intervals&lt;-rbind(lower,upper)\n#example interval\n  intervals[,1]\n\n#we now check each interval to see if it contains the mean\nsuccesses&lt;-0\nfor(i in 1:ncol(intervals)){\n  #if 0.5 is in the interval, add 1\n  if((intervals[1,i]&lt;0.5)&(intervals[2,i]&gt;0.5))\n    successes&lt;-successes+1\n}\n#here is the coverage probability...\ncoverage.prob&lt;-successes/ncol(intervals)\nreturn(coverage.prob)\n})\n#here we find the coverage probability for many alphas\nalphas&lt;-seq(from=0.001,to=0.1,by=0.005)\ncoverages&lt;-getCovProb(alphas)\n#adds a scatter plot\nplot(alphas,coverages)\n#adds a line\nabline(a=1,b=-1,col=\"blue\")\n\n\n\n\n\n\n\n\nFor more information you can visit here . It is also very easy to find tutorials on the web (Youtube is good), you could also look at the book by Lafaye, Drouilhet and Liquet (2013).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R software</span>"
    ]
  }
]